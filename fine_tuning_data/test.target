Abstract|The ICGC/TCGA Pan-Cancer Analysis of Whole Genomes Consortium has systematically documented the variation in whole genomes associated with cancer across 38 tumour types and 2,658 whole cancer genomes. Most cancer genomes will experience 4 to 5 driver mutations in the whole genome. About 5 of cancer genomes do not have any identified driver mutations, which suggests that research towards cancer driver mutations is incomplete. Both common and rare variants of mutations, such as point mutations, structural variations, and clustering of mutations in the event of chromothripsis, describe the nature of cancer progression in humans.
Conclusions and future perspectives|The PCAWG study systematically quantifies the characteristic profiles of cancer mutations across 2,658 whole cancer genomes and 38 cancer types. The database of knowledge poses the potential to bridge cancer genomics and meaningful clinical outcomes with further research.
Methods|Subsets of driver mutations were identified that have high confidence on the basis of current literature knowledge through a rank-and-cut approach. A threshold was developed that assigned mutations with the highest driver ranking as the most probable driver mutations. The BWA-MEM algorithm was used to align each sample to the human reference build hs37d5. Three pipelines were used to identify mutations: Sanger, EMBL/DKFZ, and Broad. Within these pipelines, call-set benchmarking, merging, variant genotyping and statistical phasing methods were used to identify mutations.
Abstract|Patients with chronic skin diseases and scarring were treated with CBD ointment to test if CBD is effective against these issues. CBD improved the condition of the skin, and can be used safely to treat those with chronic skin conditions
Abstract|In this study, the researchers wanted to understand the relationship between race, trends in the safety and quality of obstetric services, and trends in maternal and neonatal mortality. They calculated indicators of hospital quality and patient safety as well as maternal and neonatal mortality by race using data from the Nationwide Inpatient Sample from 2000 to 2009. They explored differences between black and white women for these factors, adjusted for age, comorbidities, and geographic region.The study found that changes in the quality and safety of obstetric services differed by region but changes over time were constant for both white and black women. Cesarean deliveries (C-sections) increased for both black and white women, but more rapidly for white women. Indicators of obstetric safety improved for both black and white women, while neonatal injuries decreased. Even though these factors improved, maternal and neonatal mortality remained constant during the study period, with black women having higher rates for both than white women.In conclusion, improvements in obstetric quality and safety do not correlate with improvements in maternal and neonatal mortality. They do not explain racial disparities for pregnancy outcomes in black and white women. More research about quality measures related to pregnancy outcomes are necessary to fully understand these disparities.
Abstract|Previous studies have shown that black people have lower triage acuity scores (TAS), which determine how urgent a patient’s condition is in emergency departments (EDs), than white people. However, these studies did not determine whether this was due to bias of triagers or proper triage according to the severity of patients’ conditions. In this study, the researchers wanted to determine if black people are assigned lower TAS even after adjusting for demographic and clinical differences. They also wanted to understand if lower TAS caused longer wait times in EDs. The researchers analyzed data from an ED electronic medical record system, including demographic and clinical information, TAS, and ED waiting times. Triage scores were assigned from 1-5, with 1 being the most urgent. TAS and wait times were compared by race after adjusting for age, gender, insurance status, time of day, day of week, comorbidities, and abnormal vital signs. The researchers found that black people had an average TAS of 2.97, compared to 2.81 for white people, which was a significantly lower acuity rating. Black people also had longer wait times compared to white people. The results of this study show that racial bias may influence the triage process.
Abstract|Black people with diabetes have worse health outcomes and diabetes management compared to white people. In this study, the researchers wanted to understand if there were certain risk factors for poor diabetes control/outcomes that were more prevalent in black people than white people. The researchers used a case-control study and enrolled 764 black and white patients with diabetes who received care. Cases in the study had poor control over at least two of these three outcomes: HbA1c levels, systolic blood pressure, and cholesterol. Controls were patients with good control over all three. The researchers investigated the following risk factors: depression, low health literacy, poor medication adherence, patients’ capacity to reduce their own cardiovascular risk, and poor communication between patients and their healthcare provider. The researchers found that for black people and not white people, depression and poor medication adherence were associated with a greater chance that they would be labeled as a “case” rather than a “control.” This means that depression and poor medication adherence are risk factors for poor diabetes control in black patients. These risk factors may be important targets to address racial disparities in diabetes control and management.
Abstract|The ability of light to pass through an electrode (optical transparency) and the ability of an electrode to carry a current (electrical conductivity) are necessary for technology like solar powered generators, lights, and touch screens. Usually, the electrodes have to be unfavorably less transparent than the structures they are built on, so the relative transmittance is less than 100. This study used a dielectric-metal-dielectric-based electrode with a higher transparency than what is previously used, which allows the relative transmittance to equal about 100.
Introduction|Indium tin oxide (ITO) is usually used as the go to transparent electrode because it has a high transmittance and electrical conductivity. But ITO has plenty of disadvantages like the low level of indium on Earth and the low flexibility of the compound. To find a better alternative to ITO, scientists are considering clear electrodes with metal networks instead. These clear electrodes have the benefits of ITO, but have downsides like shorting out and looking hazy. The most recent solution uses dielectric-metal-dielectric (DMD) transparent electrodes. In this case, thin metal film is placed between two layers of antireflection devices, called dielectrics. Dielectric 1 simply refers to the dielectric on the top of the structure, and dielectric 2 refers to the dielectric under the metal film. Together, dielectric 1 and 2 sandwich the metal, and the substrate (usually PET) is on the bottom of the whole structure. This version has all of the benefits of the other materials previously used. To get an ideal conductivity, the entire electrode system has to be more transparent than the structure it is built on.This study aims to see how close they could get the transmittance while maintaining a functioning conductivity. They used a DMD-based transparent electrode on polyethylene terephthalate (PET) as the substrate to achieve a relative transmittance of greater than 100, which could potentially replace ITO.
Results|Strategy to achieve relative transmittance surpassing 100The relative transmittance was optimized by using seven different factors. The high number of factors can make it difficult to develop the best electrode system design. This study aims to maximize two of the factors: optical transmittance and electrical conductivity. Silver, copper, and gold are all considered as metallic layers, but silver has the lowest absorption so it is the material used. Complex vector addition and algebraic manipulation of variables are used to determine the maximum thickness and maximum speed that light travels through the material (called the refractive index).Proof of ConceptThough most other studies have used the same material for both of their dielectric components in the DMD, the math done in this study suggests that two different dielectric materials may actually be the most successful design. PET is always used as the optical substrate. Titanium dioxide (TiO2) is used as the dielectrics. The ideal structure to obtain a relative transmittance of over 100 as described in the study is: PET substrate/24nm TiO2/9nm Ag/63nm. Later, we will see that different materials are used for dielectric 1 and dielectric 2 with favorable results.Experimental DemonstrationsUsing thin silver films can lead to high transparency, but this can cause the issue of large surface roughness. Large surface roughness can decrease the electrical and optical function of the DMD electrode. This study demonstrates that the roughness can be reduced by using impure silver, that means adding copper (Cu) or another metal in small amounts to make the silver sample impure. This is called Cu-doped silver. Cu-doped silver was used with PET as the substrate, zinc oxide (ZnO) as dielectric 1, and aluminium oxide (Al2O3) as dielectric 2. This highly optimized model was the first study to obtain over 100 transmittance for a DMD electrode. With more optimization, the transmittance could reach even higher levels, and the DMD electrode could be used in a variety of settings, like high temperature and humidity conditions.
Discussion|Using a Cu-doped silver film in a DMD electrode can provide a thick, smooth, low optical loss, and highly electrically conductive system. The DMD electrode provides 88.4 absolute transmittance compared to the PET substrate, which has an absolute transmittance of 88.1. The DMD electrode offers an alternative to the ITO electrodes, and offers a relative transmittance of over 100.
Methods|Strategy to achieve relative transmittance surpassing 100The relative transmittance was optimized by using seven different factors. The high number of factors can make it difficult to develop the best electrode system design. This study aims to maximize two of the factors: optical transmittance and electrical conductivity. Silver, copper, and gold are all considered as metallic layers, but silver has the lowest absorption so it is the material used. Complex vector addition and algebraic manipulation of variables are used to determine the maximum thickness and maximum speed that light travels through the material (called the refractive index).Proof of ConceptThough most other studies have used the same material for both of their dielectric components in the DMD, the math done in this study suggests that two different dielectric materials may actually be the most successful design. PET is always used as the optical substrate. Titanium dioxide (TiO2) is used as the dielectrics. The ideal structure to obtain a relative transmittance of over 100 as described in the study is: PET substrate/24nm TiO2/9nm Ag/63nm. Later, we will see that different materials are used for dielectric 1 and dielectric 2 with favorable results.Experimental DemonstrationsUsing thin silver films can lead to high transparency, but this can cause the issue of large surface roughness. Large surface roughness can decrease the electrical and optical function of the DMD electrode. This study demonstrates that the roughness can be reduced by using impure silver, that means adding copper (Cu) or another metal in small amounts to make the silver sample impure. This is called Cu-doped silver. Cu-doped silver was used with PET as the substrate, zinc oxide (ZnO) as dielectric 1, and aluminium oxide (Al2O3) as dielectric 2. This highly optimized model was the first study to obtain over 100 transmittance for a DMD electrode. With more optimization, the transmittance could reach even higher levels, and the DMD electrode could be used in a variety of settings, like high temperature and humidity conditions.
Abstract|Implicit bias negatively affects doctor-patient relationships and communications. However, its effect on interactions between oncologists and their patients is unknown. In this study, the researchers examined how implicit bias influences oncology interactions. They also examined whether implicit bias in oncologists negatively affect patients confidence in and perceived difficulty of suggested cancer treatments. 18 non-black physicians and 112 black patients participated. Oncologists completed a implicit racial bias test weeks before treatment discussions with new patients, which were recorded. People watched the recordings and rated oncologist communication and the length of discussion between patients and oncologists. After their discussion, patients were asked questions about patient-centeredness, how well they remembered their discussion, distress, trust, and perceptions of recommended treatments. The researchers found that oncologists with more implicit bias had shorter interactions with patients, who rated oncologist communication as less patient-centered and less supportive. More implicit bias was also associated with patient difficulty of remembering the discussion, less patient confidence in recommended treatments, and greater perceived difficulty of completing recommended treatments. In conclusion, implicit bias negatively affects oncologist-patient communication and may be a source of health disparities that must be addressed in oncology.
Abstract|In this study, the researchers wanted to determine characteristics associated with the high COVID-19 incidence on Native American reservations. They examined 287 Native American Reservations and tribal homelands, which had 861 cases as of April 10th, 2020. The relationship between COVID-19 infection rate and certain community and household characteristics was examined. The researchers found that COVID-19 cases were more likely to be found on Native American Reservations with a higher proportion of households without indoor plumbing and those that were English-only. They found no relationship between COVID-19 cases and household overcrowding measures. In conclusion, the lack of indoor plumbing and access to drinkable water may be an important determinant of COVID-19 infection on Native American reservations. Access to information about COVID-19 transmission and prevention in Native American languages may also be important in reducing COVID-19 cases in these communities. Previous studies have pointed to household plumbing overcrowding, and language as being risk factors for other infectious diseases and these risk factors are still relevant for COVID-19. Investing in public health and household infrastructure in these communities are necessary to protect them from high COVID-19 incidence.
Abstract|Machine learning may be used as an alternative to traditional alignment-based approaches to identify new anti-microbial resistance (AMR) genes, especially when microbes cannot be grown in labs. In a previous study, the researchers created an algorithm that identified AMR genes using protein characteristics, called “features,” in Gram-negative bacteria using machine learning. In this study, the researchers show that this algorithm also works for Gram-positive bacteria, identifying AMR genes that cause resistance to bacitracin and vancomycin with 87-90 accuracy. The researchers present a software tool using this algorithm and machine learning approach.
Introduction|Antimicrobial resistance (AMR) is when bacteria become less susceptible to antimicrobial substances. AMR in bacteria is caused by many things, such as overexpression/duplication of existing genes, mutations and/or obtaining resistance genes from neighboring bacteria. 2.8 million people a year are infected by resistant bacteria in the U.S each year, resulting in 350,000 deaths. Resistant bacteria are a threat to human health worldwide, making it important to develop efficient tools to predict AMR.AMR is traditionally predicted by aligning sequence data with reference databases. Although this is a reliable method for highly conserved AMR genes, it can also produce many false positives even when the sequence data is very different from the reference database. Machine-learning techniques may be used as an alternative solution, predicting new AMR genes from metagenomic and pan-genomic data. However, these approaches used only a small number of genetic features to predict AMR genes and were inaccurate due to lack of feature-selection to remove irrelevant and redundant features.The researchers recently created a game-theory-based feature selection approach that was applied to Gram-negative bacteria that accurately predicted AMR genes (93-99 prediction accuracy) that give bacteria resistance to acetyltransferase, B-lactamase, and dihydrofolate reductase. In this study, the researchers test this approach from their previous study with sequence data from Gram-positive bacteria. They combined the results of both studies to create “Prediction of Antimicrobial Resistance via Game Theory” (PARGT), a computer software designed to identify AMR genes in both Gram-positive and negative bacteria. The software can be used to detect bac and van resistance genes in Gram-positive bacteria and aac, bla, and dfr genes in Gram-negative bacteria.
Results|Validation of PARGTIn their previous study, the researchers considered sequences for aac, bla, and dfr for Acinetobacter, Klebsiella, Campylobacter, Salmonella, and Escherichia as sequences to train the machine learning and tested the accuracy of the machine learning with sequences from Pseudomonas, Vibrio, and Enterobacter. The machine learning algorithm was able to correctly classify aac, bla, and dfr at 93, 99, and 97 accuracy respectively.In this study, the researchers tested this algorithm for AMR genes in Gram-positive bacteria. They considered 25 and 52 AMR sequences for van and bac respectively using Clostridium and Enterococcus to train machine learning. They selected features of AMR genes based on their relevance, non-redundancy, and interdependence with other features. They tested the trained algorithm using 6 bac AMR sequences, 9 van AMR sequences, and 14 non-AMR sequences in Staphylococcus, Streptococcus, and Listeria.The algorithm successfully identified all 6 bac genes (true positives) but misclassified two non-AMR genes as AMR (false positives). The sensitivity, specificity, and accuracy for the van sequences was 100, 86, and 90 respectively.The algorithm successfully identified all 8 of 9 van genes (true positives) but misclassified two non-AMR genes as AMR (false positives). The sensitivity, specificity, and accuracy for the van sequences was 89, 86, and 87 respectively.The AMR sequences’ NCBI accession number, protein names, and whether they were true or false positives can be seen in Tables 1 and 2 for bac and van respectively.Performance comparison with BLASTp and Kalign toolsThe researchers compared their algorithm’s performance against traditional alignment tools, specifically BLASTp and Kalign. Although the AMR sequences could be less identical to bacterial sequences for these tools to correctly identify them, this also led to a high number of false positives in which non-AMR sequences were miscategorized as being AMR. In addition, although BLASTp and Kalign perform better when the AMR sequences and bacterial sequences have a high similarity, the researchers’ algorithm performed better at low similarities, since it identifies AMR sequences using features of AMR genes rather than direct similarity.
Discussion|In this study, the researchers developed the “Prediction of Antimicrobial Resistance via Game Theory” (PARGT) software package to identify AMR genes in both Gram-negative and positive bacteria. PARGT generates protein features automatically and performs predictions based on the sequence the user gives the software. In addition, users can update PARGT by inputting their own known AMR and non-AMR sequences to train the algorithm so it can predict with better accuracy. In their previous study, the researchers proved that PARGT could accurately predict AMR genes in Gram-negative bacteria. In this study, they proved that PARGT works in Gram-positive bacteria as well, with a prediction accuracy from 87-90. PARGT performed better for bac since more sequences were available to train it, but traditional alignment tools (BLASTp and Kalign) worked for van since there was more sequence similarity.
Methods|GTDWFE algorithm for feature collectionCandidate features were collected using a literature search. The GTDWFE algorithm selected the best features based on relevance, non-redundancy, and interdependency of other features.Machine-learning algorithmAfter identifying the best features to use, they were used to train a machine-learning model. They then selected the best model to identify bac and van datasets.Overview of PARGT softwarePARGT is an open-source software written using Python and R. PARGT uses the best features as identified by the GTDWFE algorithm to make predictions. It also allows users to add AMR and non-AMR sequences to further train the machine-learning model, which could increase PARGT’s accuracy.Architecture of PARGTUsers input a set of known AMR and non-AMR sequences as a training dataset, generating the best features for identification in bacterial sequences. The trained machine learning model is then used to predict AMR sequences. PARGT can predict aac, bla, and dfr resistance genes in Gram-negative bacteria and bac and van resistance genes for Gram-positive bacteria.DatasetsAMR sequences were collected from the Antibiotic Resistance Genes Database, and non-AMR sequences were obtained from PATRIC.Data availabilityNCBI accession numbers for proteins can be accessed at https://github.com/abu034004/PARGT.Code availabilityThe PARGT software package and user manual are available at https://github.com/abu034004/PARGT.
Abstract|The density of the interstellar medium, the matter and radiation that exists between stars in space, determines where stars form and release energy and drives how galaxies change over time. Variations in this density result from how gases move (gas motion), but the nature of this motion across space and in different galaxies is unknown. Dense gas that forms stars is most likely formed by a combination of gas instability, collision, and turbulence. However, it is still difficult to understand the precise origin of stars because gas motion is difficult to quantify across the large spatial scale. In this study, the researchers measured the gas motion of the Milky Way and NGC 4321 galaxies, from which they obtained data ranging from 10-1-103 parsecs (pc). They detected fluctuations in velocity that were the same regardless of spatial scale and galaxy environments. Statistical analyses of these fluctuations showed how gases are densified to form stars. The researchers found oscillating gas flows with wavelengths from 0.3-400pc. These flows are most likely coupled with similarly oscillating densification mechanisms that most likely form through instabilities in gravity. These results demonstrate that the density of interstellar medium is controlled by gas flows and densification mechanisms, which are independent, and exist in many orders of magnitude in spatial scale.