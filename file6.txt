
Methods|The Bergamo province, which is extensively affected by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) epidemic, is a natural observatory of virus manifestations in the general population. In the past month we recorded an outbreak of Kawasaki disease; we aimed to evaluate incidence and features of patients with Kawasaki-like disease diagnosed during the SARS-CoV-2 epidemic.
Abstract|ObjectiveIn this systematic review, we will discuss the evidence on the occurrence of central nervous system (CNS) involvement and neurological manifestations in patients with COVID-19.MethodsMEDLINE (accessed from PubMed) and Scopus from December 01, 2019 to March 26, 2020 were systematically searched for related published articles. In both electronic databases, the following search strategy was implemented and these key words (in the title/abstract) were used: “COVID 19” OR “coronavirus” AND “brain” OR “CNS” OR “neurologic”.ResultsThrough the search strategy, we could identify two articles about neurological involvement by COVID-19. One of these publications was a narrative review and the other one was a viewpoint. However, the authors scanned the reference lists of the included studies and could identify multiple references. One study, specifically investigated the neurological manifestations of COVID-19 and could document CNS manifestations in 25% of the patients. Most of the studies investigated the manifestations of COVID-19 in general.ConclusionWhile neurological manifestations of COVID-19 have not been studied appropriately, it is highly likely that some of these patients, particularly those who suffer from a severe illness, have CNS involvement and neurological manifestations. Precise and targeted documentation of neurological symptoms, detailed clinical, neurological, and electrophysiological investigations of the patients, attempts to isolate SARS-CoV-2 from cerebrospinal fluid, and autopsies of the COVID-19 victims may clarify the role played by this virus in causing neurological manifestations.Keywords: CNS, Coronavirus, COVID-19, Neurological, Seizure
1. Introduction|Coronavirus is one of the major viruses that primarily targets the human respiratory system, but it also has neuroinvasive capabilities and can spread from the respiratory tract to the central nervous system (CNS). Previous epidemics or pandemics of coronaviruses include the severe acute respiratory syndrome (SARS) in 2002 and the Middle East respiratory syndrome (MERS) in 2012. The most recent pandemic of coronavirus infection is coronavirus disease (COVID-19) that is caused by SARS-CoV2 [1,2]. The symptoms of COVID-19 infection usually appear after an incubation period of about five days. The most common symptoms of COVID-19 illness are fever, cough, and fatigue; other symptoms include headache, hemoptysis, and dyspnea, among others. In the most severe cases, patients may develop pneumonia, acute respiratory distress syndrome, acute cardiac problems, and multiorgan failure [1]. The first cases of COVID-19 were reported in December 2019 [1]; however, when we searched the MEDLINE (accessed from PubMed), from December 01, 2019 to March 26, 2020, with the key word “COVID 19”, surprisingly 1655 articles were yielded. This shows that COVID-19 pandemic is of great global public health concern.Coronavirus infections have been associated with neurological manifestations (e.g., febrile seizures, convulsions, change in mental status, and encephalitis) [2,3]. Neurotropic and neuroinvasive capabilities of coronaviruses have been described in humans. Upon nasal infection, coronavirus enters the CNS through the olfactory bulb, causing inflammation and demyelination [3]. In this systematic review, we will discuss the evidence on the occurrence of CNS involvement and neurological manifestations in patients with COVID-19.
2. Methods|The report of this systematic review was made according to the recommendations of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement [4,5] (Fig. 1 ). The review protocol was not previously registered. MEDLINE (accessed from PubMed) and Scopus from December 01, 2019 to March 26, 2020 were systematically searched for related published articles. In both electronic databases, the following search strategy was implemented and these key words (in the title/abstract) were used: “COVID 19” OR “coronavirus” AND “brain” OR “CNS” OR “neurologic”. Articles written in English were all included in this search. To ensure literature saturation, the authors scanned the reference lists of the included studies or relevant reviews identified through the search. Both authors participated through each phase of the review independently (screening, eligibility, and inclusion). They independently screened the titles and abstracts yielded by the search against the inclusion criteria. They obtained full reports for all titles that appeared to meet the inclusion criteria or where there was any uncertainty. Authors screened the full text reports and decided whether these meet the inclusion criteria. They resolved any disagreement through discussions. Neither of the authors were blind to the journal titles or to the study authors or institutions.Open in a separate windowFig. 1Preferred reporting items for systematic reviews and meta-analyses (PRISMA) flow diagram of the study.The following data were extracted from the included studies: study authors, study designs, main results, and limitations. The methodological quality of the included studies was assessed by the authors. The class of evidence was defined following the American Academy of Neurology criteria for classification of evidence in studies of causation (Appendix 1) [6].2.1. Standard protocol approvals, registrations, and patient consentsThe Shiraz University of Medical Sciences Institutional Review Board approved this systematic review.
3. Results|Through the search strategy, we could identify two articles about neurological involvement by COVID-19 (Table 1, Table 2 ) [7,8]. One of these publications was a narrative review [7] and the other one was a viewpoint [8]. However, to ensure literature saturation, the authors scanned the reference lists of the included studies and could identify multiple references [9–14]. Table 3 shows the summary of these studies on the CNS manifestations of COVID-19. One study, specifically investigated the neurological manifestations of COVID-19 and could document CNS manifestations in 25% of the patients [9]. However, the authors did not perform electroencephalography (EEG) or cerebrospinal fluid (CSF) analysis. Another retrospective study investigated acute cerebrovascular disease occurrence following COVID-19 [10]. Other studies investigated the manifestations of COVID-19 in general; they did not specifically pay attention to the neurological manifestations [[11], [12], [13], [14]].Table 1The search keywords included “COVID 19” and those in the first column of the table.Medline (PubMed)Scopus (Article title, Abstract, Keywords)KeywordsPrimary hintsRelevant articlesPrimary hintsRelevant articlesBrain17021 (duplicate)Neurologic1111 (duplicate)CNS1100Total19232 (2 duplicates)Open in a separate windowTable 2The search keywords included “coronavirus” and those in the first column of the table.Medline (PubMed)Scopus (Article title, Abstract, Keywords)KeywordsPrimary hintsRelevant articlesPrimary hintsRelevant articlesBrain222 (2 duplicates)91 (duplicate)Neurologic11 (duplicate)41 (duplicate)CNS41 (duplicate)30Total274 (4 duplicates)162 (2 duplicates)Open in a separate windowTable 3Neurological manifestations of COVID-19.Author/yearMethodsNeurological manifestationsLimitationsLevel of evidenceMao/ 2020 [9]Retrospective case series of 214 admitted patientsCNS manifestations: in 25%. Headache (13%), dizziness (17%), impaired consciousness (8%), acute cerebrovascular problems (3%), ataxia (0.5), and seizures (0.5%)No CSF analysis; no EEG study; no clear definition of symptomsIIILi/ 2020 [10]Retrospective case series of 221 admitted patients5% developed acute ischemic stroke, 0.5% had cerebral venous sinus thrombosis, and 0.5% had cerebral hemorrhageOther related neurological manifestations were not studied.IIHuang/ 2020 [11]Prospective study of 41 admitted patientsHeadache in 8%Not specifically studied neurological manifestations. No CSF or EEG studiesIYang/ 2020 [12]Retrospective study of 52 critically ill adult patientsHeadache in 6%Not specifically studied neurological manifestations. No CSF or EEG studiesIIWang/ 2020 [13]Retrospective case series of the 138 hospitalized patientsDizziness in 9%; Headache in 7%Not specifically studied neurological manifestations. No CSF or EEG studiesIIChen/ 2020 [14]Retrospective case series of the 99 hospitalized patientsConfusion in 9%; Headache in 8%Not specifically studied neurological manifestations. No CSF or EEG studiesIIOpen in a separate windowCNS: central nervous system; CSF: cerebrospinal fluid; EEG: electroencephalography.
5. Conclusion|While neurological manifestations of COVID-19 have not been studied appropriately yet, it is highly likely that some of these patients, particularly those who suffer from a severe illness, have CNS involvement and neurological manifestations. Precise and targeted documentation of the neurological symptoms (e.g., headache, dizziness, etc.) and signs (e.g., change in mental status, meningeal signs, etc.), detailed clinical, neurological, and electrophysiological investigations (e.g., EEG) of the patients (particularly those with a change in mental status), attempts to isolate SARS-CoV-2 from CSF, and autopsies of the COVID-19 victims may clarify the roles played by this virus in causing neurological manifestations.
Abstract|WHO declared SARS-CoV-2 a global pandemic. The present aim was to propose an hypothesis that there is a potential association between mean levels of vitamin D in various countries with cases and mortality caused by COVID-19. The mean levels of vitamin D for 20 European countries and morbidity and mortality caused by COVID-19 were acquired. Negative correlations between mean levels of vitamin D (average 56 mmol/L, STDEV 10.61) in each country and the number of COVID-19 cases/1 M (mean 295.95, STDEV 298.7, and mortality/1 M (mean 5.96, STDEV 15.13) were observed. Vitamin D levels are severely low in the aging population especially in Spain, Italy and Switzerland. This is also the most vulnerable group of the population in relation to COVID-19. It should be advisable to perform dedicated studies about vitamin D levels in COVID-19 patients with different degrees of disease severity.Keywords: COVID-19, SARS-CoV2, Vitamin D, Cholecalciferol, Calcitriol
Background/aims|WHO declared COVID-19 caused by the virus SARS-CoV-2 a global pandemic. Little is known about the potential protective factors. In the case of COVID-19 we should delineate the protective factors in anti-infective agents that might protect against infection and factors that improve the outcome once the infection has been produced.Previous observational studies report independent associations between low serum concentration of 25-hydroxyvitanim D and susceptibility to acute respiratory tract infections [1]. In a systematic review and meta-analysis of 25 randomised controlled studies, Martineau et al. has described that vitamin D protected against acute respiratory tract infection overall [2]. In a review of the literature, regarding the possible role of vitamin D in the prevention of influenza virus infection, Gruber-Bzura noticed that the data generate controversies and doubts [3].Calcitriol (1,25-dihydroxyvitamin D3) exerts pronounced impacts on ACE2/Ang(1–7)/MasR axis with enhanced expression of ACE2 [4]. ACE-2 is the host cell receptor responsible for mediating infection by SARS-CoV-2. Starting from this, it might suggest a higher risk of infection. However, this has not been shown to date and previous studies identified associations between higher levels of ACE2 and better coronavirus disease health outcomes. In the lung, ACE2 was shown to protect against acute lung injury [5].We hypothesize that vitamin D may play a protective role for COVID-19.The primary aims of this study are to assess if there is any association between the mean levels of vitamin D in various countries and the mortality caused by COVID-19. The secondary aim was to identify if there is any association between the mean vitamin D levels in various countries and the number of cases of COVID-19.
Results|We have observed a negative correlation between levels of mean vitamin D (average 56.79 nmol/L, STDEV 10.61) and number of cases of COVID-19/1 M population in each country [average 1393.4, STDEV 1129.984, r(20) = − 0.4435; p value = 0.050], and between the mean vitamin D levels and the number of deaths caused by COVID-19/1 M (Fig. 1) [average 80.42, STDEV 94.61, r(20)-value = − 0.4378; p value = 0.05) (Table ​(Table11).Open in a separate windowFig. 1Mean vitamin D levels per courtry versus COVID-19 cases and mortality/1M population
Discussions|We have identified a potential crude association between the mean vitamin D levels in various European countries with COVID-19 cases/1M and COVID-19 mortality.The European Calcified Tissue Society Working Group has defined severe vitamin D deficiency as a serum 25(OH)D level lower than 30 nmol/L [6].The Seneca study showed a mean serum vitamin D level of 26 nmol/L in Spain, 28 nmol/L in Italy and 45 nmol/L in the Nordic countries, in older people [6]. In Switzerland, mean vitamin D level is 23 nmol/L in nursing homes and in Italy 76% of women over 70 years of age have been found to have circulating levels below 30 nmol/L [6]. These are the countries with high number of cases of COVID-19 and the aging people is the group with the highest risk for morbidity and mortality with SARS-CoV2. Isaia et al. reported 25(OH)D circulating levels less than 12 ng/mL (30 nmol/L) in 76% of Italian women over 70 years of age, in late Winter [7].Vitamin D deficiency is a major public health problem worldwide in all age groups [8, 9] but vitamin D status deteriorates with age, above 70 years of life, due to decreased sun exposure and cutaneous synthesis [10]. It is poor in the institutionalized people, 75% of them being severely vitamin D deficient (serum 25(OH)D < 25 nmol/L) [6].The Southern European countries have lower levels of vitamin D because of decreased exposure (prefer the shade in strong sun) [10] and also as skin pigmentation decreases vitamin D synthesis [11]. Northern part of Europe’s mean levels are better as a consequence of the consumption of cod liver oil and vitamin D supplements as well as fortification of milk and milk products (Finland) [6].The crude association observed in the present study may be explained by the role of vitamin D in the prevention of COVID-19 infection or more probably by a potential protection of vitamin D from the more negative consequences of the infection.Regarding the protective role of vitamin D against infection with SARS-Cov2, we can start by looking at the effect on other respiratory infections.Martineau et al. concluded in a meta-analysis that vitamin D supplementation was safe and protective against acute respiratory tract infections. They described that patients who were severe vitamin D deficient experienced the most benefit [2].The pathology of COVID-19 involves a complex interaction between the SARS-CoV2 and the body immune system. Calcitriol (1,25-dihydroxyvitamin D3) exerts pronounced impacts on ACE2/Ang(1–7)/MasR axis with enhanced expression of ACE2 [4]. ACE2 is the host cell receptor responsible for mediating infection by SARS-CoV-2. From this perspective it might be apparent that the risk of infection can be higher. However, vitamin D has multiple roles in the immune system that can modulate the body reaction to an infection. Abu-Amer et al. have described that vitamin D deficiency impairs the ability of macrophages to mature, to produce macrophage-specific surface antigens, to produce the lysosomal enzyme acid phosphatase, and to secrete H2O2, a function integral to their antimicrobial function [12]. This might explain, in part, why Martineau et al. have observed that vitamin D was protective in cases of hypovitaminosis [5]. Crucial in the innate immune response are the toll-like receptors which recognise molecules related to pathogens and when activated release cytokines and induce reactive oxygen species and antimicrobial peptides, cathelicins and defensins. Several of the toll-like receptors affect or are affected by vitamin D receptor induction [3].COVID-19 is caused, beside the virus virulence by the release of pro-inflammatory cytokines. Vitamin D has been found to modulate macrophages' response, preventing them from releasing too many inflammatory cytokines and chemokines [13]. Ciu et al. found that calcitriol (1,25-dihydroxyvitamin D3) exerted pronounced impact on ACE2/Ang(1–7)/MasR axis with enhanced expression of ACE2, MasR and Ang(1–7) generation [4].Zhen et al. have shown that H5N1 flu infection-induced lung injury can be alleviated by the administration of recombinant human ACE2 protein [14].Kuka et al. have identified that higher levels of ACE2 are associated with better outcomes for coronavirus disease and it has been shown that in the lung, ACE2 protects against acute lung injury [5].Xie et al. investigated ACE2 expression in lungs and the effect caused by ageing and gender on its expression, in a rodent model study. The authors described that the decrease of ACE2 was relatively small between the young-adult and the middle-aged groups (25% for male and 18% for female, respectively), but the content of ACE2 decrease by 67% in older female rats and even higher, 78% in older male rats as compared to younger groups [15]. This decrease of ACE2 with age and gender seems to parallel the increase in COVID-19 mortality as well as the higher mortality in the male population.We acknowledge that this cross-sectional analysis has limitations. The number of cases/country is affected by the number of tests performed and also by the different measures taken by each country to prevent the spread of infection, and the difference in the number of infected patients in the population will also mean different levels of exposure for the population. Mortality might be a better marker of the number of cases in the population although even that can be influenced by the variations in the approach or management of the disease. However, the aim of the present study was to lay out an hypothesis to be taken forward and be investigated utilizing robust study designs.In conclusion, we found significant crude relationships between vitamin D levels and the number COVID-19 cases and especially the mortality caused by this infection. The most vulnerable group of population for COVID-19, the aging population, is also the one that has the most deficit Vitamin D levels.Vitamin D has already been shown to protect against acute respiratory infections and it was shown to be safe. It should be advisable to perform dedicated studies about vitamin D levels in COVID-19 patients with different degrees of disease severity.
Abstract|The ongoing outbreak of coronavirus disease 2019 (COVID-19) has spread rapidly on a global scale. Although it is clear that severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is transmitted through human respiratory droplets and direct contact, the potential for aerosol transmission is poorly understood1,2,3. Here we investigated the aerodynamic nature of SARS-CoV-2 by measuring viral RNA in aerosols in different areas of two Wuhan hospitals during the outbreak of COVID-19 in February and March 2020. The concentration of SARS-CoV-2 RNA in aerosols that was detected in isolation wards and ventilated patient rooms was very low, but it was higher in the toilet areas used by the patients. Levels of airborne SARS-CoV-2 RNA in the most public areas was undetectable, except in two areas that were prone to crowding; this increase was possibly due to individuals infected with SARS-CoV-2 in the crowd. We found that some medical staff areas initially had high concentrations of viral RNA with aerosol size distributions that showed peaks in the submicrometre and/or supermicrometre regions; however, these levels were reduced to undetectable levels after implementation of rigorous sanitization procedures. Although we have not established the infectivity of the virus detected in these hospital areas, we propose that SARS-CoV-2 may have the potential to be transmitted through aerosols. Our results indicate that room ventilation, open space, sanitization of protective apparel, and proper use and disinfection of toilet areas can effectively limit the concentration of SARS-CoV-2 RNA in aerosols. Future work should explore the infectivity of aerosolized virus.
Methods|Data reportingNo statistical methods were used to predetermine sample size. The experiments were not randomized and the investigators were not blinded to allocation during experiments and outcome assessment.Sample collectionThe sampling was conducted between 17 February and 2 March 2020 in the locations in two rounds as shown in Table 1. All aerosol samples were collected on presterilized gelatin filters (Sartorius). A total of 30 aerosol samples of total suspended particles were collected on 25-mm-diameter filters loaded into styrene filter cassettes (SKC) by sampling air at a fixed flow rate of 5.0 l min−1 using a portable pump (APEX2, Casella). A total of three size-segregated aerosol samples was collected using a miniature cascade impactor (Sioutas Impactor, SKC) that separated aerosols into five ranges (>2.5 μm, 1.0–2.5 μm, 0.50–1.0 μm and 0.25–0.50 μm on 25-mm filter substrates, and 0–0.25 μm on 37-mm filters) at a flow rate of 9.0 l min−1. A total of two aerosol deposition samples was collected using 80-mm-diameter filters packed into a holder with an effective deposition area of 43.0 cm2 and the filters were placed intact on the floor in two corners of the intensive care unit room of Renmin Hospital for 7 days. Sampling durations and operation periods are described in Supplementary Table 1. All sampling instruments were located in the centre of the respective sampling area, where the sampling inlet was at a height of 1.5 m from the floor. Considering the limited experimental conditions and the small sample size, the integrity and robustness of the experiment protocol was examined extensively in the laboratory before field sampling and these results are described in Supplementary Table 2.Analytical method and data analysisAfter the collection of aerosol samples, all samples were processed immediately in the BSL-2 laboratory of Wuhan University. The 25-, 37-mm and 80-mm filter samples were dissolved in deionized water, after which TRIzol LS reagent (Invitrogen) was added to inactivate SARS-CoV-2 viruses and extract RNA according to the manufacturer’s instructions. First-strand cDNA was synthesized using the PrimeScript RT kit (TakaRa). Optimized ddPCR was used to detect the presence of SARS-CoV-2 viruses according to a previous study10. Analysis of the ddPCR data was performed using QuantaSoft software (Bio-Rad). The concentration reported by the procedure equals the number of copies of template per microlitre of the final 1× ddPCR reaction, which was normalized to copies m−3 in all of the results; therefore, the virus or viral RNA concentration in aerosol is expressed in copies m−3 throughout. A detailed protocol is provided in the Supplementary Information.Reporting summaryFurther information on research design is available in the Nature Research Reporting Summary linked to this paper.
Abstract|The recent outbreak of coronavirus infectious disease 2019 (COVID-19) has gripped the world with apprehension and has evoked a scare of epic proportion regarding its potential to spread and infect humans worldwide. As we are in the midst of an ongoing pandemic of COVID-19, scientists are struggling to understand how it resembles and differs from the severe acute respiratory syndrome coronavirus (SARS-CoV) at the genomic and transcriptomic level. In a short time following the outbreak, it has been shown that, similar to SARS-CoV, COVID-19 virus exploits the angiotensin-converting enzyme 2 (ACE2) receptor to gain entry inside the cells. This finding raises the curiosity of investigating the expression of ACE2 in neurological tissue and determining the possible contribution of neurological tissue damage to the morbidity and mortality caused by COIVD-19. Here, we investigate the density of the expression levels of ACE2 in the CNS, the host–virus interaction and relate it to the pathogenesis and complications seen in the recent cases resulting from the COVID-19 outbreak. Also, we debate the need for a model for staging COVID-19 based on neurological tissue involvement.Keywords: Coronavirus, SARS-CoV-2, COVID-19, ACE2 tissue distribution, host−virus interaction, spike protein
6. Conclusions and Future Directions|Autopsies of the COVID-19 patients, detailed neurological investigation, and attempts to isolate SARS-CoV-2 from the endothelium of cerebral microcirculation, cerebrospinal fluid, glial cells, and neuronal tissue can clarify the role played by this novel COVID-19 causing coronavirus in the ongoing mortalities as has been in the recent outbreak. It is important to mention here that although the cerebral damage may complicate a COVID-19 infection, it appears that it is the widespread dysregulation of homeostasis caused by pulmonary, renal, cardiac, and circulatory damage that proves fatal in COIVD-19 patients. With that being said, a dominant cerebral involvement alone with the potential of causing cerebral edema in COVID-19 can take a lead in causing death long before systemic homeostatic dysregulation sets in. Access of the COVID-19 virus to the brain via the transcribrial route, as described previously for other CNS targeting pathogens,7 could have been the case in a recently reported patient with hyposmia and the cases of acute respiratory failure in COVID-19,5 which needs to be further elucidated by isolating the SARS-CoV-2 virus from the zones that are in proximity to the olfactory bulb. It is expected that the differences in the sequence of spike proteins between COVID-19 virus and SARS-CoV (Figure ​Figure22A) will enable scientists to identify epitopes in COVID-19 virus for the development of monoclonal antibodies against this virus. With the recent COVID-19 outbreak, there is an urgent need to understand the neurotropic potential of the COVID-19 virus in order to prioritize and individualize the treatment protocols based on the severity of the disease and predominant organ involvement. Also, a staging system based on the severity and organ involvement is needed in COVID-19 in order to rank the patients for aggressive or conventional treatment modalities.
Abstract|Approximately 100,000 individuals in the United States currently await kidney transplantation, and 400,000 individuals live with end-stage kidney disease requiring hemodialysis. The creation of a transplantable graft to permanently replace kidney function would address donor organ shortage and the morbidity associated with immunosuppression. Such a bioengineered graft must have the kidney's architecture and function and permit perfusion, filtration, secretion, absorption and drainage of urine. We decellularized rat, porcine and human kidneys by detergent perfusion, yielding acellular scaffolds with vascular, cortical and medullary architecture, a collecting system and ureters. To regenerate functional tissue, we seeded rat kidney scaffolds with epithelial and endothelial cells and perfused these cell-seeded constructs in a whole-organ bioreactor. The resulting grafts produced rudimentary urine in vitro when perfused through their intrinsic vascular bed. When transplanted in an orthotopic position in rat, the grafts were perfused by the recipient's circulation and produced urine through the ureteral conduit in vivo.
Abstract|Flavonoids are known to trigger the intrinsic genetic adaptive programs to hypoxic or oxidative stress via estrogen receptor engagement or upstream kinase activation. To reveal specific structural requirements for direct stabilization of the transcription factors responsible for triggering the antihypoxic and antioxidant programs, we studied flavones, isoflavones and catechols including dihydroxybenzoate, didox, levodopa, and nordihydroguaiaretic acid (NDGA), using novel luciferase-based reporters specific for the first step in HIF1 or Nrf2 protein stabilization. Distinct structural requirements for either transcription factor stabilization have been found: as expected, these requirements for activation of HIF ODD-luc reporter correlate with in silico binding to HIF prolyl hydroxylase. By contrast, stabilization of Nrf2 requires the presence of 3,4-dihydroxy- (catechol) groups. Thus, only some but not all flavonoids are direct activators of the hypoxic and antioxidant genetic programs. NDGA from the Creosote bush resembles the best flavonoids in their ability to directly stabilize HIF1 and Nrf2 and is superior with respect to LOX inhibition thus favoring this compound over others. Given much higher bioavailability and stability of NDGA than any flavonoid, NDGA has been tested in a 1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine (MPTP)-animal model of Parkinson’s Disease and demonstrated neuroprotective effects.Keywords: Parkinson’s disease model, glutathione depletion model, HIF prolyl hydroxylase, lipoxygenase, fisetin, luteolin, Keap1
Abstract|Background and aimsThe outbreak of the new coronavirus, SARS-CoV-2, causes a respiratory disease and individuals with pre-existing cardiometabolic disorders display worse prognosis through the infection course. The aim of this minireview is to present epidemiological data related to metabolic comorbidities in association with the SARS-CoV-2.MethodsThis is a narrative mini-review with Pubmed search until April 23, 2020 using the keywords COVID-19, SARS-CoV-2, treatment of coronavirus and following terms: diabetes mellitus, obesity, arterial hypertension, ACE-inhibitors, cytokine storm, immune response and vitamin D.ResultsStudies indicate that obese individuals are more likely to develop infections, and that adipose tissue serves as a pathogen reservoir. In diabetic individuals higher rate of inflammatory processes is seen due to constant glucose recognition by C type lectin receptors. Hypertensive individuals, usually grouped with other conditions, are treated with drugs to reduce blood pressure mostly through ACEi and ARB, that leads to increased ACE2 expression, used by SARS-CoV-2 for human’s cell entry. Until now, the studies have shown that individuals with those conditions and affected by COVID-19 present an uncontrolled release of pro-inflammatory cytokines and an unbalanced immune response, leading to the cytokine storm phenomenon. Vitamin D is highlighted as a potential therapeutic target, because in addition to acting on the immune system, it plays an important role in the control of cardiometabolic diseases.ConclusionCurrently, since there is no proven and effective antiviral therapy for SARS-CoV-2, the efforts should focus on controlling inflammatory response and reduce the risks of associated complications.Keywords: Diabetes mellitus, Obesity, Hypertension arterial, Vitamin D, Immune response
1. Introduction|Diabetes, hypertension and other cardiovascular diseases (CVD) are strongly related to a higher risk of mortality or disease’s severity among COVID-19 patients. In China, from 44,672 confirmed cases, 4.7% were critical, with a case-fatality rate (CFR) of 49%. Patients without comorbidities had lower CFR (0.9%), while those with CVDs, diabetes and hypertension had higher rates (10.5%, 7.3%, 6.5%, respectively) [1]. Obesity is another comorbidity raising the risk of complications in COVID-19 infection. The immune system acts upon inflammation that occurs in adipose tissue due to obesity, increasing vulnerability to infections [2]. Increased circulating levels of many cytokines and proteins released by adipocytes are associated with inflammation in obese individuals [3]. Inflammation from adipose tissue generates chronic and systemic metabolic alterations leading to dyslipidemia, hypertension, CVD and diabetes, thus increasing the risk of infection by Severe Acute Respiratory Syndrome-Coronavirus-2 (SARS-CoV-2)2. It is thought that SARS-CoV-2, as SARS-CoV and MERS-CoV, suppress anti-viral type IFN-γ responses in the early stage of infection leading to an uncontrolled viral replication. This mechanism later leads to an influx of neutrophils and monocytes/macrophages, resulting in hyperproduction of pro-inflammatory cytokines that can damage lung tissue (i.e. pneumonia, acute respiratory distress syndrome). Specific Th1/Th17 cells may be activated and contribute to increased inflammation. These facts and the higher mortality by COVID-19 observed in those with underlying diseases indicates that immune response is a determining factor to COVID-19 outcome [4].According to phylogenetic analyses, SARS-CoV-2 is related to other existing respiratory infections, such as SARS-CoV and Middle East respiratory syndrome-Coronavirus (MERS-CoV) [5]. SARS-CoV infection accounted for 8098 cases and 774 deaths in 26 countries [6], meanwhile, MERS-CoV infection was identified in 27 countries, with 2449 confirmed cases and 845 deaths [7]. As is being observed in COVID-19, MERS-CoV infection also caused more severe complications in older and immunocompromised patients with a history of diabetes, renal failure, and lung diseases [8]. According to genome sequencing, SARS-CoV-2 is about 89% identical to bat SARS-like-CoVZXC21, 82% identical to human SARS-CoV and about 50% to MERS-CoV [5,9]. Several phylogenetic analyses suggests the bat as most probable animal reservoir for SARS-CoV-2, and as both SARS-CoV and MERS-CoV were transmitted from bats to palm civets or dromedary camels, and then to humans, it is possible though that another animal plays the role of intermediate host between bat and human in SARS-CoV-2 infection [10]. These viruses have a structural protein, the Spike glycoprotein (S) that is responsible for its binding to host cells [11]. According to Hoffman [12] protein S is primed by human serine protease (TMPRSS2) and recognized by the cell receptor. A recent study described by Liu [11] shows that SARS-CoV entering in the respiratory tract depends on a receptor-binding domain (RBD) to bind to the host, and protein S has two trimers that bind to the heterodimer of angiotensin-converting enzyme II (ACE2). A synthetic SARS-CoV-2 RBD analysis indicated that the virus enters into the host cell through ACE2 binding. However, MERS-CoV binds specifically to another receptor, dipeptidil peptidase 4 (DPP4) [11,13]. Therefore, since SARS-CoV-2 is analogous to SARS-CoV, it is suggested that both use ACE2 as the main mechanism for cell entry.The pathophysiology of SARS-CoV-2 is not yet well understood, it is known to cause an acute lung injury and that this condition resembles SARS-CoV, which results in aggressive inflammation initiated by viral replication [14]. In this review, we tried to discuss and report possible mechanisms of inflammatory responses mediated by SARS-CoV-2 in individuals with pre-existing cardiometabolic diseases and to speculate possible therapeutic target that can be applied to obtain a better immune response, reduce pro-inflammatory profile, and consequently reduce critical levels of the disease.
Abstract|Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a newly emerged coronavirus that is responsible for the current pandemic of coronavirus disease 2019 (COVID-19), which has resulted in more than 3.7 million infections and 260,000 deaths as of 6 May 20201,2. Vaccine and therapeutic discovery efforts are paramount to curb the pandemic spread of this zoonotic virus. The SARS-CoV-2 spike (S) glycoprotein promotes entry into host cells and is the main target of neutralizing antibodies. Here we describe several monoclonal antibodies that target the S glycoprotein of SARS-CoV-2, which we identified from memory B cells of an individual who was infected with severe acute respiratory syndrome coronavirus (SARS-CoV) in 2003. One antibody (named S309) potently neutralizes SARS-CoV-2 and SARS-CoV pseudoviruses as well as authentic SARS-CoV-2, by engaging the receptor-binding domain of the S glycoprotein. Using cryo-electron microscopy and binding assays, we show that S309 recognizes an epitope containing a glycan that is conserved within the Sarbecovirus subgenus, without competing with receptor attachment. Antibody cocktails that include S309 in combination with other antibodies that we identified further enhanced SARS-CoV-2 neutralization, and may limit the emergence of neutralization-escape mutants. These results pave the way for using S309 and antibody cocktails containing S309 for prophylaxis in individuals at a high risk of exposure or as a post-exposure therapy to limit or treat severe disease.
Abstract|With the growth of social media as a platform to share information, veganism is becoming more visible, and could be becoming more accepted in sports and in the health and fitness industry. However, to date, there appears to be a lack of literature that discusses how to manage vegan diets for athletic purposes. This article attempted to review literature in order to provide recommendations for how to construct a vegan diet for athletes and exercisers. While little data could be found in the sports nutrition literature specifically, it was revealed elsewhere that veganism creates challenges that need to be accounted for when designing a nutritious diet. This included the sufficiency of energy and protein; the adequacy of vitamin B12, iron, zinc, calcium, iodine and vitamin D; and the lack of the long-chain n-3 fatty acids EPA and DHA in most plant-based sources. However, via the strategic management of food and appropriate supplementation, it is the contention of this article that a nutritive vegan diet can be designed to achieve the dietary needs of most athletes satisfactorily. Further, it was suggested here that creatine and β-alanine supplementation might be of particular use to vegan athletes, owing to vegetarian diets promoting lower muscle creatine and lower muscle carnosine levels in consumers. Empirical research is needed to examine the effects of vegan diets in athletic populations however, especially if this movement grows in popularity, to ensure that the health and performance of athletic vegans is optimised in accordance with developments in sports nutrition knowledge.Keywords: Vegan, Vegetarian, Plant-based, Diet
Background|Vegan diets might be becoming more visible, owing to the proliferation of social media as a means to share information, experiences and discuss opinions [1]. Promoted by some for alleged health benefits such as reduced risk of heart disease, lower LDL, blood pressure, type II diabetes and cancer [2, 3], veganism is a form of vegetarianism that prohibits the consumption of animal products [4]. Several high-profile athletes, such as former world heavyweight champion boxer David Haye and ladies tennis champion Venus Williams, have reportedly adopted vegan diets in recent times. Quite often, veganism is the product of strong ethical beliefs concerning animal welfare, and vegan activists have been subject to stigma [5], stereotyping [6] and negative attitudes [7], due in part to their vocal denigration of animal consumerism. The increased visibility of high-profile vegan competitors might suggest that veganism could be becoming more appealing for some, especially if more successful athletes adopt and publicize their vegan lifestyles.Poorly constructed vegan diets however might predispose individuals to macronutrient (protein, n-3) and micronutrient (vitamin B12 and vitamin D; iron, zinc, calcium, iodine) deficiencies [2, 3, 8, 9]. This is of particular concern if little attention is paid to accommodating for the nutrients that are excluded due to the elimination of animal products from the diet [9]. Some have alleged that a vegan diet could offer potential performance benefits due to the antioxidant (polyphenols), micronutrient (vitamin C, E) and carbohydrate-rich foods typical of plant-based diets assisting training and enhancing recovery [10, 11]. However, empirical research validating this claim is either equivocal or missing [12]. Indeed, there appears to be a lack of research into veganism in sport in general, despite interest in literature elsewhere [13]. In order to ensure that vegan diets meet both health and performance needs, basic dietary requirements have to be met and sport-specific diet-related objectives need to be achieved [9, 14]. The aim of this article is to address this point, and to provide practical recommendations for sports dieticians, coaches and trainers who might work with vegan athletes. Particular attention will be paid to the achievement of macro and micronutrient requirements for athletic and health-related purposes in this article, as well as a discussion of supplements and ergogenic aids that might be of use to performers who adopt this lifestyle choice.
Main Text|The information in this narrative has been extrapolated from a broad range of academic disciplines, such as the epidemiological and health sciences, in addition to sports nutrition literature. This is due to little information being available that discusses or investigates veganism in sport and health and fitness-related contexts. Therefore, in some instances, recommendations provided herein have yet to be fully authenticated via scientific investigation, and serve as illustrative concepts until further validation can be undertaken.EnergyFor most athletes, a well-constructed diet (omnivorous or otherwise) should provide sufficient energy in order to achieve energy balance [15]. However, data suggests that a negative energy balance is common in endurance athletes and athletes participating in weight-making and aesthetic sports (such as combat sports, gymnastics, skating and dancing, etc.) [15]. Very large athletes might also find it difficult to achieve energy balance, particularly during high-volume training phases [16, 17]. Of particular concern in sports that require low body mass, some female athletes might be at risk of developing low bone-mineral density [18]. This is likely to be exacerbated by a poorly-constructed hypocaloric diet [18]. Additionally, high intensity training can reduce appetite [19], and hectic travel schedules, poor food availability (whilst abroad or away from home) and gastrointestinal discomfort might mean that some athletes find it difficult to meet their energy requirements due to various factors [17, 20].The consequences of insufficient energy are important. Immunity might become compromised, leading to illnesses and time off from training and competition [15, 21]. Weight loss can ensue, and can lead to the loss of muscle mass, reduced strength, lower work capacity and a lack of satisfactory training adaptation [15]. Managing energy balance is thus important for all athletes, but this issue is likely to be compounded further when a habitual diet promotes early satiation and reduced appetite, such as a vegan diet [3, 4, 8–11]. Well-accepted methods of calculating energy intake include estimates such as the Cunningham or Harris-Benedict eqs. [22], Dietary Reference Intakes (DRIs) and/or other literature-based guidelines can all be used to determine nutrient needs and design diets [17]. The International Society of Sports Nutrition (ISSN) recommends that energy requirements should be scaled to activity level, body-mass and mode of exercise [16], to ensure that individual-specific needs are met [17]. Such recommendations are prudent in light of the preceding discussion, as well as the likelihood that athletes possess individual-specific energy and nutrient requirements which differ on the basis of sport, training and competition characteristics [15–17].Data indicates that vegans consume less energy than omnivores [8], and research suggests that vegetarian diets generally appear to be lower in protein, fat, vitamin B12, Riboflavin, vitamin D, calcium, iron and zinc when compared to an omnivorous diet [8, 14, 23, 24]. Table 1 details vegetarian diets as described in the literature, and highlights how the diets differ based on the extent of their restrictions. Some vegan diets promote the consumption of raw foods only, and data suggests that these diets might lead to poor macronutrient absorption and weight loss when consumed ad libitum [25]. Vegetarian and vegan diets can also lead to very high fibre consumption [14, 24, 25], and plant-based foods therefore tend to have low energy density and promote early satiety [26]. While these factors might be helpful for weight-loss purposes [27], these factors might lead to problems when trying to achieve a high Calorie diet. Where a high Calorie diet is needed, increasing feeding frequency [28] and increasing consumption of energy dense foods such as nuts, seeds and oils [29] might be helpful to ensure that Calorie goals are met. Monitoring and adjusting the diet on the basis of unwanted body mass fluctuations in such cases would also allow for diets to be tailored to individuals’ energy and nutrient requirements [30]. Table 1Vegetarian Diets: DefinitionsTypea DescriptionFlexitarianb Occasionally consumes animal flesh (meat, poultry) and fish, eggs, dairyPesco-vegetarianExcludes animal flesh but does include fishLacto-ovo vegetarianExcludes all flesh; includes diary and eggs onlyLacto vegetarianExcludes all flesh and eggs; includes dairy onlyOvo vegetarianExcludes all flesh and dairy; includes eggs onlyVeganExcludes all animal productsMacrobiotic vegetarianb Variable dietary restrictions; includes wild meat/game and fish in some variations of the dietFruitarianIncludes fruit, nuts, seeds and a some vegetablesOpen in a separate window aDefinitions from Phillips 2004 [14] bReaders are advised to exercise caution in their interpretation of Flexitarian and Macrobiotic diets as vegetarian diets; owing to their selective inclusion of meat, poultry, fish and seafood, such diets might not be truly vegetarianMacronutrientsProteinThe role of protein in the athlete’s diet has garnered much attention over the years, and there has been ongoing debate about whether athletes require greater amounts of protein than non-athletic populations [31–33]. The consensus appears to be that athletes require more protein than the lay population [33, 34]. Data also indicates that protein requirements should be tailored to reflect sport-specific and training-goal requirements [35–37]. Typical recommendations therefore include 1.6–1.7 g ∙ kg−1 ∙ day−1 for strength and power athletes and 1.2–1.4 g ∙ kg−1 ∙ day−1 for endurance-sport athletes—values notably larger than the 0.8 g ∙ kg−1 ∙ day−1 recommended for most non-active adults [36, 37]. Values as high as 4.4 g ∙ kg−1 ∙ day−1 have also been investigated in the literature recently, with favourable body-composition effects noted as a results of its composition [38].The role of protein in the athlete’s diet is multifaceted. Protein serves as a substrate for exercise performance and a catalyst for exercise adaptation [32]. The balance between Muscle Protein Breakdown (MPB) and Muscle Protein Synthesis (MPS) is known as Net Protein Balance (NPB). Achieving a positive NPB via elevated MPS promotes exercise recovery, adaptation and anabolism [32, 38, 39]. During negative energy balance adaptive mechanisms preserve Fat Free Mass (FFM) under hypocaloric conditions [33, 40]. Despite this, dieting athletes and bodybuilders might still require elevated protein intakes due to the need to preserve lean mass and promote satiety [33, 39]. Concurrent resistance and endurance training might also compound the need for extra protein during a hypocaloric diet [33, 39]. Athletes involved in weight-categorised and aesthetic sports need to be cognisant of optimizing protein intakes, where the preservation of FFM and optimization of relative strength is likely to be advantageous to performance. The ISSN provides a broad protein recommendation of 1.4–2.0 g ∙ kg ∙ day−1, which is likely to be appropriate for most athletic contexts [34]. However, for athletes in need of losing body-mass, recommendations of up to 1.8–2.7 g ∙ kg ∙ day−1 have been provided in literature [33, 39], and values as high as 2.3–3.2 g ∙ kg ∙ FFM ∙ day−1 have been suggested for bodybuilders aiming to achieve competition-level leanness [39].Vegan athletes however appear to consume less protein than their omnivorous and vegetarian counterparts [11]. The optimisation of protein intakes for vegan athlete requires that attention is paid to the quantity and quality of protein consumed [41]. Plant-based protein sources are often incomplete, missing important essential amino acids, and typically contain less Branched Chain Amino Acids (BCAA) than their animal-based equivalents [34, 35]. Leucine appears to be a primary trigger of MPS, and plays an important role in promoting recovery and adaptation from exercise [32, 34, 41]. Interestingly, evidence suggests that milk-based proteins might be superior to other protein sources at promoting MPS, mediated in part by the richness of its BCAA content [42, 43]. Similarly, the habitual consumption of milk as part of a diet and resistance-training programme might lead to better muscle hypertrophy when compared to a soy-protein-supplemented equivalent [44, 45]. This is might be due to milk’s superior amino acid composition [45]. Indeed, plant-based proteins often lack essential amino acids [46], and animal-based proteins therefore possess a greater biological value due to the presence of all essential amino acids in the food [46]. Common examples of the limiting amino acids in plant-based proteins include lysine, methionine, isoleucine, threonine and tryptophan. Of these, lysine appears to be to be most commonly absent, particularly from cereal grains [46]. Foods such as beans and legumes are rich sources of lysine however, and leucine can be obtained from soy beans and lentils. Other BCAAs can be found in seeds, tree nuts and chickpeas, meaning that these amino acids can be obtained by consuming a variety of protein-rich, plant-based foods [14, 46]. Indeed, the Academy of Nutrition and Dietetics (AND) have recommended that a range of plant-based proteins should be consumed by vegetarians in order to meet their protein and amino acid requirements [47]. Further, the once-popular recommendation of combining protein sources to achieve a complete essential amino acid profile in each feeding is no longer considered necessary [14]. Foods such as grains, legumes, nuts and seeds should be included in the vegan diet to ensure that all EAAs are present, and that adequate BCAA are consumed to support recovery and adaptation from training. Examples of high-protein vegan-friendly foods can be found in Table 2. Table 2High Protein FoodsFoodProtein per 100 ga Pumpkin seeds (dried, uncooked)30.2Lentils (red, split, uncooked)24.6Black beans (uncooked)21.6Almonds (raw)21.2Tempeh20.3Tofu (calcium set)17.3Oats (rolled)16.9Quinoa (uncooked)14.1Open in a separate window aData from USDA food composition database SR28Plant-based protein supplements that feature in the literature and are commercially available include soy (and soy isolate), pea, rice, hemp and composite/blended protein products [45–48]. Supplemental protein might be of interest to vegan athletes, particularly if achieving sufficient protein via wholefoods is either difficult or inconvenient. Emerging data is beginning to support the efficacy of plant-based-protein powders at improving recovery from training [48] and fostering muscle hypertrophy as part of a resistance training program [45]. Recent evidence also suggests like-for-like responses when comparing supplemental plant and dairy proteins on body composition and exercise performance as part of a training programme [48], contrasting previously-reported data [45]. In comparison to dairy-based protein supplements however, plant-based supplements appear to be much less researched at this time, and further research is needed to understand the effects of individual (rice, pea, hemp, etc.) and blended products on postprandial MPS [49].Protein digestibilityThe digestibility of plant-based protein appears to be markedly less than that of animal products, which might need to be accounted for when designing a vegan diet [50]. The Protein Digestibility Corrected Amino Acid Score (PDCAAS) and Digestible Indispensable Amino Acid Score (DIAAS) are metrics that rate the quality of proteins based on their digestibility [51]. The PDCAAS has been criticised for disregarding anti-nutrient factors that affect protein absorption, and for truncating protein sources that score in excess of its 1.0 ceiling [51]. The DIAAS does neither, and is perhaps a superior system for rating protein digestibility [41, 51]. Both systems however indicate that animal-derived proteins score higher than plant-based sources [51]. Interestingly, soy protein possesses a PDCAAS of 1.0 and appears to be comparable to whey protein isolate. However, when factoring in anti-nutrient factors such as phytic acid and trypsin inhibitors, which limit the absorption of nutrients, whey protein isolate appears to be superior to soy protein when using the DIAAS (1.09 vs. 0.91) [41]. Other important plant-based protein sources such as rice, peas and hemp all score markedly lower than animal-based sources such as eggs, chicken and beef using either system [41, 51, 52]. Indeed, it has been suggested that vegetarians might need to consume more protein than meat eaters to compensate for the poorer digestibility of plant-based sources [50]. Values of up to 1.0 g ∙ kg−1 ∙ day−1 (vs. the RDA’s 0.8 g ∙ kg−1 ∙ day−1) have been suggested for a non-athletic, vegetarian population, who might consume eggs and dairy products in addition to plant-based proteins [50]. Due to the absence of all animal proteins in the diet, it might be prudent for vegan athletes to aim for protein intakes towards the higher end of the ISSN’s protein recommendation of 1.4 to 2.0 g ∙ kg ∙ day−1, whilst in an energy-neutral or energy-positive state. In some instances, values of up to 1.8 g ∙ kg−1 to 2.7 g ∙ kg ∙ day−1 might be appropriate during weight loss phases, to compensate for the reduced digestibility and low biological value of plant-based sources [33, 39].CarbohydrateVegan diets tend to be higher in carbohydrates, fibre, fruits, vegetables, antioxidants and phytochemicals than omnivorous diets [53]. The consumption of micronutrient and phytochemical-rich foods is an important benefit of any plant-based diet [3, 9]. This might help to mitigate the effects of excess inflammation and promote recovery from training, although this has yet to be confirmed empirically [10, 12]. It has been suggested that some endurance athletes might intentionally adopt a vegan diet in order to meet their carbohydrate needs, or to assist weight management goals [10, 11, 54]. Carbohydrate requirements in sport has been the focus of literature debate for some time [55], and athletic diets generally require carbohydrate intakes of 4 to 12 g ∙ kg−1 to support high training volumes, depending on the mode of exercise, the athlete’s gender and goal of the athlete’s diet [17].Achieving an adequate carbohydrate intake via a vegan diet is relatively straightforward, and grains, legumes, beans, tubers, root vegetables and fruits can all be consumed to meet carbohydrate requirements satisfactorily. In order to achieve sufficient protein via the consumption of whole foods as recommended in this article, it is recommended that vegans consume beans, pulses, lentils and grains daily—foods that are also abundant in carbohydrate. However, recall that these foodstuffs are rich sources of fibre. Fibrous, non-digestible carbohydrates and lignin provide volume and bulk, are resistant to digestion and absorption, and promote early satiation and enhance prolonged satiety signalling [47, 56, 57]. For athletes requiring higher energy intakes, the consumption of fibre-rich foods to achieve protein and carbohydrate adequacy might prove to be difficult for some. Due to the lectins in foods such as beans, grains, nuts and potatoes [58], as well as the fermentation of resistant starch and indigestible carbohydrates (found in oats, peas, beans, fruits, and in certain vegetables and lentils), a high-fibre diet can also promote gastric distress in some cases [38, 59, 60]. In order to achieve sufficient carbohydrate for the athletes involved in high-volume training phases it might be appropriate (in some contexts) to choose some lower-fibre foods when developing high-carbohydrate meals, as long as sufficient micronutrient status (particularly B vitamins) can be ensured. Foods such as rice, pasta, noodles and buckwheat contain less fibre than oats, lentils, beans and wholegrain breads, and removing the skin from tubers and root vegetables reduces the fibre contents of these foods whilst maintaining decent carbohydrate levels.Carbohydrate timing and supplementationThe optimisation of carbohydrate consumption with respect to training and competition has been debated in the literature [61]. Conventional wisdom suggests that maintaining high muscle glycogen stores (achieved via a carbohydrate-rich diet), consuming carbohydrates before and during exercise (scaled to absolute intensity and mode of exercise), consuming multiple-transportable carbohydrates (such as a glucose-fructose mixture), and carbohydrate mouth rinsing (to delay fatigue) might enhance performance during middle-distance and endurance events [37, 62]. The effects of carbohydrate consumption before and during short-duration high-intensity sports are equivocal [36], however carbohydrate feedings 0–60 min prior to exercise have been indicated for events lasting >30 mins [37, 62]. Carbohydrate doses during activity can be scaled based on the event, where more/less carbohydrate is consumed with respect to the time and intensity of sport/exercise performance [37, 62]. In most cases, carbohydrate supplements appear to be vegan-friendly, and so their consumption is feasible for most vegan athletes. Consuming calcium-fortified fruit juices as a liquid carbohydrate might serve dual purposes however, and enable vegans to meet both carbohydrate and calcium needs whilst concomitantly offering possible ergogenic advantages if used as indicated in the literature [61].FatVegan diets are typically lower in total and saturated fat and higher in n-6 fats than omnivorous and vegetarian diets [8, 13, 63]. This trend appears to be associated with reductions in heart disease, hypertension, type II diabetes, cholesterol and cancer [63], and is a purported health benefit of veganism. However, the role of fat in the diet is an area of much discussion, and deleterious effects of fat consumption are not universally accepted [64–66]. Indeed, in some cases, high-fat diets have even been promoted [55]. Interestingly, research has indicated that low-fat dieting might negatively influence testosterone levels in males [67]. This might be of interest to athletes needing to maximise anabolism and adaptation to resistance training. However, despite reporting lower total and saturated fat intakes, evidence has also suggested that vegan males do not have statistically lower androgen levels than omnivores [68]. Relationships between fat consumption, hormones and sport performance might require additional investigation. In many instances, it appears that the health implications of a dietary fat might reflect its fatty acid composition [64, 69], meaning that attention should be paid to the quantity and quality of fat consumed. Achieving recommended values of 0.5–1.5 g ∙ kg ∙ day−1 (or 30% of daily caloric intake) is feasible for vegan athletes given adequate consumption of oils, avocados, nuts and seeds.ALA, EPA and DHADue to an absence of marine-sourced fats, vegans appear to consume fewer n-3 fatty acids and possess lower serum n-3 fatty acid levels than omnivores and other vegetarians [8, 13, 63, 70]. This might have important health and performance implications. The n-3 fatty acids are important for normal growth and development, and appear to play an important role in cardiovascular health [71], in inflammatory and chronic disease [72], and might improve exercise-induced bronchoconstriction (EIB) and immunity [73]. Of interest to athletes, n-3 fats might also increase nitric oxide production [71, 74], and improve heart-rate variability [75]. Both n-6 and n-3 fatty acids are parent fatty acids for eicosanoids (prostaglandins, thromboxanes and leukotrienes), and n-3 fatty acids appear to possess anti-inflammatory, antithrombotic, antiarrhythmic; hypolipidemic, vasodilatory and antiproliferative properties [71, 72]. Both n-6 and n-3 fatty acids are essential, however the long chain n-3 fatty acids eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA) are considered to be under-consumed in the modern western diet in general [72], and in vegans in particular [47, 70].There is ongoing debate about the quantity and/or ratio of n-3 to n-6 needed to manipulate the synthesis of pro and anti-inflammatory eicosanoids in order to impact health and performance favourably [76–78]. In the UK, an upper limit of 10% of energy from pro-inflammatory n-6 fatty acids has been recommended by the Department of Health to reduce negative effects of overconsumption [78]. Elsewhere, Sanders [79] and Philips [14] recommend that vegetarian diets limit linoleic acid consumption (an n-6 fatty acid), found in sunflower, corn and safflower oils, for similar purposes. The n-3 α-linolenic acid (ALA) is an important constituent of cellular membrane and is converted to EPA at ~8% efficiency in humans, which appears to be both age and gender-specific in magnitude [80]. Roughly 0.5% of ALA is converted to DHA, highlighting humans’ poor ability to enzymatically convert ALA to this fatty acid [81]. While humans do convert a small amount of ALA to DHA, the primary source of this in the diet is cold water fish and seafood. EPA and DHA exert many of the reported health and performance benefits of n-3 fatty acid consumption and is now a popular supplement [80]. Supplemental ALA has been shown to increase blood EPA levels [82] but does not appear to affect DHA status [83]. Microalgae oil is rich in DHA (and EPA) and might be a useful supplement for vegans and vegetarians. Microalgae-oil supplements have been shown to raise both blood EPA and DHA levels [84]. However, recommendations for vegan-friendly DHA supplements do not appear in the literature at this time [9]. Recommendations do appear for other food sources of the n-3 ALA, such as flax seeds, walnuts, and chia seeds [9, 14]. Interestingly, flax/linseeds are also rich in lignan precursors, which might offer broader health-related benefits [85], and chia is also a complete protein [86]. Combining whole-food sources of ALA as indicated in this article with supplemental DHA derived from microalgae oil might optimise a vegan’s n-3 fatty acid intake, and improve health concurrently with any health and performance-enhancing effect that augmented n-3 diets might offer athletes [76, 77]. Research detailing how to optimise n-3 consumption for vegans is missing at the time of writing; however, recommendations of 1–2 g ∙ day−1 of combined EPA and DHA at a ratio of 2:1 have been suggested for athletes elsewhere [77]. To achieve a DHA dose of 500 - 1000 mg ∙ day−1, this would equate to 1–2 g of microalgae oil, or 2–4 capsules in most commercial products.MicronutrientsAchieving micronutrient sufficiency is an important concern for all athletes. The AND have indicated that attention should be paid to achieving adequacy in vitamin B12, iron, zinc, calcium, iodine and vitamin D intakes when designing a vegan diet in particular [47]. Poorly designed diets might predispose individuals to deficiency regardless of predilection, which could have detrimental health and performance implications [2, 9, 12]. This needs to be understood by those seeking to adopt veganism, and strategies to mitigate the risks of under-consuming these nutrients need to be present if a vegan diet is to optimize health and performance. Table 3 compares the nutritional implications of several diets (omnivorous, pesco-vegetarian, vegetarian and vegan), and provides recommendations for athletes and practitioners. The following section will identify and elaborate upon concerns highlighted in the literature, based upon research indicating what micronutrients might be under-consumed in a vegan diet [2–4, 8, 14, 47, 63, 70, 87]. Table 3Diet ComparisonDiet typePossible dietary Issuesa Possible sport-related issuesa Recommendationsb OmnivorousPoor ad libitum diets can lead to nutrient deficiency.Vitamin D deficiency possible (if sun exposure is poor / unlikely).Male and female athletes with low energy intake at risk of nutrient deficiencies.Calcium requirements increased during negative energy balance, amenorrhea and female athlete triad.Energy intake should be scaled to activity level.Depending on sport, 1.4–2.0 g ∙ kg−1 protein; 3–10 g ∙ kg−1 CHO; 0.5–1.5 g ∙ kg−1 fat (or, 30% energy) consumed daily.Micronutrient-rich diet sufficient to achieve DRVs; Vitamin D3 supplement might be necessary.Pesco-vegetarianSame as omnivores plus:Energyc, protein.Iron deficiency with and without anaemia a risk in female athletes.Same as omnivores, plus ensure that iron needs are met via a variety of food sources.Lacto-ovo vegetarian & Lacto-vegetarianSame as pesco-vegetarians plus:Long chain n-3 (EPA, DHA), iron, zinc, riboflavin deficiencies more likely.Same as pesco-vegetarians plus:Reduced muscle creatine and carnosine stores a possibility in males and females.Same as pesco-vegetarians plus:EPA / DHA supplement (total 1–2 g ∙ day−1; 2:1 ratio) might be needed.Increase iron (m = 14 mg & f = 33 mg ∙ day1) and zinc (16.5 mg & 12 mg ∙ day1) intakes due to reduced bioavailability of plant sources.VeganSame as vegetarians plus:Protein, fat, n-3, B12, calcium, iodine deficiencies also possible / likely in males and females.Same as vegetarians plus:Low bone-mineral density is an increased possibility in female athletes.Achieving energy balance might be a problem for larger athletes.Same as vegetarians plus:Increase protein to 1.7–2.0 g ∙ kg−1 and up to 1.8–2.7 g ∙ kg−1 during weight loss phases (obtain from range of plant-based foods).Nuts, seeds, avocados, oils to achieve 0.5–1.5 g ∙ kg−1 fat daily.EPA / DHA (microalgae); vitamin D3 (lichen) & B12 supplements might be needed; iodine in some instances too.1000 mg ∙ day−1 calcium from beans, pulses, fortified foods and vegetables.Open in a separate window aData from various sources [8–11, 13, 14, 23–25, 47, 63, 70, 87] bRecommendations from various sources [9–11, 16, 17, 22, 47] cEnergy balance a potential issue in endurance, weight-making and aesthetic sports and larger athletes regardless of diet [15]Vitamin B12Due to an absence of animal and dairy products, vegans are at an increased risk of developing Vitamin B12 (cobalamin) deficiency [87]. Cobalamin is synthesised from anaerobic microorganisms, in the rumen of cattle and sheep, and humans typically consume pre-formed cobalamin from animal products, which are the main source of B12 in the diet [88]. Plant-based sources of cobalamin are unusual, unless the plant has been contaminated by manure or from animal waste [47, 88]. Cobalamin is essential for normal nervous system function, homocysteine metabolism and DNA synthesis [88]. Insufficient cobalamin can lead to morphological changes to the blood cells and the development of haematological and neurological symptoms, such as megaloblastic anaemia and neuropathy [89]. Long-term cobalamin deficiency can lead to irreversible neurological damage, and data indicates that veganism can lead to deficiency if cobalamin is not supplemented [14]. Data from the EPIC-Oxford cohort study in the UK indicated that ~50% of vegan participants were vitamin B12 deficient [90]. An additional 21% of the vegans were also classified as having very low levels. Interestingly, despite 20% of participants consuming a B12 supplement, blood-vitamin levels between those that did vs. those that did not take supplements were no different, suggesting that the supplementation practices of the cohort were inadequate to achieve B12 sufficiency. Sources of vitamin B12 suitable for a vegan diet include B12-fortified breakfast cereals and nutritional yeast, as well as dietary supplements. Supplemental vitamin B12 products typically contain cyanocobalamin, although other forms such as methylcobalamin and hydroxocobalamin are available—the latter by prescription only. The body appears to have a limited capacity to absorb vitamin B12 supplements orally [88, 89], which is limited by the presence of intrinsic factor, a glycoprotein secreted by the stomach’s parietal cells that combines with B12 prior to absorption in the distal ileum via receptor-mediated endocytosis [89]. For an ingested 500 μg oral supplement, only an approximated 10 μg might be absorbed [89]. Because of this poor bioavailability, sublingual drops, lozenges and transdermal products have been developed and marketed under the pretence that they offer better absorption, however research supporting these claims could not be found when writing this article. The requirement for vegans to supplement with vitamin B12 is important, and vegans are advised to consume fortified foods and/or take a daily supplement to ensure an adequate intake of the vitamin [9, 14]. The Dietary Reference Intake (DRI) for vitamin B12 is 2.4 μg ∙ day−1 for adults of both sexes [91], and vegans have been advised to consume up to 6 μg ∙ day− 1 of supplemental B12 by some authors [10]. Where adequate B12 status cannot be achieved via oral supplementation and fortified food products alone, vegans might need to have serum levels monitored by a medical practitioner if deficiency is suspected [87]; subcutaneous or intramuscular injections might even be indicated in some contexts [87]; monitoring B12 status carefully might be necessary for some vegan athletes.IronThe iron status of vegetarians and vegans has received attention in the literature [92–94], and it appears that owing to a diet rich in whole-grains and legumes, both vegetarians and vegans consume similar amounts of iron as omnivores [9, 63]. However, issues with the bioavailability of plant-based iron might mean that vegans need to pay attention to ensuring that sufficiency is prioritized [92, 93]. The main source of iron in the vegan diet is found in the non-haem form, which is less bioavailable than the haem iron found in animal products [93]. Vegan diets also commonly contain dietary inhibitors such as the polyphenols tannin (found in coffee, tea, and cocoa) and phytates (found in whole grains and legumes), which reduce the amount of iron absorbed from the diet. Research into the iron status of vegans has found that female vegans appear to have lower iron stores than omnivores, and are more prone to iron-deficiency anaemia [63, 94, 95]. Male vegans appear to have a similar iron status as non-vegans and are less impacted by iron status [63]. Iron-deficiency anaemia is caused by insufficient consumption of iron (or insufficient absorption of iron) and is a decrease in red blood cells (RBCs) or haemoglobin, leading to symptoms such as tiredness and fatigue; weakness, shortness of breath and reduced exercise tolerance [95]. Iron deficiency without anaemia has also been shown to reduce endurance capacity, increase energy expenditure and impair adaptation to endurance exercise in females experiencing tissue depletion [96]. Supplementation has been shown to correct such problems and might be warranted if adequacy cannot be achieved via the diet (97). Indeed, achieving an iron-sufficient diet appears to be rudimentary for all female athletes [95–97].Interestingly, however, it has been suggested that the body can regulate iron absorption based upon blood concentrations of the mineral [14]. Low iron status can lead to intestinal adaptations that increase absorption and reduce secretion in order to maintain equilibrium [14]—an effect that appears to be present with other important micronutrients discussed in this article [3, 97]. It appears that humans can adapt to a wide range of iron statuses and intakes, and vegetarians and vegans generally do not appear to suffer adverse health effects because of reduced iron absorption [98]. Hunt [93], however, recommends that iron intakes for vegetarians be increased by 80%, so that adult males and females achieve a recommended intake of 14 mg ∙ day−1 and 33 mg ∙ day−1 (vs. the Recommended Daily Allowance’s 8 mg ∙ day−1 and 18 mg ∙ day−1), due to the aforementioned bioavailability issues. The Institute of Medicine (IOM) concur, and suggest that iron requirements for vegetarians are 1.8 times greater than omnivores [92]. Elevated intakes of iron for vegetarians and vegans have been refuted however on the basis that high iron intakes might increase susceptibility to heart disease and cancer [99], and that supplemental iron might affect the bioavailability of other minerals and copper [14]. Indeed, it has also been suggested that such recommendations have exaggerated requirements by basing recommendations off of acute feeding studies, where the effects of iron inhibitors and enhancers might have been artificially pronounced [3, 92, 100]. Non-haem iron absorption can be enhanced (as well as inhibited), and consuming non-haem iron-rich foods in conjunction with vitamin C appears to increase absorption [9, 92]. Vegan athletes should therefore look to achieve iron sufficiency by choosing wholefood iron sources, reducing their consumption of inhibitor-containing foodstuffs such as tea, coffee and cocoa (when eating iron-rich meals), consume vitamin C containing foods concurrently to enhance absorption, and incorporate soaked, sprouted and/or fermented foods in their diets, if palatable. In cases of where individuals might be prone to iron deficiency, i.e. females with large menstrual blood losses, monitoring iron status and considering supplementation might be necessary. A list of food sources for iron and other nutrients discussed in this article can be found in Table 4. Table 4Vegan-Friendly Food SourcesNutrientVegan-friendly sourcesProteinPulses, grains, legumes, tofu, quinoa, nuts, seeds, vegetablesALAFlax seeds, walnuts, chia seeds, hemp seedsEPAa Seaweed, algaeDHAMicroalgae oil, seaweedVitamin B12Supplements, fortified foods, plant milks, nutritional yeast (fortified), fermented soyb, mushroomsb IronLegumes, grains, nuts, seeds, fortified foods, green vegetablesZincBeans, nuts, seeds, oats, wheat germ, nutritional yeastCalciumTofu (calcium set), fortified plant milks and juice, kale, broccoli, sprouts, cauliflower, bok choiIodineSeaweed, cranberries, potatoes, prunes, navy beans, iodized saltVitamin DLichen-derived D3 supplementsOpen in a separate window aEPA can also be enzymatically converted from ALA and retroconverted from DHA [83, 84] bMight not be a reliable source of this nutrientZincZinc is a constituent of enzymes involved in metabolic processes that relate to DNA stabilisation and gene expression, and is important in cell growth, repair and protein metabolism [92]. Similar to iron, zinc is widely available in plant-based foods but is also not readily absorbed [93]. Similarly as well, the body appears to adapt to lower intakes of zinc by reducing losses and increasing absorption in order to maintain equilibrium [3, 97]. It has been suggested therefore that vegetarians do not need to pay special attention to consuming this mineral [3]. However, the IOM have suggested that vegetarians might need to consume up to 50% more zinc than non-vegetarians owing to its poor bioavailability [92]. Indeed, common vegan sources of zinc include beans, whole grains, nuts and seeds (Table ​(Table4)—foods4)—foods that also contain phytate [93]. However, processing foods can reduce phytate too. Leavening bread activates phytase, breaking down phytic acid, and soaking, fermenting and sprouting nuts and grains can all reduce phytate levels and increase nutrient bioavailability [101]. Based on the IOM’s suggestion, it has been recommended that male vegans consume up to 16.5 mg ∙ day−1 of zinc (vs. the RDA of 11 mg ∙ day−1) and females up to 12 mg ∙ day−1 (vs. 8 mg ∙ day−1) [10]. Zinc bioavailability appears to be enhanced by dietary protein and inhibited by supplemental folic acid, iron, calcium, copper and magnesium, but might not be affected by the whole-food sources of these nutrients [101]. In order to achieve the above recommendations, vegans should look to consume zinc-rich foods such as hemp and pumpkin seeds, and other grains, nuts and beans (Table ​(Table4),4), and look to adopt processing methods that improve mineral absorption, such as soaking and fermenting, as suggested earlier. If this is not achievable, a supplement should be considered. Owing to issues concerning bioavailability, zinc supplements should not be consumed at the same time as supplemental forms of the aforementioned minerals. Multivitamin/mineral formulas might be inadequate if to ensure nutrient adequacy in this instance.CalciumCalcium is abundant in a wide range of foodstuffs, most notably dairy products. Data indicates that vegans consume less calcium than omnivores and other vegetarians [63]. Indeed, Canadian vegans have been shown to consume only 578 mg ∙ day−1 compared with the 950 mg ∙ day−1 and 875 mg ∙ day−1 of omnivores and ovo-lacto vegetarians [102]. Vegans have been shown to be at a higher risk of fracture due to lower calcium intakes [103]. Low intakes of calcium are particularly problematic for children and teenagers, where higher calcium requirements are required for bone development [78, 104]. As with other minerals, the body appears to be able to regulate calcium status during periods of low consumption. When habitual calcium intakes are low, and when sufficient vitamin D is present, an increased proportion of calcium is absorbed from food [104]. It has been suggested that lower protein intakes typical of a vegan diet might contribute to greater calcium retention due to high-protein diets promoting calcium excretion in the urine [104, 105]. However, evidence demonstrates that protein-rich diets have no effect on calcium retention [105], and in some instances work synergistically with calcium to improve calcium retention and bone metabolism [34, 105]. It is widely recommended that adequate calcium is necessary for blood clotting, nerve transmission, muscle stimulation, vitamin D metabolism and maintaining bone structure [106]. Indeed, the importance of calcium for the vegan athlete reflects its role in the maintenance of skeletal health during weight-bearing exercise, and increased calcium losses experienced during heavy perspiration [107]. Calcium requirements might also be exacerbated during phases of calorie restriction, amenorrhea and in some instances of the female athlete triad [107]. However, it is also proposed that the RDA for calcium (1000 mg ∙ day−1) is sufficient to meet the requirements of athletic populations in most contexts, and so despite the aforesaid factors, it has been suggested that athletes do not have an elevated requirement for the nutrient in general [107].In order to meet the above requirement, vegan athletes should consume plant-based sources of calcium such as beans, pulses and green vegetables in sufficient quantities to achieve the 1000 mg ∙ day−1 recommendation [106]. Broccoli, bok choy and kale are particularly high in calcium; green vegetables such as spinach and arugula contain oxalate however, which impedes calcium absorption [104]. Vegans therefore should choose plant sources that contain low oxalate levels when designing calcium-rich meals. Calcium-fortified foods are also widely available, and examples such as calcium-fortified soy, nut milks and fruit juices are all vegan-friendly and provide readily absorbable forms of the nutrient (Table ​(Table4).4). Vegans can also consume calcium-set tofu, which is also rich in protein, to help achieve their requirements if palatable. If a vegan diet cannot achieve sufficient calcium levels, then a supplement might also be required [14].IodineIodine is an essential trace element needed for physical and mental growth and development, and plays in an important role in thyroid function and metabolism [92]. Excessively high or low intakes of iodine can lead to thyroid dysfunction, and vegans have been shown to consume both excessively high and low intakes depending on their dietary choices [108, 109]. To highlight, a study by Krajcovicova-Kudlackova and colleagues [110] found that 80% of Slovakian vegans were iodine deficient. Lightowler and Davies [109] however found that some vegans consumed excess iodine (from seaweed) in their study. Common iodine sources include fish and dairy products, and the DRI for Iodine has been set at 150 μg ∙ day−1 for adults [92]. Iodine content in foods vary according to the soil-iodine content (when growing produce), the farming methods used during production, the season it is grown in, and the species of fish (if non vegan) [111]. Goitrogens, found in cruciferous vegetables such as cabbage, cauliflower and rutabaga decrease iodine utilisation and might affect adversely thyroid function if consumed in large amounts [111]. However, cooking such foods appears to destroy many of the goitrogenic compounds present, making this effect unlikely. Raw-food vegans should look to limit the consumption of raw, goitrogenic foods where possible.Seaweed and sea vegetables are a concentrated source of iodine that are vegan-friendly. Excessively high iodine intakes have been reported in vegans who regularly consume seaweed however [109, 112], and in some cases have led to elevated Thyroid Stimulating Hormone (TSH) levels [113, 114]. Elevated TSH might reflect iodine-induced hyperthyroidism or iodine-induced hypothyroidism [114]. However, iodine intakes above 150 μg ∙ day−1 appear to be well tolerated, unless clinical susceptibilities to thyroid issues are present [115]. Iodine concentrations in seaweed can vary markedly [115], and the British Dietetic Association suggests that seaweed might not be a reliable iodine source [116]. Iodized table salt has however been indicated for vegans looking to achieve sufficient intakes [14], and iodine can be also found in foods such as potatoes, breads (in some countries) and cranberries (Table ​(Table4).4). Examples of how to achieve sufficient iodine levels (and for other nutrients discussed in this article) can be seen in Tables 5 and ​and6,6, which provide menus based on a 2500 and 3500 Calorie requirement. Where iodine sufficiency cannot be achieved through food alone, a supplement that meets the 150 μg ∙ day−1 recommendation might be advisable. Table 5Sample 2500 Calorie menua MealIngredientsBreakfast: “Nutty Banana Oatmeal” • Oats, ½ cup (uncooked)• Almond milk, fortified, unsweetened, 1 cup• Banana, 1 small (60 g)• Brazil nuts, 1nut• Flax seeds, milled, 1 tablespoon• Pumpkin seeds, 1 tablespoon• 1000 IU Vitamin D3 (lichen derived)• 2.4 μg Vitamin B12• 2 g microalgae oil supplementLunch / Pre-training: “Asian-style Kale Salad” • Kale, 1 cup, chopped• Carrot, 1 medium, shredded• Cucumber, ½ cup, shredded• Dried wakame, 1 g, sprinkles• Rice noodles, 57 g (uncooked weight)• Edamame beans, ½ cup• Tofu, calcium-set, ½ cup• Ginger, garlic, chili, to taste• Sesame oil, 1 tablespoonPost Training: “Mixed berry protein smoothie” • Pea protein isolate, 40 g• Frozen raspberries, ½ cup• Frozen strawberries, ½ cup• Frozen blueberries, ½ cup• Hemp milk, fortified, 1 cupDinner: “Garbanzo and Sweet potato Curry” • Garbanzos, 120 g, cooked• Seitan, 40 g• Sweet potatoes, 1 medium• Tomatoes, ½ can• Onion, 1 red, small• Garlic, 1 clove• Olive oil, 1 tbsp.• Long grain rice, 1 cup• Curry spices, to tasteTotal Energy2512 CaloriesProtein154 gCarbohydrate312 gTotal Fat75 g n-34.4 gALA1597 mgEPA300 mgDHA529 mgFibre (g)58 gVitamin B123.4 μgIron31.4 mgZinc15.4 mgCalcium2226 mgIodine173 μgVitamin D34.5 μgOpen in a separate window aBased on a 77 kg male gym goer; diet created using Nutritics (Nutritics Limited, Dublin, Ireland)Table 6Sample 3500 Calorie menua MealIngredientsBreakfast: “Tofu scramble with avocado toast” • Tofu, ½ block / 250 g• Whole-wheat bread, toasted, 3 large slices• Avocado, ¼, diced• Navel orange, 1 medium• 2 g microalgae oil supplementLunch / pre training: “Roast vegetable, tempeh & buckwheat salad” • Buckwheat, raw, 1 cup• Tempeh, ¾ cup / 100 g• Zucchini, ½, roasted• Eggplant, ½, roasted• Onion, red, ½, roasted• Bell pepper, ½, roasted• Olive oil, balsamic vinegar, 1 tbsp. EachPost-training snack: “Spiced apple cereal” • Puffed rice, 2 cups• Apple, 1 large• Prunes, chopped, 18 g• Cinnamon, to taste• Soy milk, unsweetened, 270 ml• Rice protein, vanilla, 1.5 scoops / 35 gDinner:“Spaghetti” • Black bean spaghetti, 75 g, raw weight• Spaghetti, dried, 88 g, raw weight• Pasta sauce, tomato, low sodium, ½ jar• Onion, red, ½• Spinach, frozen, 70 g• Garlic, basil, to taste• Olive oil, 1 tbsp.• Nutritional yeast flakes (B12 enriched), 1 tbsp.Snack: “Chia, flax and hemp seed dessert” • Chia seeds, flax seeds, hemp seeds, 2 tbsp. Each• Almond milk, 1 cup• Dates, chopped, × 2• Strawberries, ½ cupTotal Energy3446 CaloriesProtein193 gCarbohydrate443 gTotal Fat94 g n-39.8 g ALA7754 mg EPA300 mg DHA529 mgFibre (g)91 gVitamin B124.2 μgIron42 mgZinc40 mgCalcium2406 mgIodine153 μgVitamin D22.7 μgOpen in a separate window aBased on a 94 kg male strength athlete; diet created using Nutritics (Nutritics Limited, Dublin, Ireland)Vitamin DVitamin D is a fat-soluble vitamin produced in the skin, is essential for calcium absorption and bone health, and plays an important role in many physiological processes [117]. While humans synthesize vitamin D from exposure to sunlight, vitamin D can also be found in animal products and fortified foods [117]. Dietary intakes of vitamin D appear to be low in vegans who do not achieve sufficient sun exposure [118]. Cholecalciferol (D3) is an animal-derived version of vitamin D that is now widely available as a supplement [119]. Ergocalciferol [D2] is a vegan-friendly version of vitamin D but appears to be less bioavailable than cholecalciferol [119, 120]. Recently, however, vegan-friendly versions of cholecalciferol derived from lichen, a composite fungal-algae organism, have become commercially available, offering vegans a more bioavailable supplemental option. These supplements appear to be dosed similarly to animal-derived products, with dosages of 200–1000 IU per serving being common, and can be used as a like-for-like equivalent for animal-based counterparts.In the USA, the IOM recommend an RDA of 600 I.U ∙ day−1 [117] for vitamin D. In the UK the Department of Health recommend 10 μg ∙ day−1 (400 I.U) is supplemented by individuals who do not achieve adequate sun exposure [121]. Of interest to athletes, Cannell et al. [122] suggest that optimising vitamin D status might improve athletic performance, if deficiency is present. Indeed, Moran et al. [123] highlight that poor vitamin D status negatively affects muscle strength and oxygen consumption, and suggest that supplementation might protect against overuse injury via its role in calcium metabolism and skeletal muscle function. Optimising vitamin D status is perhaps an important consideration for all athletes, regardless of dietary choice [124].Effective vitamin D dosing might necessitate that supplementation is optimized via bespoke treatment strategies [125], based on individuals’ existing blood levels. In order to determine vitamin D status, plasma 25OHD levels can be sampled. Values <20 ng ∙ ml−1 are considered to be clinically deficient [126]. Optimal values might fall between 40 and 70 ng ∙ ml−1 [126, 127]. Recommendations for supplementation provided by Dalqhuist and colleagues [128] suggest that athletes should aim to achieve plasma 25OHD levels in the range of 30–40 ng ∙ ml−1. Dalqhuist and team [128] also suggest that supplemental doses of up to 4–5000 IU ∙ day−1 coupled with 50–100 μg ∙ day−1 of vitamin K1 and K2 could improve exercise recovery, allowing athletes to train more frequently. Data concerning performance-enhancing effects and toxicity at higher dosages of vitamin D have yet to be demonstrated, however a tolerable upper intake of 4000 IU ∙ day−1 has been established by the IOM [106]. Blood 25OHD values >150 ng ∙ ml−1 (plus hypercalcemia) are generally thought to signify toxicity [126]. Further research is warranted to determine optimal vitamin D doses for athletes.Supplements and ergogenic aidsCreatineResearch indicates that vegetarian and vegan diets reduce muscle creatine stores [129–131]. Creatine is a nitrogeneous, organic acid synthesized endogenously from arginine, glycine and methionine [132]. Foods such as meat, fish and poultry are rich sources of creatine but are excluded from a vegan diet. Creatine’s performance-enhancing effects have been well studied, and it appears that supplementation can improve short-term high-intensity exercise performance, muscle hypertrophy and maximal strength [132, 133]. Creatine supplementation might also lead to increased plasma volume, improved glycogen storage, improved ventilatory threshold, and reduce oxygen consumption during submaximal exercise [133]. Interestingly, data indicates that creatine supplementation might be most beneficial for athletes with low pre-existing muscle creatine stores. To highlight, Burke et al. [129] found that supplemental creatine attenuated low muscle creatine stores in vegetarians, who experienced greater improvements in FFM, maximal strength and type II muscle fibre area compared to omnivores. Creatine supplementation might therefore be an important ergogenic aid for vegan athletes to consider, and compensate for reduced muscle creatine stores experienced as a result of their lifestyle choices.Dosing creatine effectively requires the achievement of muscle creatine saturation, and regimens of 20 g ∙ day−1 for 3–7 days to load creatine followed by maintenance doses of 3–5 g ∙ day−1 are common [132]. However, a smaller dose of 3–5 g ∙ day−1 taken over a 4-week period will achieve creatine saturation over the long term similarly [134]. Other protocols, such as 1 g ∙ 30 min−1 over 20 intakes per day, have also been suggested as means to achieve maximal saturation [135]. The co-ingestion of creatine with protein and carbohydrate might increase creatine retention by way of insulin-mediated storage, but appears not to have any noticeable performance-enhancing effects beyond stand-alone ingestion [133]. For vegan athletes who decide to supplement, powder forms of synthetic creatine are vegan-friendly (capsulated products might contain bovine gelatine), and the co-ingestion of creatine with whole food and/or a protein and carbohydrate mixture might be an optimal way of achieving creatine storage.Beta alanineSimilar to muscle creatine levels, evidence also indicates that vegetarians have lower levels of muscle carnosine compared to omnivores [136, 137]. Carnosine, an intracellular proton buffer and antioxidant, is a cytoplasmic dipeptide (β-alanyl-l-histidine) found in skeletal muscle and the central nervous system, and is synthesised in situ from its rate-limiting precursor β-alanine [136]. Meat and poultry are the main sources of β-alanine in the diet, and β-alanine supplementation has been shown to increase muscle carnosine concentrations, leading to improvements in high-intensity exercise performance by way of buffering excess protons, scavenging free radicals, chelating transition metals and reducing fatigue [138, 139]. Achieving muscle carnosine saturation appears to be an important factor in effective β-alanine dosing, and research validates the efficacy of loading β-alanine in divided doses of 4–6 g ∙ day−1 for 2–4 weeks [139]. The efficacy of β-alanine supplementation has been confirmed in exercise >60 s duration, in aerobic exercise, and in attenuating muscle fatigue and improving time-trial performance in high-intensity exercise [139]. The effect of supplementation in exercise lasting <60 s is unclear however. Owing to muscle carnosine levels being lower in vegetarians than omnivores [137], it is feasible that the efficacy of β-alanine supplementation might also be augmented in vegans. Further research is necessary to validate this hypothesis however.Taurine and β-alanine share transport mechanisms, meaning that supplemental β-alanine might theoretically inhibit taurine uptake in skeletal muscle [139, 140]. Taurine is a sulphur-containing amino acid that appears to play a role in many important physiological processes in humans, including bile acid conjugation, cardiovascular function, neurotransmission and euglycemia, and is obtained from seafood, meat and dairy products [140, 141]. Vegans have been shown to consume negligible amounts of taurine [142], which is conditionally essential in some clinical contexts [141]. It has been suggested that vegans might benefit from taurine supplements owing to its absence in the vegan diet [10]. However, further support for this recommendation could not be found in the literature located for this article. If indeed supplemental β-alanine does lead to reductions in taurine in humans, then vegans might be at greater risk of experiencing taurine depletion due to its absence from the diet. However, it must be noted that β-alanine has not been shown to reduce taurine levels in humans to date, and is considered to be safe when used within the parameters of recommended dosing [139].The primary limitation of this review is the lack of research into veganism in sport. To mitigate this issue, information was gathered for this review from multiple sources, and inferences were made from the available data and via reasoned judgements. As such, many of the recommendations in this article require authentication, and so this article should serve as a catalyst for future research as well as a guidance document for athletes and practitioners. The main strength of this review is its comprehensiveness.
Conclusions|In general, vegan diets tend to be lower in Calories, protein, fat, vitamin B12, n-3 fats, calcium and iodine than omnivorous diets, whilst concurrently being higher in carbohydrates, fibre, micronutrients, phytochemicals and antioxidants. Achieving a high energy intake is difficult in some instances, owing to plant-based foods promoting satiety. Issues with the digestibility and absorption of nutrients such as protein, calcium, iron and zinc might be an issue too, meaning that athletes might need to consume higher amounts of these foods compared to omnivores and other vegetarians. However, through the strategic selection and management of food choices, and with special attention being paid to the achievement of energy, macro and micronutrient recommendations, along with appropriate supplementation, a vegan diet can achieve the needs of most athletes satisfactorily. Supplementation with creatine and β-alanine might offer augmented performance-enhancing effects in vegans, who experience low pre-existing levels of these substances, and further research is needed to investigate the performance-enhancing effects of these substances in vegan populations. For some, a vegan diet is the manifestation of important ethical beliefs, and requires diligence to sustain [5–7]. It is a central tenet of this article that similar conscientiousness needs be paid to achieving dietary sufficiency, otherwise health and performance could suffer over the long term if an individual’s nutrition is not managed appropriately.
Abstract|Severe acute respiratory syndrome–coronavirus 2 (SARS-CoV-2) causes the infectious disease COVID-19 (coronavirus disease 2019), which was first reported in Wuhan, China, in December 2019. Despite extensive efforts to control the disease, COVID-19 has now spread to more than 100 countries and caused a global pandemic. SARS-CoV-2 is thought to have originated in bats; however, the intermediate animal sources of the virus are unknown. In this study, we investigated the susceptibility of ferrets and animals in close contact with humans to SARS-CoV-2. We found that SARS-CoV-2 replicates poorly in dogs, pigs, chickens, and ducks, but ferrets and cats are permissive to infection. Additionally, cats are susceptible to airborne transmission. Our study provides insights into the animal models for SARS-CoV-2 and animal management for COVID-19 control.
Abstract|Coronavirus disease 2019 (COVID-19) is an acute infection of the respiratory tract that emerged in late 20191,2. Initial outbreaks in China involved 13.8% of cases with severe courses, and 6.1% of cases with critical courses3. This severe presentation may result from the virus using a virus receptor that is expressed predominantly in the lung2,4; the same receptor tropism is thought to have determined the pathogenicity—but also aided in the control—of severe acute respiratory syndrome (SARS) in 20035. However, there are reports of cases of COVID-19 in which the patient shows mild upper respiratory tract symptoms, which suggests the potential for pre- or oligosymptomatic transmission6,7,8. There is an urgent need for information on virus replication, immunity and infectivity in specific sites of the body. Here we report a detailed virological analysis of nine cases of COVID-19 that provides proof of active virus replication in tissues of the upper respiratory tract. Pharyngeal virus shedding was very high during the first week of symptoms, with a peak at 7.11 × 108 RNA copies per throat swab on day 4. Infectious virus was readily isolated from samples derived from the throat or lung, but not from stool samples—in spite of high concentrations of virus RNA. Blood and urine samples never yielded virus. Active replication in the throat was confirmed by the presence of viral replicative RNA intermediates in the throat samples. We consistently detected sequence-distinct virus populations in throat and lung samples from one patient, proving independent replication. The shedding of viral RNA from sputum outlasted the end of symptoms. Seroconversion occurred after 7 days in 50% of patients (and by day 14 in all patients), but was not followed by a rapid decline in viral load. COVID-19 can present as a mild illness of the upper respiratory tract. The confirmation of active virus replication in the upper respiratory tract has implications for the containment of COVID-19.
Methods|No statistical methods were used to predetermine sample size. The experiments were not randomized and investigators were not blinded to allocation during experiments and outcome assessment.Clinical samples and viral load conversionSputum and stool samples were taken and shipped in native conditions. Oro- and nasopharyngeal throat swabs were preserved in 3 ml of viral transport medium. Viral loads in sputum samples were projected to RNA copies per ml, in stool samples to copies per g and in throat swabs to copies per 3 ml, assuming that all sample components were suspended in 3 ml viral transport medium. For swab samples suspended in less than 3 ml viral transport medium, this conversion was adapted to represent copies per whole swab. An aggregated overview of samples received per day after the onset of disease from all patients is shown in Fig. 1a.RT–PCR for SARS-CoV-2 and other respiratory virusesRT–PCR used targets in the E and RdRp genes as previously described12. Both laboratories used a pre-formulated oligonucleotide mixture (Tib-Molbiol) to make the laboratory procedures more reproducible. All patients were also tested for other respiratory viruses, including HCoV-HKU1, HCoV-OC43, HCoV-NL63 and HCoV-229E, influenza virus A, influenza virus B, rhinovirus, enterovirus, respiratory syncytial virus, human parainfluenza viruses 1–4, human metapneumovirus, adenovirus and human bocavirus using LightMix-Modular Assays (Roche). Additional technical details are provided in Supplementary Methods section 1.Virus isolationVirus isolation was done in two laboratories on Vero E6 cells. In brief, 100 μl of suspended, cleared and filtered clinical sample was mixed with an equal volume of cell culture medium. Supernatant was collected after 0, 1, 3 and 5 days and used in RT–PCR analysis. Additional technical details are provided in Supplementary Methods section 2a.SerologyWe performed recombinant immunofluorescence assays to determine the specific reactivity against recombinant spike proteins in VeroB4 cells, as previously described26,28. This assay used a cloned coronavirus spike protein from HCoV-229E, HCoV-NL63, HCoV-OC43, HCoV-HKU1 or SARS-CoV-2. The screening dilution was 1:10. Plaque reduction neutralization tests were done essentially as previously described for MERS-CoV26. Serum dilutions causing plaque reductions of 90% (PRNT90) and 50% (PRNT50) were recorded as titres. Additional technical details are provided in Supplementary Methods section 2b, c.Statistical analysesStatistical analyses were done using SPSS software (version 25) or GrapPad Prism (version 8).Ethical approval statementAll patients provided informed consent for the use of their data and clinical samples for the purposes of the present study. Institutional review board clearance for the scientific use of patient data has been granted to the treating institution by the ethics committee at the Medical Faculty of the Ludwig Maximillians Universität Munich (vote 20-225 KB).Reporting summaryFurther information on research design is available in the Nature Research Reporting Summary linked to this paper.
Abstract|Carotenoids are colored compounds produced by plants, fungi, and microorganisms and are required in the diet of most animals for oxidation control or light detection. Pea aphids display a red-green color polymorphism, which influences their susceptibility to natural enemies, and the carotenoid torulene occurs only in red individuals. Unexpectedly, we found that the aphid genome itself encodes multiple enzymes for carotenoid biosynthesis. Phylogenetic analyses show that these aphid genes are derived from fungal genes, which have been integrated into the genome and duplicated. Red individuals have a 30-kilobase region, encoding a single carotenoid desaturase that is absent from green individuals. A mutation causing an amino acid replacement in this desaturase results in loss of torulene and of red body color. Thus, aphids are animals that make their own carotenoids.
Abstract|In vitro synthesis of endoplasmic reticulum-derived transport vesicles has been reconstituted with washed membranes and three soluble proteins (Sar1p, Sec13p complex, and Sec23p complex). Vesicle formation requires GTP but can be driven by nonhydrolyzable analogs such as GMP-PNP. However, GMP-PNP vesicles fail to target and fuse with the Golgi complex whereas GTP vesicles are functional. All the cytosolic proteins required for vesicle formation are retained on GMP-PNP vesicles, while Sar1p dissociates from GTP vesicles. Thin section electron microscopy of purified preparations reveals a uniform population of 60-65 nm vesicles with a 10 nm thick electron dense coat. The subunits of this novel coat complex are molecularly distinct from the constituents of the nonclathrin coatomer involved in intra-Golgi transport. Because the overall cycle of budding driven by these two types of coats appears mechanistically similar, we propose that the coat structures be called COPI and COPII.
Abstract|Brain-machine interfaces hold promise for the restoration of sensory and motor function and the treatment of neurological disorders, but clinical brain-machine interfaces have not yet been widely adopted, in part, because modest channel counts have limited their potential. In this white paper, we describe Neuralink’s first steps toward a scalable high-bandwidth brain-machine interface system. We have built arrays of small and flexible electrode “threads,” with as many as 3072 electrodes per array distributed across 96 threads. We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute. Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small implantable device that contains custom chips for low-power on-board amplification and digitization: The package for 3072 channels occupies less than 23×18.5×2 mm3. A single USB-C cable provides full-bandwidth data streaming from the device, recording from all channels simultaneously. This system has achieved a spiking yield of up to 70% in chronically implanted electrodes. Neuralink’s approach to brain-machine interface has unprecedented packaging density and scalability in a clinically relevant package.Keywords: brain-machine interface, sensory function, motor function, neurology
Introduction|Brain-machine interfaces have the potential to help people with a wide range of clinical disorders. For example, researchers have demonstrated human neuroprosthetic control of computer cursors [1-3], robotic limbs [4,5], and speech synthesizers [6] by using no more than 256 electrodes. Although these successes suggest that high-fidelity information transfer between brains and machines is possible, development of brain-machine interface has been critically limited by the inability to record from large numbers of neurons. Noninvasive approaches can record the average of millions of neurons through the skull, but this signal is distorted and nonspecific [7,8]. Invasive electrodes placed on the surface of the cortex can record useful signals, but they are limited in that they average the activity of thousands of neurons and cannot record signals deep in the brain [9]. Most brain-machine interfaces have used invasive techniques, because the most precise readout of neural representations requires recording single action potentials from neurons in distributed, functionally linked ensembles [10].Microelectrodes are the gold-standard technology for recording action potentials, but there is no clinically translatable microelectrode technology for large-scale recordings [11]. This would require a system with material properties that provide high biocompatibility, safety, and longevity. Moreover, this device would also need a practical surgical approach and high-density, low-power electronics to ultimately facilitate fully implanted wireless operation.Most devices for long-term neural recording are arrays of electrodes made from rigid metals or semiconductors [12-18]. Although rigid metal arrays facilitate penetrating the brain, the size, Young modulus, and bending stiffness mismatches between stiff probes and brain tissue can drive immune responses that limit the function and longevity of these devices [19,11]. Furthermore, the fixed geometry of these arrays constrains the populations of neurons that can be accessed, especially due to the presence of vasculature.An alternative approach is to use thin, flexible multielectrode polymer probes [20,21]. The smaller size and increased flexibility of these probes should offer greater biocompatibility. However, a drawback of this approach is that thin polymer probes are not stiff enough to directly insert into the brain; their insertion must be facilitated by stiffeners [22,21], injection [23,24], or other approaches [25], all of which are quite slow [26,27]. To satisfy the functional requirements for a high-bandwidth brain-machine interface, while taking advantage of the properties of thin-film devices, we developed a robotic approach, where large numbers of fine and flexible polymer probes are efficiently and independently inserted across multiple brain regions [28].Here, we report Neuralink’s progress toward a flexible, scalable brain-machine interface that increases channel count by an order of magnitude over prior work. Our system has three main components: ultra-fine polymer probes, a neurosurgical robot, and custom high-density electronics (all of which are described below). We demonstrate the rapid implantation of 96 polymer threads, each thread with 32 electrodes, yielding a total of 3072 electrodes.We developed miniaturized custom electronics that allow us to stream full broadband electrophysiology data simultaneously from all these electrodes (described below). We packaged this system for long-term implantation and developed custom online spike-detection software that can detect action potentials with low latency. Together, this system serves as a state-of-the-art research platform and the first prototype toward a fully implantable human brain-machine interface.
Threads|We have developed a custom process to fabricate minimally displacive neural probes that employ a variety of biocompatible thin film materials. The main substrate and dielectric used in these probes is polyimide, which encapsulates a gold thin film trace. Each thin film array is composed of a “thread” area that features electrode contacts and traces and a “sensor” area where the thin film interfaces with custom chips that enable signal amplification and acquisition. A wafer-level microfabrication process enables high-throughput manufacturing of these devices. Ten thin film devices are patterned on a wafer, each with 3072 electrode contacts.Each array has 48 or 96 threads, each of which contain 32 independent electrodes. Integrated chips are bonded to the contacts on the sensor area of the thin film using a flip-chip bonding process. One goal of this approach is to maintain a small thread cross-sectional area to minimize tissue displacement in the brain. To achieve this, while keeping the channel count high, stepper lithography and other microfabrication techniques are used to form the metal film at submicron resolution.We have designed and manufactured over 20 different thread and electrode types into our arrays; two example designs are shown in Figure 1A and B. Probes are designed either with the reference electrodes on separate threads or on the same threads as the recording electrodes (referred to as “on-probe references”). We have fabricated threads ranging from 5 µm to 50 µm in width that incorporate recording sites of several geometries (Figure 1). Thread thickness is nominally 4-6 µm, which includes up to three layers of insulation and two layers of conductor. Typical thread length is approximately 20 mm. To manage these long, thin threads prior to insertion, parylene-c is deposited onto the threads to form a film on which the threads remain attached until the surgical robot pulls them off. Each thread ends in a 16×50 µm2 loop to accommodate needle threading.Open in a separate windowFigure 1Our novel polymer probes. (A) “Linear Edge” probes, with 32 electrode contacts spaced by 50 μm. (B) “Tree” probes with 32 electrode contacts spaced by 75 μm. (C) Increased magnification of individual electrodes for the thread design in panel A, emphasizing their small geometric surface area. (D) Distribution of electrode impedances (measured at 1 kHz) for two surface treatments: PEDOT (n=257) and IrOx (n=588). IrOx: iridium oxide; PEDOT: poly-ethylenedioxythiophene; PCB: printed circuit board.Since the individual gold electrode sites have small geometric surface areas (Figure 1C), we use surface modifications to lower the impedance for electrophysiology and increase the effective charge-carrying capacity of the interface (Figure 1D). Two such treatments that we have used are the electrically conductive polymer poly-ethylenedioxythiophene doped with polystyrene sulfonate (PEDOT:PSS) [29,30] and iridium oxide (IrOx) [31,32]. In benchtop testing, we have achieved impedances of 36.97 (SD 4.68) kΩ (n=257 electrodes) and 56.46 (SD 7.10) kΩ (n=588) for PEDOT:PSS and IrOx, respectively. The lower impedance of PEDOT:PSS is promising; however, the long-term stability and biocompatibility of PEDOT:PSS are less well established than those for IrOx. These techniques and processes can be improved and further extended to other types of conductive electrode materials and coatings.To keep the electronics package small, a novel alignment and flip-chip bonding process was developed. Multilevel gold stud bumps are placed throughout the printed circuit board (PCB) to act as alignment guides and temporary holders for the thin film. A custom shuttle is used to handle, align, and place the thin film on the PCB such that holes in the thin film slide around the stud bumps. The thin film is secured into place by applying force to the gold stud bumps, which flattens them into rivets. Next, the integrated chips are bonded directly to both contacts on the sensor area of the thin film and pads on the PCB by using standard flip-chip bonding processes. A custom silicon shuttle is used to vacuum pick-up rows of 40-50 capacitors and bond a total of 192 capacitors onto the PCB. This alignment and bonding process was key to creating a package containing 3072 channels in a 23×18.5 mm2 footprint.
Robot|Thin-film polymers have previously been used for electrode probes [21], but their low bending stiffness complicates insertions. Neuralink has developed a robotic insertion approach for inserting flexible probes [28], allowing rapid and reliable insertion of large numbers of polymer probes targeted to avoid vasculature and record from dispersed brain regions. The robot’s insertion head is mounted on a globally accurate, 400×400×150 mm travel, 10-µm three-axis stage and holds a small, quick-swappable “needle-pincher” assembly (Figures 2 and ​and33A).Open in a separate windowFigure 2Needle pincher cartridge compared with a penny for scale. (A) Needle. (B) Pincher. (C) Cartridge.Open in a separate windowFigure 3Insertion process into an agarose brain proxy. (1) The inserter approaches the brain proxy with a thread. (i) needle and cannula. (ii) Previously inserted thread. (2) Inserter touches down on the brain proxy surface. (3) Needle penetrates tissue proxy, advancing the thread to the desired depth. (iii) Inserting thread. (4) Inserter pulls away, leaving the thread behind in the tissue proxy. (iv) Inserted thread.The needle is milled from 40-µm diameter tungsten-rhenium wire-stock electrochemically etched to 24-µm diameter along the inserted length (Figure 2A). The tip of the needle is designed both to hook onto insertion loops—for transporting and inserting individual threads—and to penetrate the meninges and brain tissue. The needle is driven by a linear motor, allowing variable insertion speeds and rapid retraction acceleration (up to 30,000 mm/s2) to encourage separation of the probe from the needle. The pincher is a 50-µm tungsten wire bent at the tip and driven both axially and rotationally (Figure 2B). It serves as a support for probes during transport and as a guide to ensure that threads are inserted along the needle path. Figure 3 shows a sequence of photographs of the insertion process into an agarose brain proxy.The inserter head also holds an imaging stack (Figure 4E-G) used for guiding the needle into the thread loop, insertion targeting, live insertion viewing, and insertion verification. In addition, the inserter head contains six independent light modules, each capable of independently illuminating with 405 nm, 525 nm, and 650 nm or white light (Figure 4C). The 405-nm illumination excites fluorescence from polyimide and allows the optical stack and computer vision to reliably localize the 16×50 µm2 thread loop and execute submicron visual servoing to guide, while illuminated by 650 nm light, the needle through it. Stereoscopic cameras, software-based monocular extended depth-of-field calculations, and illumination with 525 nm light allow for precise estimation of the location of the cortical surface.Open in a separate windowFigure 4The robotic electrode inserter; enlarged view of the inserter-head shown in the inset. (A) Loaded needle pincher cartridge. (B) Low-force contact brain position sensor. (C) Light modules with multiple independent wavelengths. (D) Needle motor. (E) One of four cameras focused on the needle during insertion. (F) Camera with wide angle view of the surgical field. (G) Stereoscopic cameras.The robot registers insertion sites to a common coordinate frame with landmarks on the skull, which, when combined with depth tracking, enables precise targeting of anatomically defined brain structures. An integrated custom software suite allows preselection of all insertion sites, enabling planning of insertion paths optimized to minimize tangling and strain on the threads. The planning feature highlights the ability to avoid vasculature during insertions, one of the key advantages of inserting electrodes individually. This is particularly important, since damage to the blood-brain barrier is thought to play a key role in the brain’s inflammatory response to foreign objects [33].The robot features an autoinsertion mode, which can insert up to six threads (192 electrodes) per minute. Although the entire insertion procedure can be automated, the surgeon retains full control, and, if desired, can make manual microadjustments to the thread position before each insertion into the cortex. The neurosurgical robot is compatible with sterile shrouding and has features to facilitate successful and rapid insertions such as automatic sterile ultrasonic cleaning of the needle. The needle pincher cartridge (Figure 2C) is the portion of the inserter head that makes direct contact with brain tissue and is a consumable that can be replaced midsurgery in under a minute.With this system, we have demonstrated an average of 87.1% (SD 12.6%) insertion success rate over 19 surgeries. In this study, precise manual adjustments were made to avoid microvasculature on the cortical surface, slowing total insertion time from the fastest possible time. Even with these adjustments, the total insertion time for this study averaged approximately 45 min for an approximate insertion rate of 29.6 electrodes per minute (Figure 5). Insertions were made in a 4×7 mm2 bilateral craniotomy with >300 µm spacing between threads to maximize cortical coverage. This demonstrates that robotic insertion of thin polymer electrodes is an efficient and scalable approach for recording from large numbers of neurons in anatomically defined brain regions.Open in a separate windowFigure 5A packaged sensor device. (A) Individual neural processing application-specific integrated circuit capable of processing 256 channels of data. This particular packaged device contains 12 of these chips for a total of 3072 channels. (B) Polymer threads on parylene-c substrate. (C) Titanium enclosure (lid removed). (D) Digital USB-C connector for power and data.
Electronics|Chronic recording from thousands of electrode sites presents significant electronics and packaging challenges. The density of recording channels necessitates placing the signal amplification and digitization stack within the array assembly; otherwise, the cable and connector requirements would be prohibitive. This recording stack must amplify small neural signals (<10 µVRMS) while rejecting out-of-band noise, sample and digitize the amplified signals, and stream out the results for real-time processing—all using minimal power and size.The electronics are built around our custom Neuralink application-specific integrated circuit (ASIC), which consists of 256 individually programmable amplifiers (“analog pixels”), on-chip analog-to-digital converters (ADCs), and peripheral control circuitry for serializing the digitized outputs. The analog pixel is highly configurable: The gains and filter properties can be calibrated to account for variability in signal quality due to process variations and the electrophysiological environment. The on-chip ADC samples at 19.3 kHz with 10-bit resolution. Each analog pixel consumes 5.2 µW, and the whole ASIC consumes approximately 6 mW, including the clock drivers. Performance of the Neuralink ASIC is summarized in Table 1, and a photograph of the fabricated device is shown in Figure 6A.Table 1Neuralink application-specific integrated circuit.VariableValueNumber of channels256Gain, dB42.9-59.4Bandwidth, kHz3-27Input-referred noise (3 Hz-10 kHz), µVRMS5.9Maximum differential input range, mVPP7.2Analog-to-digital converter resolution, bit10Analog pixel power, µW5.2Open in a separate windowOpen in a separate windowFigure 6Thread implantation and packaging. (A) An example perioperative image showing the cortical surface with implanted threads and minimal bleeding. (B) Packaged sensor device (“System B”) chronically implanted in a rat.The Neuralink ASIC forms the core of a modular recording platform that allows for easy replacement of constitutive parts for research and development purposes (Figure 6). In the systems discussed here, a number of ASICs are integrated into a standard PCB using flip-chip integration. Each system consists of a field-programmable gate array; real-time temperature, accelerometer, and magnetometer sensors; and a single USB-C connector for full-bandwidth data transfer. The systems are packaged in titanium cases that are coated with parylene-c, which serves as a moisture barrier to prevent fluid ingress and prolong functional lifetime.We describe two such configurations that we have built—a 1536-channel recording system (“System A”) and a 3072-channel recording system (“System B”)—summarized in Table 2. System A employs the current-generation Neuralink ASIC, while System B uses an earlier revision with comparable functionality but poorer performance specifications.Table 2Two recording system configurations.VariableValueSystem ASystem BNumber of channels15363072Sampling rate, kHz19.318.6Total system power consumption, mW550750Total system size, mm324.5×20×1.6523×18.5×2Implant weight, g1115Open in a separate windowSystem B was designed to maximize channel density and is used for applications that demand extremely high channel count. In contrast, System A was designed to facilitate faster and more reliable manufacturing; it can be built five times faster than System B with better yields.An Ethernet-connected base station converts the data streams from these systems into multicast 10 GB Ethernet user datagram protocol packets, allowing downstream users to process the data in a variety of ways, for example, visualizing the data in real time [34] or writing the data to disk. Each base station can connect to up to three implants simultaneously. These devices are further supported by a software ecosystem that allows for plug and play usability with zero configuration: Neural data begin streaming automatically when a cable is connected.
Electrophysiology|We have implanted both Systems A and B in male Long-Evans rats, as described in the section Robot. All animal procedures were performed in accordance with the National Research Council’s Guide for the Care and Use of Laboratory Animals and were approved by the Neuralink Institutional Animal Care and Use Committee. Electrophysiological recordings were taken as the animals freely explored an arena equipped with a commutated cable that permitted unrestricted movement. System A can record 1344 of 1536 channels simultaneously; the exact channel configuration can be arbitrarily specified at the time of recording; System B can record from all 3072 channels simultaneously. Digitized broadband signals were processed in real time to identify action potentials (spikes) using an online detection algorithm.Spike detection requirements for real-time brain-machine interface are different from most conventional neurophysiology requirements. While most electrophysiologists spike-sort data offline and spend significant effort to reject false-positive spike events, brain-machine interface events must be detected in real time and spike detection parameters must maximize decoding efficacy. Using our custom online spike-detection software, we found that a permissive filter that allows an estimated false-positive rate of approximately 0.2 Hz performs better than setting stringent thresholds that may reject real spikes (data not shown).Given these considerations, we set a threshold of >0.35 Hz to quantify the number of electrodes that recorded spiking units. Since we typically do not spike sort our data, we do not report multiple units per channel. Brain-machine interface decoders commonly operate without spike sorting with minimal loss of performance [35,36]. Moreover, recent results show that spike sorting is not necessary to accurately estimate neural population dynamics [37].Data from a recent experiment using System A are shown in Figures 7 and ​and8.8. In this experiment, 40 of 44 attempted insertions were successful (90%) for a total of 1280 implanted electrodes, of which 1020 were recorded simultaneously. The broadband signals recorded from a representative thread show both local field and spiking activity (Figure 7). A sample output of the spike detection pipeline is shown in raster form in Figure 8. In this example, two overlapping recording configurations were used to record from all 1280 implanted channels. On this array, our spiking yield was 43.4% of the channels, with many spikes appearing on multiple neighboring channels, as has been observed in other high-density probes [16,17,21]. On other System A arrays, we observed a spiking yield of 45.60% (SD 0.03%) across 19 surgeries, with a maximum spiking yield of 70%.Open in a separate windowFigure 7The broadband signals recorded from a representative thread. Left: Broadband neural signals (unfiltered) simultaneously acquired from a single thread (32 channels) implanted in rat cerebral cortex. Each channel (row) corresponds to an electrode site on the thread (schematic at left; sites spaced by 50 μm). Spikes and local field potentials are readily apparent. Right: Putative waveforms (unsorted); numbers indicate channel location on thread. Mean waveform is shown in black.Open in a separate windowFigure 8Our devices allow the recording of widespread neural activity distributed across multiple brain regions and cortical layers. Left: Thread insertion sites (colored circles) are indicated on the rendered rodent brain [38]. Right: Raster of 1020 simultaneously recorded channels, sorted per thread (color corresponds to insertion site). Inset: Enlarged raster of spikes from a single thread. This thread corresponds to the one shown in Figure 7.
Discussion|We have described a brain-machine interface with a high-channel count and single-spike resolution. It is based on flexible polymer probes, a robotic insertion system, and custom low-power electronics. This system serves two main purposes: It is a research platform for use in rodents and serves as a prototype for future human clinical implants. The ability to quickly iterate designs and testing in rodents allows for the rapid refinement of devices, manufacturing processes, and software. Because it is a research platform, the system uses a wired connection to maximize the bandwidth for raw data streaming. This is important for performance assessments and crucial for the development of signal processing and decoding algorithms. In contrast, the clinical devices that derive from this platform will be fully implantable, which requires hermetic packaging, and have on-board signal compression, reduced power consumption, wireless power transmission, and data telemetry through the skin without percutaneous leads.Modulating neural activity will be an important part of next-generation clinical brain-machine interfaces [39], for example, to provide a sense of touch or proprioception to neuroprosthetic movement control [40,41]. Therefore, we designed the Neuralink ASIC to be capable of electrical stimulation on every channel, although we have not demonstrated these capabilities here.This brain-machine interface system has several advantages over previous approaches. The size and composition of the thin-film probes are a better match for the material properties of brain tissue than commonly used silicon probes and may therefore exhibit enhanced biocompatibility [28,21]. In addition, the ability to choose where our probes are inserted, including into the subcortical structures, allows us to create custom array geometries for targeting specific brain regions while avoiding vasculature. This feature is significant for creating a high-performance brain-machine interface, as the distribution of electrodes can be customized depending on the task requirements. Lastly, the miniaturization and design of the Neuralink ASIC affords great flexibility in system design and supports very high channel counts within practical size and power constraints.In principle, our approach to brain-machine interfaces is highly extensible and scalable. Here, we report simultaneous broadband recording from over 3000 inserted electrodes in a freely moving rat. In a larger brain, multiple devices with this architecture could be readily implanted, and we could therefore interface with many more neurons without extensive re-engineering. Further development of surgical robotics could allow us to accomplish this without dramatically increasing surgery time.Although significant technological challenges must be addressed before a high-bandwidth device is suitable for clinical application, with such a device, it is plausible to imagine that a patient with spinal cord injury could dexterously control a digital mouse and keyboard. When combined with rapidly improving spinal stimulation techniques [42], in the future, this approach could conceivably restore motor function. High-bandwidth neural interfaces should enable a variety of novel therapeutic possibilities.
Abstract|Background and aimsCOVID-19 and low levels of vitamin D appear to disproportionately affect black and minority ethnic individuals. We aimed to establish whether blood 25-hydroxyvitamin D (25(OH)D) concentration was associated with COVID-19 risk, and whether it explained the higher incidence of COVID-19 in black and South Asian people.MethodsUK Biobank recruited 502,624 participants aged 37–73 years between 2006 and 2010. Baseline exposure data, including 25(OH)D concentration and ethnicity, were linked to COVID-19 test results. Univariable and multivariable logistic regression analyses were performed for the association between 25(OH)D and confirmed COVID-19, and the association between ethnicity and both 25(OH)D and COVID-19.ResultsComplete data were available for 348,598 UK Biobank participants. Of these, 449 had confirmed COVID-19 infection. Vitamin D was associated with COVID-19 infection univariably (OR = 0.99; 95% CI 0.99–0.999; p = 0.013), but not after adjustment for confounders (OR = 1.00; 95% CI = 0.998–1.01; p = 0.208). Ethnicity was associated with COVID-19 infection univariably (blacks versus whites OR = 5.32, 95% CI = 3.68–7.70, p-value<0.001; South Asians versus whites OR = 2.65, 95% CI = 1.65–4.25, p-value<0.001). Adjustment for 25(OH)D concentration made little difference to the magnitude of the association.ConclusionsOur findings do not support a potential link between vitamin D concentrations and risk of COVID-19 infection, nor that vitamin D concentration may explain ethnic differences in COVID-19 infection.Keywords: COVID-19, Vitamin D, Ethnicity
1. Introduction|A novel coronavirus that manifested in 2019 (SARS-CoV-2) has led to a pandemic of pneumonia-related illness (COVID-19) with an estimated case fatality of around 1%. There is an urgent need to better understand risk factors for contracting the infection and for poorer prognosis thereafter.There is growing evidence that COVID-19 disproportionately affects black and minority ethnic individuals, with the Intensive Care National Audit and Research Centre reporting that a third of confirmed cases admitted to critical care in England are non-white [1]. This compares with the 2011 Census figures which show that 14% of the general population of England and Wales identify themselves as black and minority ethnic individuals [2]. Similarly, in the United States a pattern of higher risk has been observed in African Americans [3]. Consequently, the relationship between ethnicity and COVID-19 has been identified as an urgent public health research priority [4].Several factors have been proposed to explain the apparent greater risk of COVID-19 infection in ethnic minority groups. UK Government statistics show that people from black and minority ethnic backgrounds are more likely than white British people to live in the most socioeconomically deprived areas of England [5]. Furthermore, certain minority ethnic groups experience a higher burden of comorbid disease [6], which may put them at higher risk of more severe COVID-19 infection [7,8].One potential mediator could be the higher prevalence of apparent vitamin D deficiency in black and minority ethnic populations [4]. Vitamin D is variably inversely associated with multiple health outcomes and mortality (although these associations may not be causal) [9]. Most vitamin D results from production in the skin following exposure to ultraviolet (UV) radiation from the sun. Individuals with dark skin have, on average, lower concentrations of blood vitamin D because the melanin in dark skin does not absorb as much UV [10]. Furthermore, deficiency is more common in high latitude countries such as the UK. Whilst most chronic conditions have not been improved by vitamin D supplementation, a recent meta-analysis of randomised trials suggested vitamin D may lessen the risk of acute respiratory infections [11].In this study, we hypothesised that blood 25 hydroxyvitamin D (25(OH)D) concentration was associated with COVID-19 risk among UK Biobank participants, and explained wholly, or in part, the higher incidence of COVID-19 infection in ethnic minority participants.
4. Results|Results were available for 2724 COVID-19 tests conducted on 1474 individuals. Complete data on (25(OH)D) concentration and covariates were available for 348,598 UK Biobank participants. Of these, 449 had a positive COVID-19 test. Table 1 presents study participants by presence or absence of positive COVID-19 test result. Median 25(OH)D concentration measured at recruitment was lower in patients who subsequently had confirmed COVID-19 infection (28.7 (IQR 10.0–43.8) nmol/L) than other participants (32.7 (IQR 10.0–47.2) nmol/L). It predicted COVID-19 infection univariably (Table 2 ; OR = 0.99, 95% CI 0.99–0.999, p = 0.013), but not after adjustment for covariates (OR = 1.00; 95% CI = 0.998–1.01; p = 0.208). Exposures that did predict COVID-19 status in the multivariable logistic regression were male sex (OR = 1.41; 95% CI = 1.16–1.71; p-value = 0.001), higher socioeconomic deprivation (highest vs lowest Townsend quintile OR = 1.89; 95% CI = 1.37–2.60; p-value<0.001), poorer self-reported health status (poor health vs excellent health OR = 2.32; 95% CI = 1.45–3.72; p-value<0.001), age at assessment (OR = 1.02; 95% CI = 1.00–1.03; p-value = 0.016), being overweight (OR = 1.34; 95% CI = 1.04–1.72; p-value = 0.024) or obese (OR = 1.62; 95% CI = 1.23–2.14; p-value = 0.001), and non-white ethnicity (blacks OR = 4.30, 95% CI = 2.92–6.31, p-value<0.001; South Asians OR = 2.42, 95% CI = 1.50–3.93, p-value<0.001) (Fig. 1 ).Table 1Characteristics of study population by presence or absence of confirmed COVID-19 infection.No COVID-19COVID-19P valueN (%)N (%)SexMale168,391 (48.37)265 (59.02)<0.001Female179,758 (51.63)184 (40.98)Self-reported ethnicityWhite331,464 (95.21)385 (85.75)<0.001Black5022 (1.44)32 (7.13)South Asian5917 (1.70)19 (4.23)Other5746 (1.65)13 (2.90)Current smoking statusYes312,037 (89.63)398 (88.64)0.493No36,112 (10.37)51 (11.36)Townsend deprivation quintile170,669 (20.30)61 (13.59)<0.001270,726 (20.31)76 (16.93)370,644 (20.29)64 (14.25)470,270 (20.18)105 (23.39)565,840 (18.91)143 (31.85)BMI categoryUnderweight1759 (0.51)2 (0.45)<0.001Normal weight115,410 (33.15)95 (21.16)Overweight148,210 (42.57)194 (43.21)Obese82,770 (23.77)158 (35.19)Self-reported health ratingExcellent60,508 (17.38)45 (10.02)<0.001Good203,640 (58.49)227 (50.56)Fair69,676 (20.01)133 (29.62)Poor14,325 (4.11)44 (9.80)Long-standing illness, disability or infirmityYes237,470 (68.21)245 (54.57)<0.001No110,679 (31.79)204 (45.43)DiabetesYes329,324 (94.59)400 (89.09)<0.001No18,825 (5.41)49 (10.91)Median (IQR)Median (IQR)Vitamin D32.7 (10.0–47.2)28.7 (10.0–43.8)<0.01Age at assessment49 (38–57)49 (40–58)<0.05SBP138 (125–151)138 (127–153)0.177DBP82 (75–89)83 (76–90)<0.01Open in a separate windowCategorical variables compared by chi [2] test; continuous variables compared by Mann-Whitney U test.N number; BMI body mass index; IQR inter-quartile range; SBP systolic blood pressure; DBP diastolic blood pressure.Table 2Association between Vitamin D and confirmed COVID-19 infection.UnivariableMultivariableaOR (95% CI)p-valueOR (95% CI)p-valueVitamin D (nmol/L)0.99 (0.99–0.999)0.0131.00 (0.998–1.01)0.208Vitamin D deficient (<25 nmol/L)1.37 (1.07–1.76)0.0110.92 (0.71–1.21)0.564Vitamin D insufficient (<50 nmol/L)1.19 (0.99–1.44)0.0680.88 (0.72–1.08)0.232Open in a separate windowOR odds ratio; CI confidence interval.aAdjusted for ethnicity, sex, month of assessment, Townsend deprivation quintile, household income, self-reported health rating, smoking status, BMI category, age at assessment, diabetes, SBP, DBP, and long-standing illness, disability or infirmity.Open in a separate windowFig. 1Forest plot of factors associated with COVID-19 infection.When participants were categorised into vitamin D deficient (<25 nmol/L) and not deficient the pattern of results was similar to those observed with vitamin D concentration entered as a continuous variable (univariable OR = 1.37, 95% CI = 1.07–1.76, p-value = 0.011; adjusted OR = 0.92, 95% CI = 0.71–1.21, p-value = 0.564) (Table 2). When participants were categorised into vitamin D insufficient (<50 nmol/L) and sufficient there was no association with COVID-19 infection either univariably (OR = 1.19, 95% CI = 0.99–1.44, p-value = 0.068), nor multivariably (OR = 0.88, 95% CI = 0.72–1.08, p-value = 0.232) (Table 2).In the study, 331,849 (95.20%) participants were white, 5054 (1.45%) black, 5936 (1.70%) South Asian, and 5759 (1.65%) other. Of the 449 participants with confirmed COVID-19 infection, 385 (85.75%) were white, 32 (7.13%) black, 19 (4.23%) South Asian, and 13 (2.90%) other. Median 25(OH)D concentration was 33.8 (IQR 10.0–48.1) nmol/L in white participants, 21.0 (IQR 10.0–29.9) in black participants, 14.5 (IQR 15.5–22.1) in South Asian participants, and 23.3 (IQR 10.0–33.7) nmol/L in others. In this study 38,778 (11.69%) white, 1834 (36.29%) black, 3403 (57.33%) South Asian, and 1671 (29.02%) of other participants were vitamin D deficient at baseline.In logistic regression, black ethnicity and South Asian ethnicity were both associated with confirmed COVID-19 infection univariably (OR = 5.49, 95% CI = 3.82–7.88, p-value<0.001; OR = 2.76, 95% CI = 1.74–4.39, p-value<0.001 respectively) compared with whites. Adjustment for 25(OH)D concentration made little difference to the magnitude of the associations (Table 3 ). Results were similar when, instead of adjusting for 25(OH)D concentration, adjustment was made separately for vitamin D deficiency and vitamin D insufficiency.Table 3Association between ethnicity and confirmed COVID-19 infection.UnivariableAdjusted for Vitamin D concentrationMultivariableaOR (95% CI)p-valueOR (95% CI)p-valueOR (95% CI)p-valueWhite (referent)111Black5.49 (3.82–7.88)<0.0015.32 (3.68–7.70)<0.0014.30 (2.92–6.31)<0.001South Asian2.76 (1.74–4.39)<0.0012.65 (1.65–4.25)<0.0012.42 (1.50–3.93)<0.001Other1.95 (1.12–3.39)0.0181.90 (1.09–3.32)0.0241.87 (1.07–3.28)0.029Open in a separate windowOR odds ratio; CI confidence interval.aAlso adjusted for sex, month of assessment, Townsend deprivation quintile, household income, self-reported health rating, smoking status, BMI category, age at assessment, diabetes, SBP, DBP, and long-standing illness, disability or infirmity.There was no significant interaction between ethnicity and vitamin D deficiency (OR = 0.90; 95% CI = 0.66–1.23; p-value = 0.515).
5. Discussion|Our findings are consistent with previous studies [7,19] in demonstrating a higher risk of confirmed COVID-19 infection in ethnic minority groups. Vitamin D has been suggested as possibly protective of COVID-19 infection [[20], [21], [22]] and, if so, could plausibly play a role in ethnic variations in COVID-19 infection. However, we found no association between 25(OH)D and COVID-19 infection after adjusting for potential confounders. Therefore, despite 25(OH)D concentration being lower in black and minority ethnic participants, there was no evidence that it might play a role in their higher risk of COVID-19 infection.It has been suggested in some media outlets that language barriers may contribute to ethnic differences in COVID-19 risk. This is unlikely to contribute to the risk we observed in UK Biobank because all participants spoke English (albeit with the possibility of fluency variation). The association with ethnicity was only slightly attenuated after adjustment for socioeconomic and lifestyle differences in white and black and minority ethnic participants; the risk of COVID-19 remained around 4-fold in black participants and more than 2-fold in South Asians. Further studies are required to determine the mechanisms underlying ethnic variations in risk of COVID-19 infection and its severity. It may be that pathways related to cardiometabolic conditions or differences in cardiorespiratory reserve, or potentially other social factors, are more relevant, as we have recently discussed [23].Our study replicated findings showing increased risk of COVID-19 in black and minority ethnic individuals, in men, and in people who are overweight or obese. It is surprising that we did not observe an association between diabetes or blood pressure and COVID-19 risk. Other studies have shown increased risk of hospitalisation and severe illness requiring ventilation in patients with diabetes and hypertension [7,24]. However, ours is a relatively healthy general population cohort. Furthermore, we do not have information on the severity of COVID-19 and have included all positive tests rather than only severe cases.We did not show an independent association between smoking and COVID-19. Evidence from the literature is mixed. Initial studies suggested that current smoking increases the risk of severe infection [25]. However, this has since been disputed with some evidence that smoking may even protect against initial infection [26].The strengths of UK Biobank include its extensive phenotyping which enables the adjustment for demographic and lifestyle risk factors, disease and ill health, its large sample size, and its central processing laboratory for biochemical assays. However, it is not representative of the general population, in that participants live in less socioeconomically deprived areas, are predominantly Caucasian, and have fewer self-reported health conditions [27]. We have demonstrated that ethnic differences in COVID-19 infection exist in this relatively healthy population. Baseline measurements, including 25(OH)D concentration and health status, were obtained a decade ago. It would be preferable to have measurements immediately preceding development of COVID-19. However, 25(OH)D concentrations vary more by season than year, and generally track over time [28].Our study is the first to assess whether there is an association between blood 25(OH)D concentration and COVID-19 risk. We found no such link, suggesting that measurement of 25(OH)D would not be useful to assess risk in clinical practice. Furthermore, our results suggest that vitamin D is unlikely to be the underlying mechanism for the higher risk observed in black and minority ethnic individuals and vitamin D supplements are unlikely to provide an effective intervention.
6. Conclusion|Our analyses of UK Biobank data provided no evidence to support a potential role for (25 (OH)D) concentration to explain susceptibility to COVID-19 infection either overall or in explaining differences between ethnic groups.
Abstract|Alzheimer’s disease (AD) is characterized by synaptic and neuronal loss, which occurs at least partially through oxidative stress induced by oligomeric amyloid-β (Aβ)-peptide. Carnosic acid (CA), a chemical found in rosemary and sage, is a pro-electrophilic compound that is converted to its active form by oxidative stress. The active form stimulates the Keap1/Nrf2 transcriptional pathway and thus production of phase 2 antioxidant enzymes. We used both in vitro and in vivo models. For in vitro studies, we evaluated protective effects of CA on primary neurons exposed to oligomeric Aβ. For in vivo studies, we used two transgenic mouse models of AD, human amyloid precursor protein (hAPP)-J20 mice and triple transgenic (3xTg AD) mice. We treated these mice trans-nasally with CA twice weekly for 3 months. Subsequently, we performed neurobehavioral tests and quantitative immunohistochemistry to assess effects on AD-related phenotypes, including learning and memory, and synaptic damage. In vitro, CA reduced dendritic spine loss in rat neurons exposed to oligomeric Aβ. In vivo, CA treatment of hAPP-J20 mice improved learning and memory in the Morris water maze test. Histologically, CA increased dendritic and synaptic markers, and decreased astrogliosis, Aβ plaque number, and phospho-tau staining in the hippocampus. We conclude that CA exhibits therapeutic benefits in rodent AD models and since the FDA has placed CA on the ‘generally regarded as safe’ (GRAS) list, thus obviating the need for safety studies, human clinical trials will be greatly expedited.
Main|Alzheimer’s disease (AD) affects 5% of people over age 65 years, and its prevalence is increasing.1 Although AD manifests amyloid plaques and tau tangles, loss of synapses, eventually accompanied by neuronal loss, more closely correlates with cognitive decline.2 Damage to neurons occurs at least partially through generation of oxidative and nitrosative stress, due to excessive generation of reactive oxygen/nitrogen species (ROS/RNS) triggered by oligomeric amyloid-β (Aβ) peptide.3, 4, 5, 6Activation of the Keap1/Nrf2 (kelch-like ECH-associated protein 1/nuclear factor erythroid 2-related factor 2) pathway upregulates transcription of phase 2 antioxidant and anti-inflammatory proteins. We and others have shown that this can be a valuable therapeutic strategy in several neurodegenerative diseases (reviewed in Satoh et al.7). Here, we tested this approach in mouse models of AD using a compound known to activate the Keap1/Nrf2 pathway and to be clinically tolerated because of its presence in herbs widely-used in cooking. A critical factor that suggested the use of Nrf2 activators in this context was the finding that deficits in spatial learning, as seen in amyloid precursor protein (APP) transgenic mice, were ameliorated after intra-hippocampal injection of a lentiviral vector expressing Nrf2.8 Conversely, in humans with AD, decreased expression of Nrf2 in hippocampal neurons and astrocytes has been reported.9 The Keap1/Nrf2 pathway can be activated by an electrophilic compound when it reacts with a specific thiol on Keap1, releasing Nrf2 in the cytoplasm so it can enter the nucleus, where it binds to the antioxidant response element (ARE) on the promoters of phase 2 genes.10 Interestingly, several exogenous electrophilic compounds, including natural products, have been shown to activate this pathway, and thereby provide neuroprotection against ROS/RNS.An important issue, however, with electrophilic drugs is that they non-specifically react with thiol groups, and because glutathione (GSH) is the predominant thiol in normal cells, GSH can be depleted by electrophiles thus paradoxically lowering the threshold for cell toxicity in unstressed cells.11 An alternative strategy is to use pro-electrophilic drugs (PEDs) that are activated by the very oxidation in redox-stressed cells that is injurious and in which GSH has already been depleted.7 Carnosic acid (CA) represents such a PED that is found in the herbs rosemary and sage, which reportedly exhibit antioxidant and anti-inflammatory properties.12, 13 Our group elucidated the mechanistic basis for this action, showing that CA is relatively innocuous until activated by ROS at the site of damage. We demonstrated that ROS converts CA to the active electrophilic form, which affords protection in animal models of neural damage7, 14In the present study, we demonstrate the neuroprotective effects of CA both in vitro and in vivo in two transgenic mouse models of AD. Our neurobehavioral and histological readouts suggest that CA, administered trans-nasally in vivo, can be an effective treatment for AD in these rodent models.
Results|CA treatment ameliorates Aβ-induced spine loss in cultured rat cortical neuronsWe first assessed potential therapeutic benefits of CA in primary cortical neurons prepared from rat embryos. After 14–16 days in culture, neuronal cells were transfected with pmax-GFP to visualize dendritic spines. We then exposed the transfected cells to synthetic amyloid-β peptide 1-42 (Aβ42) oligomers (250 nM) in the presence or absence of CA. Four days after exposure, we fixed the neurons and quantified the number of dendritic spines per micrometer of dendritic length. As shown in Figure 1, Aβ exposure significantly reduced dendritic spine density in rat cortical neurons compared to control. In contrast, CA treatment concomitant with the Aβ exposure ameliorated oligomeric Aβ-induced dendritic spine loss.Figure 1Carnosic acid (CA) treatment ameliorates Aβ-induced dendritic spine loss in cultured rat neurons. (a) Images of rat cortical neurons transfected with pmax-GFP and then exposed to synthetic Aβ42 (250 nM oligomers) with or without CA (10 μM) for 4 days. After fixation, dendritic spines were visualized by GFP fluorescence. (b) Quantification of spine density per micrometer of dendritic length. Neurons exposed to oligomeric Aβ manifested significantly reduced dendritic spine density, and treatment with CA ameliorated this loss. Values are mean+S.E.M. (n>20 fields of dendritic spines for each condition; *P<0.05 by ANOVA compared with other conditions). Scale bar, 5 μmFull size imageCA treatment rescues deficits in spatial learning in hAPP-J20 AD miceWe assessed the effects of treatment with CA on spatial learning in hAPP-J20 mice. To this end, both wild-type (WT) and hAPP-J20 mice received transnasal CA administration for 3 months beginning at the age of 3 months. We then evaluated these mice in the Morris water maze. As shown in Figure 2a, after acclimatization and during the hidden platform trials, we found that on all three days (training days 1-3) hAPP-J20 mice without CA treatment spent significantly longer times in reaching the hidden platform than WT mice (treated with either vehicle or CA (WT-Control and WT-CA)). These data indicate that hAPP-J20 mice exhibited substantial deficits in the learning phase of the water maze test. This finding is consistent with previous reports that showed learning deficits in 6-month-old hAPP-J20 mice.15 Importantly, however, although the hAPP-J20 mice that received CA treatment (J20-CA) took a significantly longer time to reach the platform on the first day of training compared to WT, by subsequent training day 3 they took a similar amount of time to reach the platform (Figure 2a). This finding indicates that treatment with CA improves learning in hAPP-J20 mice. We next tested memory function in these mice. However, because we found that only a small number of hAPP-J20 vehicle-treated (control) mice learned the location of the hidden platform during our training protocol (Figure 2a), their memory could not be tested. Therefore, we performed a ‘non-inferiority trial’ to compare the effect of CA on hAPP-J20 mice vs. WT to determine if they were statistically different or not. We tested three groups of mice (WT-Control, WT-CA and J20-CA) in probe trials in which the platform was removed from the pool and the time spent in different quadrants of the pool was recorded during a 60-second-long trial. As shown in Figure 2b, all three groups of mice spent significantly more time in the trained quadrant (bottom left) than in the opposite quadrant (top right). These data are consistent with the notion that these mice remembered the location of the hidden platform following the three days of training. Furthermore, we found that the performance of the J20-CA mice was not significantly different from the WT-Control and WT-CA groups. Collectively, these results indicate that treatment with CA improved hAPP-J20 mice in the learning phase of the Morris water maze test and normalized their performance in the memory phase.Figure 2CA treatment improves spatial learning in hAPP-J20 mice. (a) J20 and WT littermate controls were assessed for learning in the Morris water maze test after treatment with CA or vehicle (Control). Evaluation of time to find the hidden platform during training in the four groups of mice indicates that CA treatment improved spatial learning in J20 mice (n⩾6 mice/group; **P<0.03 by two-way ANOVA with Tukey’s multiple comparisons test). (b) Following training, a probe test, in which the hidden platform was removed, was performed to assess spatial memory. Mice were scored for the time spent in the bottom left quadrant (prior location of the platform) versus the top right quadrant (n⩾5 mice/group; **P<0.01 by Student’s t-test). The performance of J20-CA mice is not significantly different from those of WT-Control and WT-CA mice by two-way ANOVA with Tukey’s multiple comparisons test. Dashed line indicates time spent in a quadrant by chance alone (15 s)Full size imageCA Treatment rescues dendritic loss in hAPP-J20 mouse brain, as reflected by the number of MAP2-positive cellsWe next evaluated the effects of CA treatment on the dendritic/neuronal loss that occurs in hAPP-J20 mice.16 For this experiment, WT and hAPP-J20 mice received either vehicle or CA trans-nasally for 3 months starting at 3 months of age. We then prepared hippocampal sections from the four groups of mice (WT-Control, WT-CA, J20-Control and J20-CA) and performed quantitative confocal immunohistochemistry using MAP2 monoclonal antibody. WT-Control and WT-CA brains showed robust MAP2 signal in the hippocampus, while J20-Control exhibited a decrement in MAP2 signal (Figure 3a, top). When quantified by measuring percent (%) area of MAP2-positive neuropil (Figure 3a, bottom), J20-Control hippocampus exhibited a significant decrease in MAP2 signal. Remarkably, J20-CA hippocampus showed significantly greater MAP2 signal than that of J20-Control and similar to that of WT-Control and WT-CA. These results indicate that CA treatment can rescue the dendritic loss that occurs in hAPP-J20 mice.Figure 3CA treatment reverses deficits in neuropil and synaptic density in hAPP-J20 mice. Quantitative immunohistochemistry of hippocampal sections prepared from wild-type (WT) and J20 mice treated with CA or vehicle (Control). (a) Immunohistochemistry with MAP2 antibody. CA treatment restored MAP2 staining/neuropil density to normal levels in the hippocampus of hAPP-J20 mice. (b) Immunohistochemistry with synaptophysin antibody. CA treatment restored synaptophysin staining/synapse density to normal levels in both cortex and hippocampus of hAPP-J20 mice (n=4–10 mice/group; **P<0.01 by ANOVA). Scale bar, 250 μmFull size imageCA Treatment rescues synaptic loss in hAPP-J20 mouse brain, as reflected by synaptophysin stainingWe also examined the effects of CA treatment on the synaptic loss that occurs in the hAPP-J20 mouse model of AD.17 Quantitative confocal immunohistochemistry, expressed as percent (%) area decorated by synaptophysin antibody, revealed a significant reduction in synaptophysin signal in the J20-Control hippocampus compared to WT (Figure 3b). In contrast, the synaptophysin immunosignal was normal in J20-CA hippocampi, indicating that CA treatment rescues synaptic loss in the hAPP-J20 hippocampus.CA Treatment decreases astrocytosis in hAPP-J20 mouse brain, as reflected by the number of GFAP-positive cellsWe next examined the effects of CA treatment on the gliosis that occurs in hAPP-J20 mice.17 Immunohistochemistry using GFAP monoclonal antibody revealed a significant increase in GFAP signal in J20-Control but not in J20-CA hippocampus (Figure 4a). This finding indicates that treatment with CA can prevent gliosis in the hippocampus of hAPP-J20 mice.Figure 4CA treatment prevents reactive astrocytosis and blocks accumulation of Aβ protein aggregates in hAPP-J20 mice. Quantitative immunohistochemistry of hippocampal sections from wild-type (WT) and J20 mice treated with CA or vehicle (Control). (a) Immunohistochemistry with GFAP antibody. CA treatment decreased GFAP staining/astrocytosis in the hippocampus of hAPP-J20 mice. (b) Immunohistochemistry with Aβ antibody. Aβ protein aggregates were significantly decreased after treatment with CA (n=4–10 mice/group; **P<0.01 by ANOVA). Scale bar, 250 μmFull size imageCA Treatment reduces amyloid accumulation in hAPP-J20 mouse brain, as reflected by Aβ immunohistochemistryPrevious work had shown that CA could suppress production of Aβ in vitro,18 but heretofore this has not been demonstrated in intact brain. hAPP-J20 mice express high levels of Aβ peptide in multiple brain areas, including cortex and hippocampus.17 Therefore, we examined the effects of treatment with CA on Aβ levels in hAPP-J20 mouse brain. Quantitative confocal immunohistochemistry was performed using anti-human Aβ 4G8 antibody (recognizing amino-acid residues 17-24 of the Aβ peptide). WT-Control treated and WT-CA treated hippocampus showed undetectable levels of Aβ. In contrast, J20-Control treated hippocampus showed high levels of Aβ signal. Remarkably, levels of Aβ were significantly reduced in J20-CA treated brain sections (Figure 4b), indicating that CA treatment suppressed Aβ levels in hAPP-J20 mice in vivo.Effects of CA treatment on 3xTg AD mouse brainNext, we assessed the effects of CA treatment in a second AD mouse model in order to test for generalization of our findings. Since these mice develop neuropathological phenotypes more slowly than hAPP-J20 mice, we started CA treatment at age 6 months of age, trans-nasally administering CA or vehicle for the next 3 months. At 9 months of age, mice were killed, and hippocampal sections prepared for immunohistochemical analysis by quantitative confocal microscopy. As shown in Figure 5, CA treatment of 3xTg mice resulted in (1) increased MAP2 signal, (2) decreased GFAP signal and (3) increased synaptophysin labeling. In addition, 3xTg mice have been reported to manifest increased amounts of p-tau.19 We found that CA treatment reduced the levels of p-tau signal in 3xTg mouse brain. Taken together, these data support the notion that CA treatment rescues or ameliorates pathophysiological phenotypes of AD in two separate mouse models of the disease.Figure 5CA treatment improves AD brain markers in 3xTg AD mice. Quantitative immunohistochemistry of hippocampal sections from 3xTg AD mice treated with CA or vehicle (Control). After CA treatment, 3xTg AD hippocampus displayed increased MAP2/neuropil and synaptophysin/synaptic densities, and reduced GFAP/astrocytosis and p-tau (n=8–9 mice/group; *P<0.05, **P<0.01 by t-test). Scale bar, 40 μmFull size image
Discussion|We have previously shown that pro-electrophilic drugs (PEDs) that activate the Keap1/Nrf2 transcriptional pathway can increase the levels of phase 2 antioxidant and anti-inflammatory enzymes in neural tissue to afford neuroprotection (Figure 6).7, 14 PEDs display an advantage over conventional electrophilic compounds that activate the Nrf2 pathway, including dimethylfumarate, in that they do not deplete glutathione from normal cells in the body; therefore, PEDs are expected to be better tolerated clinically than electrophilic drugs.7 Moreover, we and others have previously shown that CA crosses the blood-brain barrier and is detectable at neuroprotective concentrations in various CNS tissues when it is administered via various routes (intraperitoneal, oral and transnasal).20, 21, 22 Importantly, we have previously demonstrated ‘target engagement’ of CA via its reaction with a critical cysteine residue in Keap1 to activate Nrf2 in our models. We showed activation of the Nrf2 pathway via reporter gene assay, chromatin immunoprecipitation (ChiP), electrophoretic mobility shift assay (EMSA) and RNA and immunoblot readouts of phase 2 enzymes (reviewed in Satoh et al.7).Figure 6Schematic model showing action of a pro-electrophilic drugs (PEDs) in activating the Nrf2 transcriptional pathway. In this case the PED Carnosic Acid (CA) is activated by reactive oxygen species (ROS) to the active quinone form. This activated form of the drug reacts with a critical thiol (-SH) group on the cytoplasmic protein Keap1, which releases the transcription factor Nrf2. Nrf2 then is free to enter the nucleus where it transcriptionally activates a series of endogenous, neuroprotective antioxidant and anti-inflammatory proteins known as phase 2 enzymesFull size imageWhile electrophilic compounds, including kavalactone, decursin, sulforaphane, oleanolic acid, CDDO-methylamide and curcumin, have been shown to attenuate Aβ toxicity in cell-based or animal models,23, 24 PEDs have not previously been tested in vivo in transgenic models of AD. Accordingly, in the present study, we sought to test the effects of the PED, CA, in two different transgenic mouse models of AD. Specifically, we show that CA treatment of hAPP-J20 mice for three months ameliorates impairment in learning on the Morris water maze test. We also demonstrate that CA rescues or ameliorates histological damage in the hippocampus of both the hAPP-J20 and 3xTg AD mouse models. We found that treatment with CA decreased (i) dendritic loss, assessed by MAP2 staining, (ii) synaptic loss, assessed by synaptophysin staining, (iii) gliosis, assessed by GFAP staining and (iv) deposition of Aβ peptide. In addition, in the 3xTg model in which mutant tau is expressed, CA prevented accumulation of p-tau. Collectively, these data show that treatment with CA rescues various AD-related phenotypes in hAPP-J20 and 3xTg mice.Concerning the mechanism of these protective effects, our group previously showed that oxidative stress converts CA into an active electrophile that reacts with Keap1 at a critical cysteine residue. This results in less ubiquitination and degradation of Nrf2, leaving free Nrf2 to enter the nucleus to activate expression of endogenous phase 2 genes.7 These genes encode antioxidant enzymes, including heme oxygenase I (HO-1), γ-glutamyl cysteine synthetase (γ-GCS), NAD(P)H:quinone oxido-reductase (NQO-1) and sulfiredoxin (SRXN1).10 In AD, aberrant production of ROS and RNS are thought to interfere with normal cellular processes and thus contribute to the pathogenesis of neurodegeneration.25 Thus, it is likely that the neuroprotective effect of CA in vivo in AD transgenic mice occurs, at least in part, by activation of the Keap1/Nrf2 pathway and hence induction of antioxidant phase 2 enzymes.Moreover, CA may exert other actions in neurons via activation of Nrf2 in addition to induction of antioxidant enzymes. For example, CA was recently reported to suppress Aβ42 production and oligomerization in the SH-SY5Y neural cell line.18 The authors attributed this suppression of Aβ production to transcriptional upregulation of α-secretase (but not β-secretase) by CA. α-Secretase induces alternate cleavage of APP, avoiding generation of Aβ42. The upregulation of α-secretase transcription is potentially mediated by Nrf2 activation, but CA has been shown to activate other transcriptional pathways as well.7 In addition, activation of Nrf2 has been shown to decrease the level of phospho-tau protein,26 which is thought to mediate neuronal damage in AD downstream of aberrant Aβ activity.27In summary, based on these findings in vitro and in two AD transgenic mice in vivo, our study shows that CA represents a promising small molecule therapeutic that may be useful in AD. Furthermore, CA is found in the herbs rosemary and sage, and hence because of its long safety record is registered on the FDA’s ‘generally regarded as safe’ (GRAS) list. Hence, its excellent safety profile should expedite human clinical trials with this compound not only for AD, but also for other neurodegenerative diseases where oxidative stress is thought to contribute to pathogenesis.
Abstract|BackgroundOn the basis of data from a phase 2 trial that compared the checkpoint inhibitor ipilimumab at doses of 0.3 mg, 3 mg, and 10 mg per kilogram of body weight in patients with advanced melanoma, this phase 3 trial evaluated ipilimumab at a dose of 10 mg per kilogram in patients who had undergone complete resection of stage III melanoma. MethodsAfter patients had undergone complete resection of stage III cutaneous melanoma, we randomly assigned them to receive ipilimumab at a dose of 10 mg per kilogram (475 patients) or placebo (476) every 3 weeks for four doses, then every 3 months for up to 3 years or until disease recurrence or an unacceptable level of toxic effects occurred. Recurrence-free survival was the primary end point. Secondary end points included overall survival, distant metastasis–free survival, and safety. ResultsAt a median follow-up of 5.3 years, the 5-year rate of recurrence-free survival was 40.8% in the ipilimumab group, as compared with 30.3% in the placebo group (hazard ratio for recurrence or death, 0.76; 95% confidence interval [CI], 0.64 to 0.89; P<0.001). The rate of overall survival at 5 years was 65.4% in the ipilimumab group, as compared with 54.4% in the placebo group (hazard ratio for death, 0.72; 95.1% CI, 0.58 to 0.88; P=0.001). The rate of distant metastasis–free survival at 5 years was 48.3% in the ipilimumab group, as compared with 38.9% in the placebo group (hazard ratio for death or distant metastasis, 0.76; 95.8% CI, 0.64 to 0.92; P=0.002). Adverse events of grade 3 or 4 occurred in 54.1% of the patients in the ipilimumab group and in 26.2% of those in the placebo group. Immune-related adverse events of grade 3 or 4 occurred in 41.6% of the patients in the ipilimumab group and in 2.7% of those in the placebo group. In the ipilimumab group, 5 patients (1.1%) died owing to immune-related adverse events. ConclusionsAs adjuvant therapy for high-risk stage III melanoma, ipilimumab at a dose of 10 mg per kilogram resulted in significantly higher rates of recurrence-free survival, overall survival, and distant metastasis–free survival than placebo. There were more immune-related adverse events with ipilimumab than with placebo. (Funded by Bristol-Myers Squibb; ClinicalTrials.gov number, NCT00636168, and EudraCT number, 2007-001974-10.)
IntroductionIpilimumab,|a fully human monoclonal antibody that blocks cytotoxic T-lymphocyte antigen 4 (CTLA-4) to augment antitumor immune responses, was approved by the Food and Drug Administration (FDA) and the European Medicines Agency in 2011 at a dose of 3 mg per kilogram of body weight for the treatment of advanced melanoma.1,2 On the basis of data from a phase 2 trial that indicated the potential for a dose of 10 mg per kilogram to have higher efficacy than the dose of 0.3 mg or 3 mg per kilogram in patients with advanced melanoma, although at a cost of more toxic effects,3,4 we conducted a phase 3 trial (European Organization for Research and Treatment of Cancer [EORTC] 18071) of ipilimumab at a dose of 10 mg per kilogram in patients who had resected regional lymph node–positive (stage III) melanoma with a high risk of recurrence.The likelihood of systemic metastatic disease among patients with stage III melanoma correlates closely with microscopic versus palpable nodal disease and with the number of positive nodes.5-7 The population of patients with stage III melanoma is heterogeneous, with disease-specific survival rates of 78% among patients with stage IIIA disease, 59% among those with stage IIIB disease, and 40% among those with stage IIIC disease.5-7 Even within the population of patients who have sentinel node–positive cancer, heterogeneity is remarkable and correlates closely with tumor load in the sentinel node (as defined by the Rotterdam criteria; see the Supplementary Appendix, available with the full text of this article at NEJM.org).8-10 Patients with a metastasis of more than 1 mm in the greatest dimension have a significantly higher risk of recurrence or death than those with a metastasis of 1 mm or less in the greatest dimension.8-10We previously reported the primary results of the EORTC 18071 phase 3 trial in which we compared adjuvant ipilimumab with placebo in patients with resected stage III melanoma.11 At a median follow-up of 2.7 years, adjuvant ipilimumab was associated with significantly prolonged recurrence-free survival, the primary end point, as compared with placebo (hazard ratio, 0.75; P=0.001). The results on the global health scale, which was the primary health-related quality-of-life end point, were not affected by ipilimumab.12 Approval from the FDA was granted in 2015 on the basis of the results of this trial. The effect of ipilimumab on overall survival and distant metastasis–free survival is important, given that the only other approved systemic therapy in the context of adjuvant therapy, interferon alfa, has a marginal effect on overall survival.13-15 Here, we report, at a median follow-up of 5.3 years, the efficacy of adjuvant therapy with ipilimumab on all survival end points in patients with high-risk stage III melanoma after complete lymph-node dissection.
Methods|Patients Eligible patients were 18 years of age or older with histologically confirmed cutaneous melanoma that was metastatic to regional lymph nodes. According to the American Joint Committee on Cancer 2009 classification, patients had stage IIIA melanoma (patients with N1a cancer [i.e., only one node involved with micrometastasis] had to have at least one metastasis measuring >1 mm in the greatest dimension) or stage IIIB or IIIC melanoma with no in-transit metastases (i.e., growing >2 cm away from the primary tumor but before reaching the nearest lymph node).2 Complete regional lymphadenectomy was required within 12 weeks before randomization. Exclusion criteria included an Eastern Cooperative Oncology Group (ECOG) performance-status score of more than 1 (on a scale from 0 to 5, with higher numbers indicating greater disability), autoimmune disease, uncontrolled infection, substantial cardiovascular disease (New York Heart Association functional class III or IV), a lactate dehydrogenase level of more than 2 times the upper limit of the normal range, use of systemic glucocorticoids, and previous systemic therapy for melanoma. Trial Design and Regimen In this randomized, double-blind, phase 3 trial, patients were enrolled at 99 centers in 19 countries. Registration was done centrally at the EORTC headquarters. A central interactive voice-response system was used for randomization and was based on a minimization technique.16 Randomization was stratified according to disease stage (stage IIIA vs. stage IIIB vs. stage IIIC with one, two, or three positive nodes vs. stage IIIC with four or more positive nodes) and geographic region (North America, Europe, or Australia). Local pharmacists, who were aware of the trial-group assignments, performed the randomization. Clinical investigators and persons collecting or analyzing the data were unaware of the trial-group assignments. Patients were randomly assigned in a 1:1 ratio to receive an intravenous infusion of ipilimumab at a dose of 10 mg per kilogram or placebo every 3 weeks for four doses, then every 3 months for up to 3 years or until disease recurrence, an unacceptable level of toxic effects, a major protocol violation, or withdrawal of consent (Fig. S1 in the Supplementary Appendix). The rules regarding the withholding of a dose of ipilimumab or placebo and the management of immune-related adverse events are detailed in the full trial protocol, available at NEJM.org. The primary end point was recurrence-free survival. The secondary end points included overall survival, distant metastasis–free survival, safety, and health-related quality of life. Assessments Patients in the two trial groups were assessed for recurrence and distant metastases every 3 months during the first 3 years and every 6 months thereafter. Physical examination and radiography of the chest, computed tomography, magnetic resonance imaging, or other imaging techniques were performed if indicated. Recurrence or metastatic lesions had to be histologically confirmed whenever possible. The first date when recurrence was observed was used in the analysis, regardless of the method of assessment. Recurrence-free survival was defined as the time from randomization until the date of first recurrence (local, regional, or distant metastasis) or death from any cause. Overall survival was defined as the time from randomization until death from any cause. Distant metastasis–free survival was defined as the time from randomization until the date of the first distant metastasis or death from any cause. Data on adverse events were collected for each group with the use of the Common Terminology Criteria for Adverse Events, version 3.0. Immune-related adverse events were determined programmatically from a prespecified list of terms from the Medical Dictionary for Regulatory Activities (MedDRA), which was updated according to each new version of MedDRA. Resolution of an immune-related adverse event of grade 3 or 4 was defined as an improvement to grade 1 or less. The grade 3 or 4 event with the longest time to resolution was selected for inclusion in the analysis. If the grade 3 or 4 event did not resolve, follow-up was censored at the last known date that the patient was alive. Similar analyses were repeated for immune-related adverse events of grade 2 through 5. Trial Oversight The trial protocol was approved by the EORTC protocol-review committee and by independent ethics committees. The trial was conducted in accordance with the Declaration of Helsinki and with Good Clinical Practice guidelines as defined by the International Conference on Harmonisation. All the patients provided written informed consent. The trial was funded and sponsored by Bristol-Myers Squibb. The trial was designed by the writing committee (the trial coordinator [the first author], the EORTC headquarters team, and a representative of the sponsor). Data were collected and computerized at the EORTC headquarters and were copied to the sponsor after the database lock. Data were analyzed independently at the EORTC headquarters and by the sponsor. The manuscript was written by two of the academic authors (the first and penultimate authors), all the coauthors commented on it, and editorial assistance was provided by professional medical writers paid by the sponsor. The two specified academic authors made the decision to submit the manuscript for publication, with the consent of all the other authors. The authors vouch for accuracy and completeness of the data and analyses and confirm the adherence of the trial to the protocol. An independent review committee, whose members were unaware of the trial-group assignments, assessed disease status and date of recurrence. An independent data and safety monitoring board assessed the safety and efficacy data every 6 months, without formal interim analyses. Only at the time of the final analysis of recurrence-free survival were interim analyses of overall survival and distant metastasis–free survival performed by an independent statistician, and the results were forwarded to members of the data and safety monitoring board. On-site source-data verification was provided by a clinical research organization. Statistical Analysis We planned for the trial to include 950 patients. In the initial protocol, we calculated that a total of 491 deaths would be required in order to provide the trial with 85% power to detect a difference in the 4.5-year overall survival rates of 42.3% in the placebo group and 52.0% in the ipilimumab group, corresponding to a hazard ratio for death of 0.76. Owing to an improvement in outcomes after recurrence (because of a change in the treatment landscape for patients with melanoma), it was decided, by means of a protocol amendment, to perform the final analyses for overall survival and distant metastasis–free survival at the same time. Given the 506 events of distant metastasis or death and 376 deaths at the clinical cutoff date (January 31, 2016), it was recomputed (with the use of a Lan–DeMets alpha-spending function) that the final analyses of overall survival and distant metastasis–free survival be performed at two-sided alpha levels of 0.049 and 0.042, respectively, so the confidence interval for the hazard ratio of the group comparison regarding these end points was set at 95.1% and 95.8%, respectively; the statistical power was 75.8% and 89.4%, respectively. The statistical analysis plan (available with the trial protocol) indicated that in order to preserve the alpha error, a hierarchical-testing approach would be applied after the analysis of the primary end point of recurrence-free survival. Overall survival was tested first, followed by distant metastasis–free survival. For the subgroup analysis, the estimated hazard ratio was plotted along with its 99% confidence interval. The main analyses of the efficacy end points included all the patients who had undergone randomization, according to the intention-to-treat principle. The safety profile was assessed in patients who received at least one dose of the randomly assigned regimen. Details of the statistical methods are provided in the Supplementary Appendix.
Results|Patients and Trial Regimen Table 1. Table 1. Demographic and Clinical Characteristics of the Patients at Baseline.From July 2008 through August 2011, a total of 951 patients underwent randomization: 475 patients were assigned to the ipilimumab group and 476 to the placebo group. The characteristics at baseline were similar between the two randomized groups (Table 1). Six patients (4 patients in the ipilimumab group and 2 in the placebo group) did not start the randomly assigned regimen (Fig. S2 in the Supplementary Appendix). The median number of doses that were received was 4 doses (interquartile range, 3 to 8) in the ipilimumab group and 8 (interquartile range, 4 to 16) in the placebo group. At least 1 maintenance dose (dose 5 and beyond) was received by 198 of 471 patients (42.0%) in the ipilimumab group and by 332 of 474 (70.0%) in the placebo group. Of 471 patients who started ipilimumab, 251 (53.3%) discontinued treatment owing to an adverse event (including 182 patients [38.6%] who discontinued within 12 weeks after randomization); in 240 patients (51.0%), the event was considered by the investigators to be drug-related. Among 474 patients who received placebo, 22 (4.6%) discontinued treatment owing to an adverse event. A total of 135 patients (28.7%) in the ipilimumab group discontinued treatment because of disease recurrence, as compared with 282 (59.5%) in the placebo group. A total of 63 patients (13.4%) in the ipilimumab group and 143 (30.2%) in the placebo group completed the 3-year treatment period (Fig. S2 in the Supplementary Appendix). The overall median follow-up was 5.3 years. The median follow-up was 5.3 years in the ipilimumab group and 5.4 years in the placebo group. Efficacy and Postprotocol Treatment Figure 1. Figure 1. Kaplan–Meier Estimates of Recurrence-free Survival (RFS), Overall Survival, and Distant Metastasis–free Survival (DMFS).Panel A shows the Kaplan–Meier estimate of recurrence-free survival according to the independent review committee. In the ipilimumab group, local or regional recurrence was reported in 96 patients, distant metastasis or death due to melanoma in 157, and death due to another cause or an unknown cause in 11. In the placebo group, local or regional recurrence was reported in 114 patients, distant metastasis or death due to melanoma in 204, and death due to another cause or an unknown cause in 5. All the statistical comparisons shown here were stratified according to the disease stage as provided at randomization. In the comparison that was stratified according to the disease stage as given on case-report forms, the hazard ratio for recurrence or death was 0.77 (95% CI, 0.65 to 0.90; P=0.001). In a per-protocol analysis of the comparison that was stratified according to the disease stage as given at randomization, the hazard ratio was 0.77 (95% CI, 0.65 to 0.91; P=0.002). Panel B shows the Kaplan–Meier estimate of overall survival. Because the number of patients with a follow-up of more than 7 years was too small, the estimated median overall survival was either unreliable or not reached. In the comparison that was stratified according to the disease stage as given on case-report forms, the hazard ratio for death was 0.73 (95.1% CI, 0.60 to 0.90; P=0.003). In a per-protocol analysis of the comparison stratified according to the disease stage as given at randomization, the hazard ratio was 0.72 (95.1% CI, 0.58 to 0.89; P=0.002). Panel C shows the Kaplan–Meier estimate of distant metastasis–free survival according to the independent review committee. In the comparison that was stratified according to the disease stage as given on the case-report forms, the hazard ratio for distant metastasis or death was 0.77 (95.8% CI, 0.65 to 0.93; P=0.004). In a per-protocol analysis of the comparison stratified according to the disease stage as given at randomization, the hazard ratio was 0.76 (95.8% CI, 0.63 to 0.91; P=0.003).In this updated analysis, the rate of recurrence-free survival at 5 years was 40.8% in the ipilimumab group, as compared with 30.3% in the placebo group (hazard ratio for recurrence or death, 0.76; 95% confidence interval [CI], 0.64 to 0.89; P<0.001) (Figure 1A). The overall significant prolongation of recurrence-free survival that was due to adjuvant ipilimumab appeared to be consistent across subgroups (Fig. S3A in the Supplementary Appendix), but the trial was not powered to provide robust subgroup analysis. Ipilimumab appeared to be helpful in patients with microscopic involvement (hazard ratio vs. placebo, 0.68) and in patients with macroscopic involvement (hazard ratio, 0.84) (Fig. S3B and S3C in the Supplementary Appendix). Table 2. Table 2. Postprotocol Treatment in All the Patients Who Underwent Randomization and in Those Who Had Disease Recurrence or Died.Among the 264 patients in the ipilimumab group who had recurrence or died, 194 had received at least one postprotocol treatment (Table 2). These treatments included ipilimumab (24 patients), anti–programmed death 1 (PD-1) therapy (24 patients), and a BRAF inhibitor (63 patients). Among the 323 patients in the placebo group who had recurrence or died, 250 received postprotocol treatment: ipilimumab (76 patients), anti–PD-1 therapy (30 patients), and a BRAF inhibitor (88 patients). Overall survival after disease recurrence was similar in the two trial groups (hazard ratio for ipilimumab vs. placebo, 0.89), which suggests that the difference in recurrence-free survival would persist in terms of overall survival. Figure 2. Figure 2. Forest Plot for Overall Survival, According to Trial Group.Results are expressed as unstratified hazard ratios for the risk of death in the ipilimumab group as compared with the placebo group with 95% confidence intervals for the analysis of the total group and with 99% confidence intervals for all the subgroup analyses. The size of the box is proportional to the total number of deaths reported in each subgroup, the diamond is centered on the overall hazard ratio for death and covers its 95% confidence interval, and the dashed line represents the overall hazard ratio for death. The P value for the univariate analysis that included all patients was provided by the unstratified log-rank test. The P value for the analysis of heterogeneity between the hazard ratios computed within the subgroups of a given variable was provided by the test of heterogeneity (see the Supplementary Appendix). The disease stage, according to the case-report forms, was determined with the use of the American Joint Committee on Cancer 2002 criteria. The number of positive lymph nodes was determined by means of pathological testing. Additional information is provided in Fig. S4A in the Supplementary Appendix.The overall survival rate at 5 years was 65.4% (95% CI, 60.8 to 69.6) in the ipilimumab group, as compared with 54.4% (95% CI, 49.7 to 58.9) in the placebo group. Overall survival was significantly longer in the ipilimumab group than in the placebo group (hazard ratio for death from any cause, 0.72; 95.1% CI, 0.58 to 0.88; P=0.001) (Figure 1B). The prolongation of overall survival with ipilimumab was generally consistent across subgroups (Figure 2, and Fig. S4 in the Supplementary Appendix). The rate of distant metastasis–free survival at 5 years was higher in the ipilimumab group than in the placebo group (48.3% vs. 38.9%; hazard ratio for distant metastasis or death, 0.76; 95.8% CI, 0.64 to 0.92; P=0.002) (Figure 1C). Safety Table 3. Table 3. Immune-Related Adverse Events.Among the 471 patients who received ipilimumab, 465 (98.7%) had an adverse event of any grade, with grade 3 or 4 adverse events occurring in 255 patients (54.1%); among the 474 patients who received placebo, 432 (91.1%) had an adverse event of any grade, with grade 3 or 4 adverse events occurring in 124 (26.2%) (Table S1 in the Supplementary Appendix). Immune-related adverse events during the trial were more frequent with ipilimumab than with placebo (Table 3). Immune-related adverse events of grade 3 or 4 occurred in 41.6% of the patients in the ipilimumab group and in 2.7% of those in the placebo group. The most common immune-related adverse events of grade 3 or 4 in the ipilimumab group were gastrointestinal (in 16.1% of the patients), hepatic (in 10.8%), and endocrine (in 7.9%). The median time to the onset of immune-related adverse events of grade 2 through 5 during the trial ranged from 4.0 weeks (skin immune-related adverse events) to 13.1 weeks (neurologic immune-related adverse events) (Table S2 in the Supplementary Appendix). Endocrine immune-related adverse events of grade 2 through 5 resolved in 51.5% of the patients, and the median time to resolution was 54.3 weeks. The majority (82 to 97%) of all other immune-related adverse events of grade 2, 3, or 4 resolved, and the median time to resolution ranged from 4.0 to 8.0 weeks. Five patients (1.1%) died owing to adverse events that were attributed to ipilimumab: three patients died from colitis (two patients with intestinal perforation), one patient from myocarditis, and one patient from multiorgan failure that was associated with the Guillain–Barré syndrome. These deaths occurred before the start of maintenance therapy. Of these patients, four had received glucocorticoids and one anti–tumor necrosis factor antibodies.
Discussion|In this randomized, phase 3 trial involving patients with resected, high-risk stage III melanoma, ipilimumab at a dose of 10 mg per kilogram significantly prolonged overall survival and distant metastasis–free survival as compared with placebo. The risk of death was 28% lower with ipilimumab than with placebo, and the risk of distant metastasis or death was lower by 24%. At 5 years, ipilimumab treatment was associated with rates that were approximately 10 percentage points higher than the rates with placebo for all end points: recurrence-free survival (40.8% vs. 30.3%), overall survival (65.4% vs. 54.4%), and distant metastasis–free survival (48.3% vs. 38.9%). The results show that at the cost of substantial toxic effects, the previously observed prolongation of recurrence-free survival with ipilimumab is confirmed in the current updated analysis and that it translated into a prolongation in overall survival and distant metastasis–free survival. Despite successful treatment with surgery (followed by adjuvant therapy in patients at high risk for disease recurrence), only approximately 45% of patients with stage III melanoma will be disease-free after 4 years; less than 40% of patients who have surgery alone will be disease-free after 4 years.17 Interferon alfa is currently approved in both the United States and the European Union for the treatment of stage III melanoma after surgery. In a literature-based meta-analysis of 17 randomized, controlled trials involving 8122 patients with high-risk cutaneous melanoma, interferon alfa prolonged the time to recurrence (hazard ratio for disease recurrence with interferon alfa vs. observation, 0.82).15 Owing to the marginal benefit in overall survival (hazard ratio for death, 0.89) and the considerable toxic effects, interferon alfa is not widely used as an adjuvant therapy.18 Although the benefit–risk profile of interferon alfa as compared with ipilimumab remains unclear, a phase 3 trial (ECOG 1609) that directly compares interferon alfa with ipilimumab at a dose of 3 or 10 mg per kilogram in patients with resected stage III or IV melanoma is ongoing (ClinicalTrials.gov number, NCT01274338). In our trial (EORTC 18071), patients were treated for up to 3 years, but only 13.4% of the patients completed this treatment period and 40% had stopped ipilimumab treatment at the end of the first four doses over the first 3 months. Thus, the EORTC 18071 trial cannot address whether maintenance treatment is necessary. In the current trial, the survival benefit of ipilimumab over placebo was generally consistent across subgroups. This benefit was observed not only in patients with microscopic involvement only (sentinel node–positive) but also in patients with macroscopic or palpable nodes. In contrast, in previous EORTC trials of adjuvant therapy in patients with melanoma,17,19-22 a significant benefit with interferon alfa was observed only in patients with microscopic involvement. Similarly, in contrast to interferon alfa, for which ulceration is the overriding determinant of activity,13,17,20-23 ipilimumab prolonged survival among patients with nonulcerated melanoma and among those with ulcerated melanoma. The rate of adverse events with ipilimumab in the context of adjuvant therapy was substantial and led to the discontinuation of treatment in approximately 40% of the patients by the end of the initial dosing period (i.e., before maintenance therapy). This frequency is higher than that observed with the same dose in the pooled analysis involving patients with advanced melanoma.2-4 The vast majority of immune-related adverse events of grade 2, 3, or 4 resolved within 4 to 8 weeks with the use of established management guidelines. However, for endocrinopathies, the median time to resolution was 54 weeks, and 48.5% of the patients who had endocrinopathy continue to take hormone-replacement therapies. In this trial, adjuvant therapy with ipilimumab was associated with a higher risk and greater degree of diarrhea, insomnia, and fatigue than placebo during the induction period, but ipilimumab did not have a negative effect on the global health scale of health-related quality of life.12 Of concern are the five patients (1.1%) in the ipilimumab group who died owing to drug-related adverse events. In the context of adjuvant therapy, the benefit–risk profile is particularly important in view of the prognostic heterogeneity observed in patients with stage III melanoma. In conclusion, adjuvant ipilimumab was associated with clinical improvements and significantly prolonged overall survival and distant metastasis–free survival, as compared with placebo, among patients with high-risk stage III melanoma, thus extending previous findings of a prolongation of recurrence-free survival. Adverse events were common but mostly transient. Some adverse events were serious, and even death from treatment occurred despite the use of established treatment algorithms.
Abstract|Childhood trauma is associated with premature declines in health in midlife and old age. Pathways that have been implicated, but less studied include social-emotional regulation, biological programming, and habitual patterns of thought and action. In this study we focused on childhood trauma’s influence via alterations in social-emotional regulation to everyday life events, a pathway that has been linked to subsequent health effects. Data from a 30-day daily diary of community residents who participated in a study of resilience in Midlife (n = 191, Mage = 54, SD = 7.50, 54% women) was used to examine whether self-reports of childhood trauma were associated with daily well-being, as well as reported and emotional reactivity to daily negative and positive events. Childhood trauma reports were associated with reporting lower overall levels of and greater variability in daily well-being. Childhood trauma was linked to greater reports of daily negative events, but not to positive events. Focusing on emotional reactivity to daily events, residents who reported higher levels of childhood trauma showed stronger decreases in well-being when experiencing negative events and also stronger increases in well-being with positive events. For those reporting childhood trauma, higher levels of mastery were associated with stronger decreases in well-being with negative events and stronger increases in well-being with positive events, suggesting that mastery increases sensitivity to daily negative and positive events. Our results suggest that childhood trauma may lead to poorer health in midlife through disturbances in the patterns of everyday life events and responses to those events. Further, our findings indicate that mastery may have a different meaning for those who experienced childhood trauma. We discuss social-emotional regulation as one pathway linking childhood trauma to health, and psychosocial resources to consider when building resilience-promoting interventions for mitigating the detrimental health effects of childhood trauma.
Introduction|Development is a lifelong process with experiences from childhood potentially having an impact on health and well-being throughout in midlife and beyond [1–3]. Childhood trauma characterized by abuse and family conflict is one of those early life experiences that not only has detrimental effects during childhood and adolescence, but can leave a “scar” well into midlife and old age [4–5]. Recent empirical evidence suggests that childhood trauma is associated with less emotional support and more strain in social relationships in adult life [6], lower levels of well-being [7], and early onset of functional limitations, disease, and premature mortality [8–10].There are several possible pathways linking childhood trauma to health in midlife, including social-emotional regulation, biological programming, and patterns of behavior [4, 11]. For example, chronic stress in childhood is associated with stronger pro-inflammatory cytokine response and resistance to anti-inflammatory properties of cortisol [12], which have long-term health consequences [13]. Similarly, chronic childhood stress is linked to unhealthy habits, such as smoking, alcohol dependency, and overeating [14, 15]. One likely pathway that has yet to be fully tested is whether childhood trauma alters day-to-day life experiences through daily well-being, and experience of and reactivity to daily negative and positive events. These components of daily life are considered a form of social-emotional regulation that has the potential to accumulate over the lifespan to shape the course of development [16–19]. As is accustomed in the daily diary literature, we define well-being as one’s level of negative and positive affect on days when no negative or positive daily event is reported [16, 19]. Social-emotional regulation is broadly defined as one’s ability to effectively manage their daily emotions in the response to specific stimuli [20]. We conceptualize social-emotional regulation in our study as changes in positive and negative affect in response to the stimulus provided by daily negative and positive events. There may be psychosocial resources that moderate one’s social-emotional regulation capacities; we examine whether mastery, a key resilience resource across the life-span, increases or decreases social-emotional regulation capacities for individuals who experienced high levels of childhood trauma. To evaluate associations between childhood trauma and social-emotional regulation, we used data from 30-day daily diaries of participants in midlife to examine (1) whether childhood trauma is associated with daily well-being, and reports of and reactivity to daily negative and positive events and (2) the role of mastery in moderating such associations.Pathways Linking Childhood Trauma to HealthChildhood trauma in the form of emotional, physical, or sexual abuse, as well as childhood misfortune can have detrimental and long-lasting effects on development across the lifespan. For example, a meta-analysis by Wegman and Stetler [21] found that the effect size linking childhood abuse to negative physical health outcomes in adulthood was d = 0.42 (Confidence Interval = 0.39–0.45). Similarly, Caspi and colleagues [22–23] found that child maltreatment and traumatic events early in life were associated with lower well-being and greater psychological distress in young adulthood. Despite the substantial evidence linking childhood trauma to health in midlife and old age, much less empirical research has been done on examining specific pathways that could underlie this relationship.The ramifications of childhood trauma for development in midlife and old age are thought to be set in motion through biopsychosocial processes that unfold over time to increase risk for ill health. Miller and colleagues [4] reviewed evidence for several pathways, including biological programming, behavioral habits, social relationships, and social-emotional regulation (reports of and reactivity to stressors), as plausible mechanisms. We focus on exploring social-emotional regulation as one pathway through which childhood trauma can lead to differential health outcomes because childhood trauma has the potential to affect the dynamics of daily life. Day-to-day living accumulates to affect development across the lifespan through various sets of experiences, interactions, and events. For example, Charles and colleagues [24] found that people who were more reactive to daily stressors (i.e., stronger decline in well-being) had an increased risk for mental health disorders over 10 years of time. One might describe these experiences as being comprised of multiple components, which we have captured through use of daily diaries completed by a mid-life sample. The diary data allows us to investigate levels of and variability in daily well-being, reports of both stressors and pleasant events, and emotional reactivity to daily negative and positive events. Well-being has many definitions in the literature, but a common feature to most of those definitions is attention to both the presence of positive affective states and the relative absence of negative emotions [25–27]. Following in this vein we operationalized well-being as according to one’s daily levels of and variability in negative and positive affect, in addition to how negative and positive affect changes in response to daily negative and positive events. Below we set forth the assumptions upon which our investigation is based.First, childhood trauma may be associated with reporting lower overall levels of well-being and greater fluctuations from day-to-day (i.e., variability). Childhood trauma could alter individuals’ strategies for regulating their desires and emotions that are essential for interpreting and experiencing their daily lives in context [28–29]. Second, childhood trauma may lead to poorer health via reporting more daily stressful events and fewer daily positive events. Daily events may be more likely to be appraised and perceived as stressful and high in severity, due to inconsistencies and growing up in an environment where behavior-event relationships were not developed due to a harsh childhood environment, e.g., contingency[30]. Childhood trauma could also result in being less engaged in goal-directed behaviors that are associated with daily positive events, resulting in deriving less benefit from these events in daily life. Third, childhood trauma may prime individuals to be more sensitive to their daily events or context; this is conceptualized as one’s emotional reactivity and assessed via examining changes in well-being on days where daily negative and positive events are reported. In adolescence, childhood maltreatment is associated with poor self-regulation deficits when encountering social stressors [31], suggesting that deficits are already present early in life and likely worsen into adulthood. Similarly, previous research in adulthood shows that early life adversity and poor parental relationship quality is associated with stronger increases in negative affect to daily stressors [32–34].In sum, there are multiple social-emotional regulation pathways through which childhood trauma may play a role in creating self-regulation deficits. Accordingly, childhood trauma may lay the groundwork for such deficits extending into midlife and beyond. These sorts of outcomes indicate that such inadequate regulatory response to stress compromises resilience.The experience of well-being Early life adversity may lead to increased sensitivity to ongoing stressors, but also differentially boost well-being through greater responsiveness to lasting changes that have a positive valence. This concept has been referred to in the literature as differential susceptibility, which is broadly defined as individual differences in one’s response to both negative and positive social contexts [8, 35]. More specifically, not only does differential susceptibility refer to a heightened reactivity to adversity, “these same attributes that make an individual particularly sensitive to adversity may also make them more responsive to interventions designed to offset the effects of adversity” [8].One way to assess differential susceptibility is by assessing individuals’ emotional reactivity to daily social stressors and positive interactions. Despite the difficulties experienced early in life, positive daily events may provide individuals the opportunity to be engaged in their daily life and pursue goal-directed behavior, resulting in the opportunity to flourish. Most of the literature examining emotional reactivity in the context of childhood trauma has primarily focused on daily stressors, whereas emotional reactivity to daily positive events has been rarely addressed [19]. Positive events, such as completing a fulfilling project at work or a social interaction with family member or friend may boost well-being differentially with those who experienced childhood trauma because of being more sensitive to their context.Mastery as a Key Resilience Factor for Confronting Childhood TraumaThe evidence provided by prior research makes clear the potential for negative consequences arising from childhood trauma. However, there is a resilience perspective on childhood adversity that is instructive insofar as it gives voice to the potential for abused persons to recover, sustain their sense of purpose and even thrive [36]. Masten [37] described the need for researchers to examine more than the effects of deficits such as early abuse; rather, to investigate how “assets, risks, and protective factors… may influence each other over time.” Positive influences include access to caring adults, cognitive skills, and personal mastery [37, 38]. In the present study, we focus on one of these personal assets as a protective factor for individuals who experienced childhood trauma: personal mastery.Personal mastery Mastery, or perceived control, has a long history in the literature of being associated with better cognitive, mental, and physical health across the lifespan [39–42] and being a resource for caregivers and patient populations to protect against declines as a function of chronic stressors [43–44]. For our purposes, mastery is considered a resource for individuals with childhood trauma to protect against reports of and emotional reactivity to daily negative events, and could also result in positive events being more uplifting (i.e., increase in well-being). Previous research in daily diary designs has shown that mastery is protective against declines in well-being as a function of daily stressors [45]. For example, Hay and Diehl [46] found that reporting higher levels of control on stressor days was associated with a less steep increase in negative affect [47]. Mastery likely serves this protective function through individuals’ perceptions of higher control over the situation, using better coping or compensatory strategies, and turning to social network members to help reduce the negative effects of stressors. In contrast, much less is known regarding mastery in the context of daily positive events. Reporting more mastery may result in more uplifting or boosts in well-being with daily positive events because these events are often times engaged in or directed by the individual [19]. Insights on the role of mastery in the context of daily positive events may be gained through considering research on agency and self-efficacy. Bandura described self-efficacy, people’s beliefs about their ability to control events, as a mechanism of agency, which may be thought of as goal-directed determination and motivation [48]. In the context of childhood trauma, a greater sense of agency might be highly stabilizing, resulting in greater reactivity with positive events.The view of oneself as the effective agent of one’s own life, once established and reinforced by lengthy and diverse experience, is carried forward into late life as a powerful influence on the current mastery of older adults [49]. However, in the context of childhood trauma, the role of mastery is less clear. We envision several scenarios regarding whether mastery will up- or down-regulate reports of and emotional reactivity to daily negative and positive events. Perceiving control over life can be potentially beneficial given early life adversity. For daily negative events, following previous research [45, 47], we expect that higher levels of mastery would be protective against declines in well-being when confronted with a daily negative event [50]. However, adults with high mastery may expect to see their world as under their control and daily stressors can effectively disconfirm those expectations, possibly giving rise to distress. Focusing on positive daily events, high mastery may result in stronger increases in well-being because positive events are often sought after and perceived as under one’s own control [19]. The person is an active agent rather than a passive recipient in the production of positive events [51–52] and this is amplified in the context of childhood trauma.The Present StudyEarly life adversity in the form of childhood trauma can have a long-term impact on health in midlife and beyond; we focus on whether childhood trauma is associated with social-emotional regulation as one potential pathway underlying this association. To do so, we use data from a 30-day daily diary of participants in midlife that permits the opportunity to examine within-person change in daily well-being and tracking within-person change in well-being as a function of daily negative and positive events.Our first objective is to examine whether childhood trauma is associated with levels and variability of daily well-being, and reports of and emotional reactivity to daily negative and positive events. We hypothesize that childhood trauma will be associated with reporting lower levels of, and greater variability in, daily well-being. Furthermore, childhood trauma will be linked to reporting more daily negative events and less engagement in daily positive events, but stronger emotional reactivity to both daily negative and positive events. Our second research question centers on the role of mastery moderating reports of and emotional reactivity to daily negative and positive events. We hypothesize that mastery will be protective against declines in well-being with daily stressors for people with high levels of childhood trauma and stronger increases in well-being as a function of daily positive events.
Method|Participants and ProcedureWe used data from the ASU Live Project, which is a large-scale study of mid-aged (aged 40–65) residents of the Phoenix metro area (N = 800) focusing on identifying individual, familial and community factors in resilience (Resilience Processes in Individuals and Communities: R01 AG26006). A total of 800 participants were recruited for the study, with a final number of 782 participating in the initial component of the study that involved self-report questionnaires. The study was multi-modal. Participants completed self-report questionnaires about early family life, personality, traumatic and stressful events, as well as qualitative interviews about participants’ most stressful life experience. One-quarter of the sample (~200) participated in an additional videotaped lab stressors component and one-quarter (~200) completed daily diaries covering a 30-day period. The laboratory stressor employed procedures for eliciting physiological and affective arousal [53]. Participants were given a stress-eliciting task that was videotaped and ratings of positive and negative affect were collected with the PANAS [27] at baseline and after each rest and task period. Saliva samples (for cortisol) were collected at baseline and following each rest and stress period.We use data from 191 participants who completed the 30-day daily diary and provided data on our variables of interest from the self-report questionnaires. On average, participants were 54 years of age (SD = 7.45, range 40–65), 54% were women, and 75% attended some college. The data used for the present study can be found in S1 Dataset.Sampling and Recruitment The parent study employed a purposive sampling strategy, also referred to as sampling for heterogeneity [54, 55], to recruit eight hundred participants from 40 census tracts across the metropolitan Phoenix area between the years 2007 and 2012. This method increased the external validity of research findings through representativeness of individuals, environment and measured outcomes. Census tracts were established through a factor analysis that yielded five dimensions that together explain 80% of the variance between census tracts. The five dimensions are: (1) social status (describing income, occupation, and education); (2) the presence of school-aged children and multi-person households; (3) retirement communities; (4) residential construction growth between 1995 and 2000; and (5) Native American communities.Potential participants were contacted in two ways: (1) through mailed recruitment letters printed in both English and Spanish, and (2) recruiters traveled to households approximately one week after letters were mailed to introduce themselves and the study, provide materials about the study, and request participation. Prior to participation, participants gave informed consent. Participants were compensated up to one hundred US dollars for participation in the main study (i.e., questionnaires, home visit, phone interview), and the 25% that was selected for the lab stressors and daily diaries were compensated up to an additional one hundred and forty US dollars. Inclusion criteria for recruitment were: (1) participant presently between the ages of 40 and 65 years, and (2) either English or Spanish speaking. Exclusionary criteria were: presence of physical, psychiatric or cognitive impairments during initial recruitment contact, as measured by the Mental Status Questionnaire [56]. No participants were excluded based on these criteria. Attempts were made to keep balance between genders. The study was approved by the Arizona State University Institutional Review Board. Prior to participation, participants gave written informed consent.Daily diaries The daily diaries collected provided accounts of participants’ daily life events near in time to the events, as they occurred. Participants were given PC tablets pre-loaded with structured questions that related to the day’s most positive and negative events, and questions designed to gauge affect. They were instructed to complete the diary each evening for 30 consecutive days, 30 minutes prior to going to sleep.MeasuresChildhood trauma We used the childhood trauma questionnaire (CTQ) to assess the degree to which individuals experienced trauma in childhood [57]. The CTQ is a retrospective report that assesses the degree to which individuals experienced emotional, physical, and sexual abuse before the age of 18 (M = 1.68, SD = 0.86, range: 1–4.8; α = 0.92). Items were answered on a 5-point Likert scale, ranging from 1 (never true) to 5 (very often true). Using the stem question, “when I was growing up” participants responded to 10 items. Sample items include “People in my family called me things like “stupid,” “lazy,” or “ugly;” “I believe that I was physically abused,” and, “Someone molested me.” Although the CTQ is a retrospective report, it has been used in clinical populations for guidelines for determining whether people reported significant emotional, physical, and sexual abuse during childhood [58]. We created a mean score across the ten items that assessed emotional, physical, and sexual abuse, with higher scores reflecting higher levels of childhood trauma. The CTQ was given in the self-report questionnaire prior to participants completing the 30-day diary.Mastery Mastery was assessed using the Pearlin Mastery Scale, α = .82 [43]. Items asked participants to rate the extent to which they believe their life is under their own control (“I can do just about anything I really set my mind to do”) using a 4-point Likert scale (1 = strongly agree to 4 = strongly disagree). Higher scores indicated more feelings of control over life circumstances. Mastery was given in the self-report questionnaire prior to participants receiving instructions on completing the 30-day diary.Daily diary: Negative and positive affect Each day, participants completed the positive affect and negative affect scale, which totaled 32 items, PANAS [27]. The Negative Affect scale consisted of 16 items that assessed a general dimension of aversive affective states, such as feeling distressed, sluggish, hostile, and sad. The Positive Affect scale consisted of 16 items that assessed a general dimension of uplifting or positive affective states, such as feeling happy, relaxed, cheerful, and calm. Respondents indicated how often they had felt this way during the past 24 hours on a 5-point scale ranging from 1 (very slightly/not at all) to 5 (extremely). Consistent with the daily diary literature [16, 19], well-being is defined as levels of negative or positive affect on days when no negative or positive event was reported and emotional reactivity is defined as changes in negative or positive affect on days when a negative or positive event was reported.Daily diary: Negative and positive daily events During completion of the daily diary each night on the Tablet, participants answered questions pertaining to daily negative and positive events. The specific wording for daily negative events was, “Think of the most stressful event that occurred today, even if it may not have been too stressful. Which category was this event in?” The categories were spouse/partner, family, friends, work, finances, health, other, and no stressful event. For daily positive events, the specific wording was, “Think of the most positive event that occurred today, even if it may not have been too positive. Which category was this event in?” The categories were spouse/partner, family, friends, work, finances, health, other, and no positive event. From these items, we created two dichotomous variables, one for negative events and one for positive events, to indicate whether or not participants reported a negative or positive event during the course of the given day. If participants reported a negative or positive event occurring in the domains of spouse/partner, family, friends, work, finances, health, or other, then the negative or positive dichotomous variable was coded as a 1, with a 0 for days indicative of no negative or positive event. Table 1 shows the breakdown of the frequency that each of the categories was reported during the course of the 30-day daily diary. We found that, on average, participants reported a negative event on 60% of the diary days, and, on average, participants reported a positive event on 79% of the diary days. The most frequent negative events reported were in the work, family, and other domains. The most frequent positive events reported were in the family, friend, and spouse/partner domains.Table 1Frequency of negative and positive daily events.Negative EventsPositive EventsEvent DomainObservations%Observations%None2,031401,08421Spouse/partner393872614Family541111,13822Friend198480616Work7861553311Finances31461363Health37472004Other45094559Open in a separate windowStatistical AnalysisMultilevel logistic regression model The first set of analyses focused on the extent to which childhood trauma was associated with differences in reports of a daily negative and positive event. Ultimately, our interest was in determining whether childhood trauma increased or decreased one’s likelihood of experiencing a negative or positive event over the course of the day. To do so, we used a multilevel logistic regression model, such that the log odds of the probability of reporting a negative or positive event was modeled as the outcome and childhood trauma was included as a person-level predictor. Models were estimated using SAS PROC GLIMMIX[59].Multilevel linear regression model The first set of analyses focused on the extent to which childhood trauma was associated with differences in reports of a daily negative and positive event. Ultimately, our interest was in determining whether childhood trauma increased or decreased one’s likelihood of experiencing a negative or positive event over the course of the day. To do so, we used a multilevel logistic regression model, such that the log odds of the probability of reporting a negative or positive event was modeled as the outcome and childhood trauma was included as a person-level predictor. Models were estimated using SAS PROC GLIMMIX [59].Multilevel linear regression model In a second set of analyses, we estimated a multilevel linear regression model [60] to examine whether childhood trauma moderated emotional reactivity to daily negative and positive events. Models were specified as WBti=β0i+β1i(negativeeventti)+β2i(positiveeventti)+eti(1) where person i’s level of well-being (either negative affect or positive affect) at day t, WBti, is a function of an individual-specific intercept parameter that represents levels of negative affect or positive affect on days when no negative or positive event was reported, β0i; an individual-specific emotional reactivity slope parameter, β1i, that captures rates of change in the outcome on days when a negative event was reported; an individual-specific emotional responsiveness slope parameter, β2i, that captures rates of change in the outcome on days when a positive event was reported and residual error, e ti.Following standard multilevel modeling procedures [60], individual-specific intercepts and slopes (βs from the Level 1 model given in Eq 1) were modeled as the Level 2 model where between-person differences were estimated (i.e., variance parameters) and are assume to be normally distributed, correlated with each other, and uncorrelated with the residual errors, e ti. The expanded model that included childhood trauma, mastery, and socio-demographics took the formβ0i=γ00+γ01(childhood traumai)+γ02(masteryi)+γ03(childhood traumaix masteryi)+γ04(agei)+γ05(genderi)+γ06(educationi)+u0iβ1i=γ10+γ11(childhood traumai)+γ12(masteryi)+γ13(childhood traumaix masteryi)β2i=γ20+γ21(childhood traumai)+γ22(masteryi)+γ23(childhood traumaix masteryi)(2)All models were estimated using SAS PROC MIXED [61], with incomplete data accommodated under missing at random assumptions at the within- and between-person levels [62].
Results|In a first step, we used the data from the 30-day daily diaries to create two aggregate measures of both negative affect and positive affect, namely a mean and standard deviation. The mean score represents one’s overall levels of negative affect and positive affect and the standard deviation score represents one’s fluctuations in negative affect and positive affect over the course of the 30-days. Table 2 shows the descriptive statistics for the variables included in the present study. The correlations from Table 2 suggest that reporting more childhood trauma was associated with overall higher levels of negative affect (r = .16, p <. 05) and lower levels of positive affect (r = −.21, p <. 05), in addition to greater variability in both negative affect (r = .21, p <. 05) and positive affect (r = .23, p <. 05). We also examined the association between childhood trauma and daily mean in negative and positive affect and variability in negative and positive affect using regression models. In the regression models, daily mean and variability in negative affect and positive affect were regressed on childhood trauma. We found substantively similar findings in that higher levels of childhood trauma were associated with higher negative affect, lower positive affect, and more variability in negative affect and positive affect. It is also worth noting the correlation between childhood trauma and mastery, r = −.24, p <. 05, suggesting that higher levels of childhood trauma is associated with reporting lower levels of mastery in midlife.Table 2Means, standard deviations, and intercorrelations among the constructs included in the study.Construct M SD 1234567891. Childhood trauma1.680.86–2. Mastery3.170.65–.24* –3. Age53.487.45–.04–.04–4. Gender0.540.50.12.004–.07–5. Education0.750.43–.03.09.19* .05–6. Mean negative affect1.260.35.16* –.24* –.02–.11.07–7. Mean positive affect3.130.80–.21* .23* .21* –.10.05–.21* –8. SD negative affect0.250.18.21* –.22* –.09.01.08.72* –.19* –9. SD positive affect0.500.23.23* .05–.08.12–.01.10–.20* .35* –Open in a separate window N = 191.*p <.05.To examine the data further, we created two groups, low and high levels of childhood trauma, based on a median split of the CTQ and examined whether there were significant differences in the study variables of interest. Table 3 shows that individuals who reported high levels of childhood trauma, on average, were more likely to report lower levels of mastery, more negative affect, less positive affect, and more variability in both negative and positive affect, but the two groups did not differ based on age, gender, and education. These preliminary results suggest that childhood trauma is associated reporting lower overall levels of and more fluctuations in well-being from day-to-day.Table 3Examining differences on key study variables based on low versus high levels of childhood trauma.Low Childhood Trauma (N = 124)High Childhood Trauma (N = 67) M (SD) M (SD)MeasuresChildhood Trauma1.19(0.20) a 2.57(0.87) b Mastery3.29(0.58) a 2.95(0.71) b Age54.02(7.54) a 52.48(7.25) a Gender0.50(0.50) a 0.62(0.49) a Education0.76(0.43) a 0.75(0.44) a Mean negative affect1.22(0.33) a 1.33(0.38) b Mean positive affect3.25(0.81) a 2.92(0.75) b SD negative affect0.23(0.18) a 0.29(0.17) b SD positive affect0.46(0.21) a 0.56(0.24) b Open in a separate window N = 191. Median split was done on childhood trauma questionnaire. Subscripts that differ between columns are statistically significant at p <.05.Linking Childhood Trauma to Daily Negative and Positive EventsReports of daily negative and positive events Table 4 shows our results from a series of analyses examining whether childhood trauma was associated with the likelihood of experiencing negative and positive events each day. Focusing on daily negative events, we found that childhood trauma was associated with an increased likelihood of reporting a negative daily event (Odds Ratio = 1.43, 95% confidence interval: 1.06, 1.94). This suggests that each one unit increase in childhood trauma is associated with a 43% increased likelihood of reporting a daily negative event on a given day. To examine this finding further, we divided our sample into two groups, people with higher levels of childhood trauma (+1 SD; n = 30) versus lower levels of childhood trauma (–1 SD; n = 64) and examined whether they differed in the percent of days where a daily negative event was reported. In comparing the two groups, we found that the high childhood trauma group, on average, experienced daily negative events on 66% of the diary days, whereas the lower childhood trauma group, on average, experienced negative events on 50% of the diary days. The inclusion of mastery and socio-demographics rendered the effect of childhood trauma no longer significant. Predictors that were associated with an increased likelihood of a daily negative event were lower mastery and more years of education. Focusing on daily positive events, childhood trauma was not associated with engagement in daily positive events (Odds Ratio = 0.78, 95% confidence interval: 0.53, 1.18). Attaining more years of education was associated with increased likelihood of engaging in daily positive events.Table 4Examining whether childhood trauma is associated with reporting a daily negative and positive event.Negative EventsPositive EventsOR[95% CI]OR[95% CI]OR[95% CI]OR[95% CI]Fixed effectsChildhood trauma1.43* [1.06, 1.94]1.20[0.89, 1.62]0.78[0.53, 1.18]0.70[0.46, 1.07]Mastery0.37* [0.25, 0.55]0.74[0.41, 1.31]Childhood trauma x mastery1.13[0.77, 1.66]1.01[0.59, 1.74]Age0.97[0.94, 1.00]1.02[0.97, 1.07]Gender0.90[0.56, 1.47]1.35[0.67, 2.75]Education2.59* [1.45, 4.61]3.23* [1.41, 7.41]Open in a separate window*p <.05.Emotional reactivity to daily negative and positive events Table 5 shows results from our multilevel linear regression model that examined whether childhood trauma moderated emotional reactivity to daily negative and positive events. First, we found that within-person daily negative events resulted in increases in negative affect (γ10 = 0.17, p <. 05) and decreases in positive affect (γ10 = −0.15, p <. 05) and that presence of a positive event was associated with decreases in negative affect (γ20 = −0.07, p <. 05) and increases in positive affect (γ20 = 0.38, p <. 05). Childhood trauma was a significant predictor of the within-person daily negative event effect, such that reporting higher levels of childhood trauma was associated with stronger increases in negative affect (γ11 = 0.03, p <. 05) and stronger declines in positive affect (γ11 = −0.07, p <. 05) on negative event days. Similarly, childhood trauma was a significant predictor of the within-person daily positive event effect, such that reporting more childhood trauma was associated with stronger increases in positive affect (γ21 = 0.11, p <. 05).2 In follow-up analyses, we examined whether emotional reactivity to stressful events differed by the source of type of event (spouse/partner, family, friends, work, finances, health, and other). For daily negative events, more childhood trauma was associated with stronger declines in well-being for negative events centered on friends and health. For daily positive events, more childhood trauma was associated with stronger increases in well-being for positive events centered on spouse/partner, family, friends, work, and other.Table 5Examining emotional reactivity in negative affect and positive affect as a function of daily negative and positive events.Negative AffectPositive AffectEstimate(SE)Estimate(SE)Fixed effectsIntercept (no negative or positive event), γ00 1.27* (0.03)2.93* (0.06)Childhood Trauma, γ01 0.06(0.03)–0.22* (0.07)Negative Event, γ10 0.17* (0.01)–0.15* (0.02)Negative Event x Childhood Trauma, γ11 0.03* (0.01)–0.07* (0.02)Positive Event, γ20 –0.07* (0.01)0.38* (0.02)Positive Event x Childhood Trauma, γ21 0.001(0.01)0.11* (0.02)Random effectsIntercept0.11* (0.01)0.58* (0.06)Residual0.09* (0.002)0.28* (0.01)Open in a separate windowIntraclass correlction (ICC): Positive affect = .672; Negative affect = .568.*p <.05. Fig 1 graphically illustrates the moderating effect of childhood trauma on within-person daily negative events. Compared to participants with lower levels of childhood trauma, participants who experienced more childhood trauma were more likely to report more negative affect and less positive affect on non-event days (left part of A and B in Fig 1). We see that stronger changes or steeper slopes in negative and positive affect as a function of daily negative events is indicative of people with childhood trauma being more emotionally reactive to daily negative events.Open in a separate windowFig 1Illustrating the moderating effect of childhood trauma on within-person daily negative events.Compared to participants with lower levels of childhood trauma, participants with higher levels of childhood trauma were more likely to report more negative affect and less positive affect on non-event days (left part of A and B in Fig 1). Stronger changes or steeper slopes in negative and positive affect as a function of daily negative events is indicative of people with higher levels of childhood trauma being more emotionally reactive to daily negative events. Fig 2 graphically illustrates the moderating effect of childhood trauma on within-person daily positive events. On days that a daily positive event was reported, people reporting higher levels of childhood trauma were more likely to show a stronger increase or boost in positive affect, suggesting that childhood trauma is associated with being more emotionally responsive to daily positive events.Open in a separate windowFig 2Illustrating the moderating effect of childhood trauma on within-person daily positive events.On days where a positive event was not reported, participants with higher levels of childhood trauma reported higher negative affect and lower positive affect (left part of A and B in Fig 2). People with higher levels of childhood trauma were more likely to show a stronger increase or boost in positive affect on days when a positive event was reported.In a subsequent model, we included mastery and socio-demographics to examine whether mastery up- or down-regulated emotional reactivity to daily negative and positive events. Table 6 shows results from these analyses. We found that in the context of high childhood trauma, mastery increased one’s emotional reactivity to daily negative (negative affect: γ13 = 0.04, p <. 05; positive affect: γ13 = −0.003, p >. 05) and positive (negative affect: γ23 = −0.04, p <. 05; positive affect: γ23 = 0.12, p <. 05) events. Reporting more mastery in the context of high childhood trauma was associated with stronger increases in negative affect on days where a negative event was reported and stronger increases in positive affect on days when a positive event was reported. Fig 3 graphically illustrates the three-way interaction among within-person daily negative and positive events and between-person childhood trauma and mastery. Part A of Fig 3 shows that high childhood trauma and high mastery is associated with stronger decreases in positive affect on days when confronted with a negative event (dotted line, circle end points). Part B of Fig 3 shows that high childhood trauma and high mastery is associated with stronger increases in positive affect on days when confronted with a positive event (dotted line, circle end points). Our findings suggest that for people who experienced childhood trauma, mastery increases one’s sensitivity to their daily event context.Open in a separate windowFig 3Illustrating the three-way interaction among within-person daily negative and positive events and between-person childhood trauma and mastery.Part A of Fig 3 shows that higher levels of childhood trauma and mastery is associated with stronger decreases in positive affect on days when confronted with a negative event (dotted line, circle end points). Part B of Fig 3 shows that higher levels of childhood trauma and mastery is associated with stronger increases in positive affect on days when confronted with a positive event (dotted line, circle end points). High mastery is represented by circle end points and black lines, whereas low mastery is represented by square end points and gray lines.Table 6Examining emotional reactivity in negative affect and positive affect as a function of daily negative and positive events: The role of mastery.Negative AffectPositive AffectEstimate(SE)Estimate(SE)Fixed effectsIntercept (no negative or positive event), γ00 1.27* (0.03)2.92* (0.06)Childhood trauma, γ01 0.05(0.03)–0.20* (0.07)Age, γ02 –0.001(0.003)0.02* (0.01)Gender, γ03 –0.09(0.05)–0.11(0.11)Education, γ04 0.07(0.06)–0.06(0.13)Mastery, γ05 –0.15* (0.04)0.25* (0.09)Childhood trauma x mastery, γ06 0.01(0.04)–0.11(0.09)Negative event, γ10 0.18* (0.01)–0.16* (0.02)Negative event x childhood trauma, γ11 0.04* (0.01)–0.10* (0.02)Negative event x mastery, γ12 0.01(0.02)–0.03(0.03)Negative event x childhood trauma x mastery, γ13 0.04* (0.02)–0.003(0.03)Positive event, γ20 –0.08* (0.01)0.41* (0.03)Positive event x childhood trauma, γ21 –0.001(0.01)0.15* (0.03)Positive event x mastery, γ22 0.08* (0.02)0.01(0.04)Positive event x childhood trauma x mastery, γ23 –0.04* (0.02)0.12* (0.03)Random effectsIntercept0.11* (0.01)0.52* (0.06)Residual0.09* (0.002)0.29* (0.01)Open in a separate windowIntraclass correlction (ICC): Positive affect = .672; Negative affect = .568.*p <.05.
Discussion|The objective of this study was to examine social-emotional regulation as one potential pathway linking childhood trauma to poorer health in midlife and beyond. We were able to test multiple components of social-emotional regulation using data from a 30-day daily diary of participants in midlife. We found that childhood trauma was associated with poorer overall levels of and greater fluctuations in daily well-being. Participants who reported higher levels of childhood trauma were more likely to also report stressors each day but their rate of reporting daily positive events did not differ. Furthermore, individuals with higher levels of childhood trauma were more emotionally reactive to daily negative and positive events, evidenced by stronger declines in well-being as a function of negative events and stronger increases in well-being with positive events (e.g., increased sensitivity to context, indicative of differential susceptibility). Lastly, mastery was not protective against decreases in well-being following negative events, but actually also increased one’s emotional reactivity to negative and positive events for people with childhood trauma. Our findings demonstrate how early life experiences still affect daily life in midlife and beyond. Furthermore, our results point to childhood trauma likely leading to poorer health in midlife through disturbances in the patterns of everyday life events and responses to those events.Pathways Linking Childhood Trauma to HealthChildhood trauma was associated with lower overall levels of and higher variability in daily well-being, which are both important constructs to take into consideration. How daily well-being is experienced likely operates through our daily behaviors and health practices that have subsequent effects on physiological processes that underlie the health effects of levels and variability in well-being [63]. For example, Ong and colleagues [64] found that sleep disturbance was associated with higher daily variability in positive affect in a mid-life sample, even though stable elevations in positive affect were associated with better sleep quality. Less consistency in daily well-being has also been tied to inflammatory markers associated with ill health [65].A second component from the daily diaries ascertaining whether those reporting childhood trauma were also more likely to report daily negative and positive events; there was an association with stressful events but not positive experiences. The findings pertaining to average number of days reporting a daily stressor when contrasting high versus low levels of childhood trauma are startling. Our results indicate that over the course of 30 days, those reporting high levels of childhood trauma (+1 SD) averaged one or more negative events on 20 days compared with an average of 15 days for those reporting low levels of childhood trauma (–1 SD). In extrapolating these numbers to over the course of a year, this results in 60 more days, on average, with daily negative events than those with little or no abuse in their childhoods. Evidence from other studies suggests that these small stressors can have cumulative effects on health and mental health in midlife and beyond [16–17, 19].Emotional reactivity to daily negative events Our findings that childhood trauma was associated with greater emotional reactivity to daily negative events is consistent with previous research on adolescents [31–33]. These vulnerabilities to daily life stressors have been linked to long-term consequences in a number of recent studies [24, 64, 66, 67] and may be driven by underlying physiology. For example, several underlying physiological changes that occur as a result of reactivity to daily negative events include activation of the sympathetic and parasympathetic nervous system [68]. These physiological pathways were not testable in our dataset, but should be explored in future research [69]. In sum, childhood trauma may lead to daily life being, on average, more stressful and unpleasant and over years and decades, this likely accumulates to shape the course of health in midlife and beyond.Emotional reactivity to daily positive events Childhood trauma was associated with greater increases in well-being on days that notable positive events occurred. This finding, coupled with showing that childhood trauma was associated with emotional reactivity to daily negative events, points to the possibility that childhood trauma makes individuals more sensitive to their daily context, otherwise known as differential susceptibility. Broadly speaking, differential susceptibility is defined as contexts through which individuals suffer with negative events and flourish with positive events [35, 70–71]. Childhood adversity may differentially boost well-being through greater responsiveness to lasting changes that have a positive valence. Early life adversity can lead to higher levels of inflammation (13), but positive emotions can offset increases in inflammation and improve immune functioning [72]. It is important to note that greater responsivity to positive events does not mean that those with greater childhood adversity had occasions when they were happier than those without those past experiences. As Fig 2 shows they approach but do not surpass levels of well-being of those without childhood trauma. Greater sensitivity, though, does suggest that this population would respond to interventions designed to help them attain more positive social experiences.Mastery as a Key Resilience Factor for Confronting Childhood TraumaBefore discussing the moderating role of mastery in the context of emotional reactivity to daily negative and positive events for those with childhood trauma, we note that childhood trauma and mastery were moderately correlated, r = −.24, suggesting that childhood trauma is associated with reporting lower levels of mastery. There is limited research in the childhood antecedents of mastery and this finding suggests that childhood trauma sets individuals on a developmental course of lower levels of mastery.Our findings on the effects of mastery did not fully match our hypotheses. Mastery was associated with a stronger increase in well-being on days when a positive event was reported. However, mastery was not always protective against declines in well-being, and even in fact exacerbated emotional reactivity to negative events in some cases. For people with high levels of childhood trauma, reporting more mastery beliefs were associated with greater decreases in well-being with daily negative events. Our findings suggest that mastery, rather being a positive personality feature across the board, increases one’s sensitivity to both good and bad events in everyday life.There are several plausible reasons for this finding. First, mastery beliefs likely constitute something different for people with high levels of childhood trauma. Individuals who experienced childhood trauma and report more mastery may have John Henryism, a term coined by James and colleagues as signifying someone who believes they can do all and need no help, when in fact with difficult stressors this is less adaptive [73]. Similarly, those reporting high levels of mastery may be low on other important resources for managing stressors such as perceived social support. Therefore, these individuals may be too self-sufficient and fearful of relying on others. Second, daily stressors are typically events, experiences, and interactions that are largely uncontrollable [74] and this could be compounded for people with childhood trauma and high in mastery. Being high on mastery may result in viewing the world in general as having control over life circumstances, but when encountered with uncontrollable daily events, this clashes with one’s expectations, resulting in stronger declines in well-being as a function of daily stressors.Resilience-Promoting InterventionsOur findings that higher levels of childhood trauma are associated with increased sensitivity to daily context (i.e., differential susceptibility) may result in people being more sensitive or responsive to interventions. There are various routes researchers can take to intervene for better health in midlife and beyond for people with high levels of childhood trauma. First, researchers can focus on increasing levels of mastery. The focus of interventions targeting mastery could be controllability and uncontrollability of daily life events, particularly, negative events. People need to acknowledge that certain daily events, whether they be negative or positive, may not be controllable. For example, Zautra and colleagues in a 25-day daily intervention focused on increasing mastery beliefs through daily phone calls, with messages that focused on identifying positive and negative experiences that were under their control, and encouraging participants to take action to improve their daily lives [75]. They found that, although mastery beliefs remained relatively stable over the course of the intervention, the intervention resulted in significant improvements in mental health [75].Second, social relationship interventions have broad-based applicability and could help people with childhood trauma reduce the toxic effects of stress on health and overall functioning [76–77]. Social Intelligence concepts have been embedded within the social and behavioral sciences literature for some time [78], and in current writings, these concepts refers to a keen awareness of the value of sustainable social connections, the ability to take another’s perspective, and the capacity to engage fully in satisfying relationships [79–80]. Focusing on the plasticity of social relationships through social intelligence training has the potential to benefit people with high levels of childhood trauma through modifying key social cognitions regarding social engagement, and enhance efficacy expectations regarding performance in social situations [81]. The approach extends beyond cognitive models and behavioral principals to include attention to evidence of barriers to social-emotional development from adverse experiences in childhood and adult life [76].Limitations and ConclusionWe note several limitations. First, our measure of childhood trauma was a retrospective self-report and people do not always report their childhood experiences, reliably. Prior studies have used the CTQ in clinical populations to obtain reliable accounts of emotional, sexual, or physical abuse in childhood [58]. The CTQ measure complements research from previous studies that have focused on childhood misfortune [3]. Second, our sample was drawn from the Phoenix metropolitan area, and though it may be fairly representative of middle-aged residents in the southwestern United States, its representativeness of midlife in other regions is unknown. Future research is needed on examine the extent to which similar associations are found in other samples of people in midlife, as well as those in young adulthood and old age. It may be that emotional reactivity to stressors is strongest for those individuals who are in young adulthood and experienced childhood trauma, as compared to people in older ages. Third, it is likely that daily well-being, and reports of and reactivity to daily negative and positive events have physiological consequences for subsequent effects on health. This is in line with research showing that stressors are associated physiological effects [68, 82–83] and future research is warranted that focuses on sympathetic and parasympathetic systems to determine how the physiological changes mediate the effects of stress to eventual health declines.Early life adversity in the form of childhood trauma has the potential to shape the course of development in midlife and beyond. We explored the extent to which daily social-emotional regulation links childhood trauma to premature declines in health, with a 30-day daily diary that allowed for probing multiple components of social-emotional regulation, such as daily level and variability in well-being, as well as experience of and reactivity to daily negative and positive events. Our findings suggest that one potential pathway linking childhood trauma to poorer overall health in midlife and beyond is through disturbances in the patterns of everyday events and responses to those events. We urge future inquiry examining specific mechanisms underlying these relationships.
Abstract|The development of the body plan in the Drosophila embryo depends on the activity of maternal determinants localized at the anterior and posterior of the egg. These activities define both the polarity of the anterior-posterior (AP) axis and the spatial domains of expression of the zygotic gap genes, which in turn control the subsequent steps in segmentation. The nature and mode of action of one anterior determinant, the bicoid(bcd) gene product, has recently been defined, but the posterior determinants are less well characterized. At least seven maternally acting genes are required for posterior development. Mutations in these maternal posterior-group genes result in embryos lacking all abdominal segments. Cytoplasmic transplantation studies indicate that the maternally encoded product of the nanos(nos) gene may act as an abdominal determinant, whereas the other maternal posterior-group genes appear to be required for the appropriate localization and stabilization of this signal. Here we show that the lack of the nos gene product can be compensated for by eliminating the maternal activity of the gap gene hunchback (hb). Embryos lacking both of these maternally derived gene products are viable and can survive as fertile adults. These results suggest that the nos gene product functions by repressing the activity of the maternal hb products in the posterior of the egg.
Abstract|BackgroundCytokeratins are specific markers of epithelial cancer cells in bone marrow. We assessed the influence of cytokeratin-positive micrometastases in the bone marrow on the prognosis of women with breast cancer. MethodsWe obtained bone marrow aspirates from both upper iliac crests of 552 patients with stage I, II, or III breast cancer who underwent complete resection of the tumor and 191 patients with nonmalignant disease. The specimens were stained with the monoclonal antibody A45-B/B3, which binds to an antigen on cytokeratins. The median follow-up was 38 months (range, 10 to 70). The primary end point was survival. ResultsCytokeratin-positive cells were detected in the bone marrow specimens of 2 of the 191 control patients with nonmalignant conditions (1 percent) and 199 of the 552 patients with breast cancer (36 percent). The presence of occult metastatic cells in bone marrow was unrelated to the presence or absence of lymph-node metastasis (P=0.13). After four years of follow-up, the presence of micrometastases in bone marrow was associated with the occurrence of clinically overt distant metastasis and death from cancer-related causes (P<0.001), but not with locoregional relapse (P=0.77). Of 199 patients with occult metastatic cells, 49 died of cancer, whereas of 353 patients without such cells, 22 died of cancer-related causes (P<0.001). Among the 301 women without lymph-node metastases, 14 of the 100 with bone marrow micrometastases died of cancer-related causes, as did 2 of the 201 without bone marrow micrometastases (P<0.001). The presence of occult metastatic cells in bone marrow, as compared with their absence, was an independent prognostic indicator of the risk of death from cancer (relative risk, 4.17; 95 percent confidence interval, 2.51 to 6.94; P<0.001), after adjustment for the use of systemic adjuvant chemotherapy. ConclusionsThe presence of occult cytokeratin-positive metastatic cells in bone marrow increases the risk of relapse in patients with stage I, II, or III breast cancer.
IntroductionThe|search for occult metastatic cells in patients with small, curatively resected tumors is of considerable importance, because early dissemination of tumor cells is one of the leading causes of relapse at distant sites1,2 and of death from cancer.3 Immunocytochemical methods to search for occult tumor cells in the bone marrow were originally used in patients with breast cancer,4 but the clinical significance of such cells is controversial. In patients with colorectal,5 gastric,6 and non–small-cell lung7 carcinomas, cytokeratin-specific monoclonal antibodies have been used to identify ectopic epithelial cells in the bone marrow. The presence of these cells was shown to influence the prognosis in patients with these tumors. In breast cancer, antibodies against antigens of the polymorphic epithelial mucin family,8 such as epithelial membrane antigen, human-milk-fat globule, or tumor-associated glycoprotein 12, have been used for this purpose.9-11 However, reports of a positive correlation between clinical outcome and the presence of cells in bone marrow that reacted with these antibodies were questioned when it was found that both epithelial membrane antigen and tumor-associated glycoprotein 12 were expressed not only by epithelial cells but also by plasmacytes and erythroid precursors.12-18 One early study of 49 patients reported that cytokeratin-specific antibodies could detect breast-cancer micrometastases in bone marrow smears.19 These results prompted us to conduct a prospective study using a standardized immunocytochemical technique, a defined number of bone marrow cells, and a monoclonal antibody against an antigen shared by various cytokeratin peptides. Additional justification for the use of cytokeratin-specific antibodies to detect breast-cancer cells in bone marrow is provided by the finding of multiple chromosomal aberrations in cytokeratin-positive micrometastases of cells in interphase by fluorescence in situ hybridization20 or by comparative hybridization of genomic DNA.21 These results demonstrate that cytokeratin-positive cells in the marrow are indeed tumor cells.
Methods|Patients From January 1994 to December 1997, 743 consecutive patients admitted to the I. Frauenklinik at Ludwig Maximilians University in Munich and the Zentralklinikum in Augsburg, Germany, were studied. Patients underwent bone marrow aspiration from both upper iliac crests after providing written informed consent and before the removal of the primary carcinoma. The procedure was approved by the institutional review boards. The stage and grade of the tumor were classified according to the tumor–node–metastasis classification of the Union Internationale contre le Cancer22 by investigators unaware of the immunocytochemical findings in bone marrow. In addition, immunocytochemical analysis of the bone marrow specimens was performed without knowledge of the histopathological results. In this manner, we examined bone marrow obtained from 552 patients with stage I, II, or III breast cancer; 153 patients with benign lesions of the breast, such as fibroadenomas, mastitis, abscesses, and cysts; 11 with simple cysts; 10 with cystadenoma of the ovaries; and 17 with cervical intraepithelial neoplasms of grade I or II. In the 552 patients with breast cancer, the primary surgical treatment consisted of breast conservation in 298 and modified radical mastectomy in 254. The tumor was completely resected in all patients, and the routine procedures included dissection of axillary lymph nodes of levels I and II. For the diagnosis of lymph-node metastasis, single embedded lymph nodes were screened at up to three levels. All 298 patients treated with breast-conserving surgery received radiation therapy. Irradiation of the chest wall followed mastectomy in 94 patients. The median absorbed dose in the target area was either 50.0 Gy, given in 25 fractions, or 50.4 Gy, given in 28 fractions (in patients who received concomitant chemotherapy). Of 170 postmenopausal women with node-positive breast cancer, 72 women who had estrogen-receptor–positive tumors received 20 to 30 mg of tamoxifen daily, and it was recommended that therapy last two to five years. Both premenopausal and postmenopausal patients with estrogen-receptor–negative tumors were treated with chemotherapy. A total of 59 patients with one to three involved axillary lymph nodes received six cycles of chemotherapy consisting of cyclophosphamide (600 mg per square meter of body-surface area), methotrexate (40 mg per square meter), and fluorouracil (600 mg per square meter) every 21 days. In 101 patients who had at least four involved regional lymph nodes, four courses of epirubicin (90 mg per square meter) and cyclophosphamide (600 mg per square meter) were administered, followed by three courses of cyclophosphamide, methotrexate, and fluorouracil. All 19 patients with evidence of inflammatory breast cancer (all of whom had node-positive cancer) received three cycles of chemotherapy before and after surgery, consisting of either epirubicin and cyclophosphamide or epirubicin (90 mg per square meter) and paclitaxel (175 mg per square meter). Of 301 patients with node-negative cancer, 33 received tamoxifen alone, 23 received more than one agent, and 245 did not receive any systemic adjuvant therapy. At the time of primary surgery, the base-line diagnostic evaluation for distant metastases included plain chest radiography, mammography of the contralateral breast, ultrasonography of the liver, and bone scanning of the entire body. These examinations showed no evidence of distant metastases in any of the patients. After surgery, the patients underwent clinical examinations every three months and were further tested only if they had symptoms. The findings reported here were documented in all patients as of August 16, 1999. Preparation of Bone Marrow The procedure for bone marrow preparation has been described previously.16 In short, while the patient was under general anesthesia bone marrow samples were obtained from each upper iliac crest by needle aspiration during primary surgery and stored in heparin-treated tubes. Mononuclear cells were separated by Ficoll–Hypaque density-gradient centrifugation (density, 1.077 g per mole) at 900×g for 30 minutes, the cells were washed and centrifuged at 150×g for 5 minutes, and 1 million cells were placed on each glass slide.16 Aspirates yielded between 4.0×106 and 6.6×107 bone marrow cells (mean, 1.5×107). Immunocytochemical Analysis For each patient, we screened 2×106 cells by bright-field microscopy; an identical number of cells served as a control for staining with an irrelevant immunoglobulin. We did not use morphologic features to identify cells; we used only immunocytochemical staining. Because there was no background staining, there were no indeterminate results. All slides were examined independently by two observers who agreed on the results for over 95 percent of specimens. In the case of discrepant results, the two investigators reevaluated the slide and eventually reached a consensus. We used monoclonal antibody A45-B/B3 (Micromet, Munich, Germany), which is directed against a common epitope on cytokeratin polypeptides, including the cytokeratin heterodimers 8–18 and 8–19,23 at a concentration of 1.0 to 2.0 μg per milliliter to detect tumor cells in cytospin preparations of bone marrow. The specificity of the antibody reaction in the bone marrow specimens was confirmed by the addition of an unrelated mouse myeloma immunoglobulin at an appropriate dilution. The breast-cancer cell line BT-20 served as a positive control for cytokeratin immunostaining.16 The reaction of the primary antibody was developed with the alkaline phosphatase anti–alkaline phosphatase technique combined with the new fuchsin stain24 to indicate antibody binding, as previously described.16 Statistical Analysis We verified all reported immunocytochemical and histopathological results and reports of events (death, relapse, or recurrent disease) during follow-up by reexamining the original data files. The primary end point was survival, measured from the date of surgery to the time of the last follow-up visit or cancer-related death. Secondary end points were locoregional relapse (including recurrences in the ipsilateral and contralateral breast) and distant metastasis and were measured in the same way as was the primary end point. We constructed Kaplan–Meier life-table curves for survival free of locoregional and distant recurrences and overall survival.25 We used the log-rank test to compare the patients with bone marrow micrometastases with those without micrometastases. Data on patients who were alive and had no evidence of disease at the end of our study were censored. We used Cox proportional-hazards analysis to estimate the prognostic effect of various variables. The variables were entered in a stepwise fashion into the model to compare the independent prognostic value of bone marrow micrometastasis with that of other prognostically relevant variables.26 We used the chi-square test to compare categorical variables. We used the Mann–Whitney U test to assess the differences in the means. A P value of less than 0.05 was considered to indicate a statistically significant difference. All tests were two-tailed. For statistical analyses, we used SPSS software for Macintosh (version 6.1.1).
Results|Detection of Micrometastases Figure 1. Figure 1. Immunostaining of Occult Metastatic Cells in Bone Marrow with Monoclonal Antibody A45-B/B3 (×1000). Panel A shows a single metastatic cell. Panel B shows a cluster of eight micrometastatic cells. There is no immunostaining of surrounding bone marrow cells. Bone marrow aspirates were obtained from 552 patients with newly diagnosed breast cancer, none of whom had a history of epithelial cancer. Of these patients, 199 (36 percent) had cytokeratin-positive tumor cells in the bone marrow at the time of the initial resection of the primary tumor. In most specimens (185 of 199 [93 percent]) the occult cells were present as dispersed single cells (Figure 1A); clusters of cells (Figure 1B) were found in only 7 percent of specimens (14 of 199). The overall frequency of occult metastatic cells in each specimen was low; there was a median of 3 cytokeratin-positive cells (range, 1 to 1223) per 2×106 bone marrow cells analyzed. The numbers of detectable tumor cells increased with the tumor stage; for example, patients with stage I cancer had a mean of 5 tumor cells per 2×106 bone marrow cells, and patients with stage II disease and those with stage III disease had means of 9 and 86 tumor cells per 2×106 bone marrow cells, respectively. Bone marrow aspirates from 191 patients with nonmalignant disease were also analyzed in a blinded fashion, before the final histopathological result was disclosed. In only two patients (1 percent) in this group — one with a chronic benign inflammation of the breast and the other with a benign cystadenoma of the ovary — were specifically stained cytokeratin-positive cells detected. Characteristics of the Patients Table 1. Table 1. Clinical Characteristics of 552 Patients with Breast Cancer, According to the Presence or Absence of Occult Metastatic Cells in Bone Marrow. Table 1 shows the clinical characteristics of the study population. Most patients (58 percent) had primary tumors that were no more than 2 cm in diameter. Larger primary tumors were associated with a higher incidence of micrometastases than were tumors that were 2 cm or less in diameter (P<0.001). Of 43 patients with stage pT4 tumors (invasion of contiguous structures), 19 had inflammatory breast cancer; 15 of these 19 patients (79 percent) had occult metastatic cells in the marrow (P<0.001). Twenty-three percent of patients with stage pT1a tumors had occult disease, as did 35 percent of patients with stage pT1b tumors and 30 percent of patients with pT1c tumors (P=0.56 for the difference among the groups). Although histologic involvement of axillary lymph nodes is the standard risk factor used for prognos-tic evaluation, we found that the incidence of bone marrow micrometastases was similar in patients with lymph-node metastasis and those without it (P= 0.13). Of 301 patients without clinical or histopathological signs of lymph-node metastases, 100 (33 percent) had cytokeratin-positive cells in the marrow (Table 1). Table 1 shows that the number of lymph nodes with metastases was significantly associated with the presence of bone marrow micrometastases (P<0.001). Bone Marrow Micrometastases and Recurrence of Disease Figure 2. Figure 2. Kaplan–Meier Life-Table Analysis of the Survival of Patients with Breast Cancer, According to the Presence or Absence of Micrometastases. Panel A shows survival free of distant metastasis. Patients with occult metastatic cells in bone marrow had a higher risk of relapse than patients without occult metastatic cells (relative risk, 5.99; 95 percent confidence interval, 3.89 to 9.23; P<0.001 by the log-rank test). Panel B shows overall survival. Patients with occult metastatic cells in bone marrow had a higher risk of cancer-related death than patients without occult metastatic cells (relative risk, 4.28; 95 percent confidence interval, 2.59 to 7.09; P<0.001). Panel C shows the overall survival of patients with node-negative cancer and patients with node-positive cancer. Patients with node-negative cancer who had occult metastatic cells had a higher risk of cancer-related death than patients with node-negative cancer who did not have occult metastatic cells (relative risk, 13.26; 95 percent confidence interval, 3.01 to 58.46; P<0.001). Patients with node-positive cancer who had occult metastatic cells had a higher risk of cancer-related death than patients with node-positive cancer who did not have occult metastatic cells (relative risk, 3.32; 95 percent confidence interval, 1.91 to 5.76; P<0.001). There was no significant difference in survival between patients with node-negative cancer who had occult metastatic cells and patients with node-positive cancer who did not have occult metastatic cells (P=0.84). After a median follow-up of 38 months (range, 10 to 70), relapse of the tumor occurred in 135 patients: 28 of these women (21 percent) had locoregional relapse, and 107 (79 percent) had distant metastases. Whereas locoregional relapses were not associated with the presence of micrometastases in bone marrow, as compared with their absence (relative risk of relapse, 0.89; 95 percent confidence interval, 0.39 to 2.01; P=0.77), distant metastasis was significantly associated with the presence of occult micrometastases in the marrow (Figure 2A). Of 33 patients with relapses at visceral sites, 13 had bone marrow involvement. In contrast, micrometastases were found in 18 of 19 patients with relapses in the skeleton and in 48 of 55 patients with relapses at visceral sites in combination with skeletal metastases (P<0.001). Bone Marrow Micrometastases and Survival Of 199 patients with occult metastatic cells, 49 died of cancer-related causes (25 percent), whereas of 353 patients without occult tumor cells in the marrow only 22 died of breast cancer (6 percent). As shown in Figure 2B, patients with bone marrow micrometastasis had a higher risk of death from cancer than patients without bone marrow micrometastases (relative risk, 4.28; 95 percent confidence interval, 2.59 to 7.09; P<0.001). Among women with cytokeratin-positive cells in the marrow, as compared with those without such cells, the relative risk of death was 3.32 among patients with node-positive cancer (95 percent confidence interval, 1.91 to 5.76; P<0.001) and 13.26 among patients with node-negative cancer (95 percent confidence interval, 3.01 to 58.46; P<0.001) (Figure 2C). Among 100 patients with node-negative cancer and micrometastases, 14 (14 percent) died of cancer-related causes, whereas only 2 patients (1 percent) died of cancer-related causes in the group of 201 patients without micrometastases. There was no significant difference in survival, however, between patients with node-negative cancer who had micrometastases and patients with node-positive cancer who did not have micrometastases (Figure 2C). Bone Marrow Micrometastases and Adjuvant Therapy Since the occurrence of locoregional relapse and distant metastasis may be influenced by adjuvant treatment, we performed a separate analysis of the 245 patients with node-negative cancer who did not receive systemic adjuvant therapy. Of these patients, 81 (33 percent) had occult metastatic cells. Clinically overt distant metastases occurred in 18 of 81 patients (22 percent) with micrometastases (relative risk of distant metastasis, 7.4; 95 percent confidence interval, 2.7 to 19.9; P<0.001), as compared with 4 of 164 patients (2 percent) without micrometastases. Moreover, among the patients who did not receive adjuvant therapy, the relative risk of cancer-related death was higher among the 81 patients with micrometastases than among the 164 without micrometastases (10 deaths [12 percent] vs. 1 death [1 percent]; relative risk, 18.9; 95 percent confidence interval, 2.4 to 70.5; P<0.001). Among the 51 patients with node-negative cancer who had well-differentiated (grade 1) or moderately well differentiated (grade 2) small tumors (≤1 cm in diameter) that were positive for estrogen receptors, 11 (22 percent) had micrometastases in the marrow. Of these 11 patients, 2 had both locoregional relapse and distant metastases at the time of the last follow-up visit, whereas no such events had occurred among the 40 patients without occult disease. This difference between the 11 patients with micrometastases and the 40 without micrometastases was not statistically significant (P=0.06). Micrometastases and Other Prognostic Variables Table 2. Table 2. Results of Univariate and Multivariate Analyses. We performed a Cox multiple-regression analysis to determine whether the presence of bone marrow micrometastases was a significant predictor of freedom from distant metastases and of overall survival that was independent of age, menopausal status, tumor size, tumor grade, estrogen-receptor status, and lymph-node status. To control for interactions related to systemic treatment, we stratified data according to the use of adjuvant therapy. No independent factor was identified that predicted locoregional recurrence. In contrast, bone marrow micrometastasis, estrogen receptors, and lymph-node metastasis were each independent predictors of both recurrence with distant metastases and cancer-related death (Table 2). On multivariate analysis, the effects of all risk factors decreased markedly, except for the effect of the presence of occult metastatic disease (Table 2).
Discussion|In this study of the hematogenous dissemination of breast-cancer cells, we used a monoclonal antibody (A45-B/B3) that binds to an antigen on cytokeratins 8, 18, and 19. These cytokeratins are expressed by normal and transformed epithelial cells23,27 but not bone marrow cells.16,17 As compared with antibod-ies against single members of the cytokeratin family, A45-B/B3 is more sensitive,16,17 perhaps in part because of the down-regulation of individual cytokeratin polypeptides in some transformed cells.28 The finding of multiple tumor-specific chromosomal aberrations in cytokeratin-positive cells in bone marrow is strong evidence that our method detects micrometastases.21,29 The controversy over the prognostic relevance of the presence or absence of cytokeratin-positive cells in the marrow9-11,19,30-33 may be explained by the use of different antibodies, staining techniques, and criteria for defining positively stained cells. The absence of detectable cytokeratin-positive cells in 189 of 191 specimens from control patients with nonmalignant disease in our study (with all analyses performed in a blinded fashion) demonstrates the specificity of A45-B/B3. The two positive results may have been caused by staining of plasmacytoid cells,34 or they may reflect the presence of an occult malignant tumor.14 The specificity of cytokeratin as a marker of epithelial cancer cells seems clear,35 but there remains the problem of the sampling error inherent in examinations of small volumes of aspirated bone marrow. The exclusion of samples with less than the median number of tumor cells (e.g., 3 tumor cells per 2×106 marrow cells) from our analysis did not change the statistical significance of our findings. Moreover, in vitro experiments showed that our assay reproducibly detected a single tumor cell among 1 million bone marrow cells (unpublished data). In this study, we examined a median of 2 million marrow cells from each patient. The shortcomings of current tumor-staging practices are revealed by the facts that distant metasta-ses eventually occur in up to 30 percent of patients with node-negative cancer36 and that approximately 40 percent of patients with node-positive cancer survive for 10 years or more.37,38 Ménard et al. reported that the presence of lymph-node metastases was not a reliable prognostic indicator in biologically defined subgroups of patients,39 suggesting that lymph-node metastases are not necessarily associated with hematogenous spread of cancer. After four years of follow-up, we found that the presence of occult micrometastases in the marrow was associated with a statistically significant reduction in overall survival. Among patients without such micrometastases, overall survival at four years was 93 percent, whereas among patients with one or more cytokeratin-positive micrometastatic cells, it was 68 percent. This association with overall survival was observed in patients with lymph-node metastases and in those without them, as well as in patients who did not receive adjuvant chemotherapy. The effect of the presence of occult micrometastases was especially clear among patients with node-negative cancer, whose overall survival was similar to that of patients with node-positive cancer who did not have micrometastases. Moreover, the presence of cytokeratin-positive cells in bone marrow was associated with a significantly higher risk of distant metastases but not of locoregional recurrences. In particular, skeletal relapse was strongly related to the presence of micrometastases, suggesting that precursor cells of overt metastases may indeed be present among the dispersed cytokeratin-positive cells we detected in the marrow at the time of diagnosis. Whether patients with bone marrow micrometastases respond differently to adjuvant chemotherapy than patients without micrometastases remains to be studied. However, we have previously demonstrated that the proliferation rate of micrometastases (which might influence their sensitivity to chemotherapy) appears to be rather low.40 In addition, micrometastases in bone marrow are frequently found after chemotherapy, and their presence increases the risk of relapse.41 Because 245 of the 301 patients with node-negative cancer in our study did not receive systemic adjuvant therapy, the influence of occult metastatic cells on prognosis could be assessed independently of such therapy. We believe that the risk of relapse among patients with node-negative cancer who have bone marrow micrometastases may be sufficiently high to warrant the administration of adjuvant chemotherapy. Our findings support the view of Fisher and colleagues,42 who maintained that different pathways of tumor-cell dissemination cause distinct patterns of metastasis. In line with this reasoning are the results of immunohistochemical studies of lymph nodes of patients presumed to have node-negative breast cancer43,44; these studies found no concordance between the presence of lymph-node metastasis and the presence of bone marrow micrometastases. Analysis of the different metastatic routes that independently predict clinical relapse may provide complementary prognostic information. Two recent studies have shown that the long latency period between diagnosis and relapse in patients with breast cancer, even in those with node-positive cancer, may signal the need to monitor these patients for 10 to 15 years to assess the influence of occult metastatic cells on survival.37,38 For this reason, we caution against the overinterpretation of our data, especially in the case of patients with node-negative cancer who have occult metastatic cells, since we have only four years of follow-up data available. Nevertheless, the finding of such cells far from the primary tumor should alert the physician to the possibility of a subsequent relapse. Whether cytokeratin-positive cells in the marrow are really precursors of metastasis may be answered in the future by genomic studies of single cells or by analyses involving gene profiling.21 With respect to therapeutic strategies whose aim is to prevent metastatic disease, the detection of bone marrow micrometastases may become a useful means of stratifying risk in the heterogeneous group of patients with node-negative breast cancer.
Abstract|Children and adults exposed to chronic interpersonal trauma consistently demonstrate psychological disturbances that are not captured in the posttraumatic stress disorder (PTSD) diagnosis. The DSM-IV (American Psychiatric Association, 1994) Field Trial studied 400 treatment-seeking traumatized individuals and 128 community residents and found that victims of prolonged interpersonal trauma, particularly trauma early in the life cycle, had a high incidence of problems with (a) regulation of affect and impulses, (b) memory and attention, (c) self-perception, (d) interpersonal relations, (e) somatization, and (f) systems of meaning. This raises important issues about the categorical versus the dimensional nature of posttraumatic stress, as well as the issue of comorbidity in PTSD. These data invite further exploration of what constitutes effective treatment of the full spectrum of posttraumatic psychopathology.
Abstract|Purpose of review: The aim of this article is to review the current literature on co-occuring posttraumatic stress disorder and substance-use disorder, with an emphasis on clinical aspects and emerging treatments. Recent findings: In clinical populations (focusing on either disorder), about 25-50% have a lifetime dual diagnosis of posttraumatic stress disorder and substance-use disorder. Patients with both disorders have a more severe clinical profile than those with either disorder alone, lower functioning, poorer well being, and worse outcomes across a variety of measures. In recent years, several promising treatment programs have been developed specifically for co-occuring posttraumatic stress disorder and substance-use disorder, with one model having been established as effective thus far. Summary: Comorbid posttraumatic stress disorder/substance-use disorder is a frequent diagnosis in clinical populations that severely affects course and outcome. Treatment approaches appropriate for this vulnerable population need to be evaluated further and implemented in routine practice.
SARS-CoV-2 and ACE2|Angiotensin-converting enzyme 2 (ACE2) is a membrane-bound aminopeptidase that has a vital role in the cardiovascular and immune systems4. ACE2 is involved in heart function and the development of hypertension and diabetes mellitus. In addition, ACE2 has been identified as a functional receptor for coronaviruses4, including SARS-CoV and SARS-CoV-2. SARS-CoV-2 infection is triggered by binding of the spike protein of the virus to ACE2, which is highly expressed in the heart and lungs4. SARS-CoV-2 mainly invades alveolar epithelial cells, resulting in respiratory symptoms. These symptoms are more severe in patients with CVD, which might be associated with increased secretion of ACE2 in these patients compared with healthy individuals. ACE2 levels can be increased by the use of renin–angiotensin–aldosterone system inhibitors. Given that ACE2 is a functional receptor for SARS-CoV-2, the safety and potential effects of antihypertension therapy with ACE inhibitors or angiotensin-receptor blockers in patients with COVID-19 should be carefully considered. Whether patients with COVID-19 and hypertension who are taking an ACE inhibitor or angiotensin-receptor blocker should switch to another antihypertensive drug remains controversial, and further evidence is required.
Acute cardiac injury|Reports suggest that the Middle East respiratory syndrome-related coronavirus (MERS-CoV) can cause acute myocarditis and heart failure5. SARS-CoV-2 and MERS-CoV have similar pathogenicity, and the myocardial damage caused by infection with these viruses undoubtedly increases the difficulty and complexity of patient treatment. Myocardial injury associated with the SARS-CoV-2 occurred in 5 of the first 41 patients diagnosed with COVID-19 in Wuhan, which mainly manifested as an increase in high-sensitivity cardiac troponin I (hs-cTnI) levels (>28 pg/ml)3. In this study, four of five patients with myocardial injury were admitted to the intensive-care unit (ICU), which indicates the serious nature of the myocardial injury in patients with COVID-19. Blood-pressure levels were significantly higher in patients treated in the ICU than in those not treated in the ICU (mean systolic blood pressure 145 mmHg versus 122 mmHg; P < 0.001)3. In another report of 138 patients with COVID-19 in Wuhan, 36 patients with severe symptoms were treated in the ICU1. The levels of biomarkers of myocardial injury were significantly higher in patients treated in the ICU than in those not treated in the ICU (median creatine kinase (CK)-MB level 18 U/l versus 14 U/l, P < 0.001; hs-cTnI level 11.0 pg/ml versus 5.1 pg/ml, P = 0.004), suggesting that patients with severe symptoms often have complications involving acute myocardial injury1. In addition, among the confirmed cases of SARS-CoV-2 infection reported by the National Health Commission of China (NHC), some of the patients first went to see a doctor because of cardiovascular symptoms. The patients presented with heart palpitations and chest tightness rather than with respiratory symptoms, such as fever and cough, but were later diagnosed with COVID-19. Among the people who died from COVID-19 reported by the NHC, 11.8% of patients without underlying CVD had substantial heart damage, with elevated levels of cTnI or cardiac arrest during hospitalization. Therefore, in patients with COVID-19, the incidence of cardiovascular symptoms is high, owing to the systemic inflammatory response and immune system disorders during disease progression.The mechanism of acute myocardial injury caused by SARS-CoV-2 infection might be related to ACE2. ACE2 is widely expressed not only in the lungs but also in the cardiovascular system and, therefore, ACE2-related signalling pathways might also have a role in heart injury. Other proposed mechanisms of myocardial injury include a cytokine storm triggered by an imbalanced response by type 1 and type 2 T helper cells3,6, and respiratory dysfunction and hypoxaemia caused by COVID-19, resulting in damage to myocardial cells.
Chronic cardiovascular damage|A 12-year follow-up survey of 25 patients who recovered from SARS-CoV infection found that 68% had hyperlipidaemia, 44% had cardiovascular system abnormalities and 60% had glucose metabolism disorders7. Metabolomics analysis revealed that lipid metabolism was dysregulated in patients with a history of SARS-CoV infection. In these patients, the serum concentrations of free fatty acids, lysophosphatidylcholine, lysophosphatidylethanolamine and phosphatidylglycerol were significantly increased compared with individuals without a history of SARS-CoV infection7. However, the mechanisms by which SARS-CoV infection leads to disorders of lipid and glucose metabolism are still uncertain. Given that SARS-CoV-2 has a similar structure to SARS-CoV, this novel virus might also cause chronic damage to the cardiovascular system, and attention should be given to cardiovascular protection during treatment for COVID-19.
Patients with pre-existing CVD|A meta-analysis showed that MERS-CoV infection was more likely to occur in patients with underlying CVD8. In patients with MERS-CoV infection and severe symptoms, 50% had hypertension and diabetes and up to 30% had heart disease. Similarly, according to the Pneumonitis Diagnosis and Treatment Program for New Coronavirus Infection (Trial Version 4), elderly people with comorbidities are more likely to be infected with SARS-CoV-2, especially those with hypertension, coronary heart disease or diabetes. Furthermore, patients with CVD are more likely to develop severe symptoms if infected with SARS-CoV-2. Therefore, patients with CVD account for a large proportion of deaths from COVID-19. In one study, among the patients with severe symptoms of COVID-19, 58% had hypertension, 25% had heart disease and 44% had arrhythmia1. According to mortality data released by the NHC, 35% of patients with SARS-CoV-2 infection had a history of hypertension and 17% had a history of coronary heart disease. Furthermore, data show that patients aged >60 years who were infected with SARS-CoV-2 had more systemic symptoms and more severe pneumonia than patients aged ≤60 years9. Therefore, in patients with SARS-CoV-2 infection, underlying CVD can aggravate the pneumonia and increase the severity of symptoms.Patients with acute coronary syndrome (ACS) who are infected with SARS-CoV-2 often have a poor prognosis. In patients with ACS, cardiac functional reserve can be reduced owing to myocardial ischaemia or necrosis. When infected with SARS-CoV-2, cardiac insufficiency is more likely to occur, leading to a sudden deterioration in the condition of these patients. Some of the patients with COVID-19 in Wuhan had previous ACS, which was associated with severe illness and high mortality. For patients with cardiac insufficiency who have underlying heart disease, SARS-CoV-2 infection might act as a precipitating factor to worsen the condition and lead to death.Drug-related heart damage during COVID-19 treatment is a concern. In particular, the use of antiviral drugs should be monitored. In a study of 138 patients with COVID-19, 89.9% were given antiviral drugs1. However, many antiviral drugs can cause cardiac insufficiency, arrhythmia or other cardiovascular disorders. Therefore, during treatment of COVID-19, especially with the use of antivirals, the risk of cardiac toxicity must be closely monitored10.
Conclusions|SARS-CoV-2 is thought to infect host cells through ACE2 to cause COVID-19, while also causing damage to the myocardium, although the specific mechanisms are uncertain. Patients with underlying CVD and SARS-CoV-2 infection have an adverse prognosis. Therefore, particular attention should be given to cardiovascular protection during treatment for COVID-19.
Abstract|Somatic mutations in cancer genomes are caused by multiple mutational processes, each of which generates a characteristic mutational signature1. Here, as part of the Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium2 of the International Cancer Genome Consortium (ICGC) and The Cancer Genome Atlas (TCGA), we characterized mutational signatures using 84,729,690 somatic mutations from 4,645 whole-genome and 19,184 exome sequences that encompass most types of cancer. We identified 49 single-base-substitution, 11 doublet-base-substitution, 4 clustered-base-substitution and 17 small insertion-and-deletion signatures. The substantial size of our dataset, compared with previous analyses3,4,5,6,7,8,9,10,11,12,13,14,15, enabled the discovery of new signatures, the separation of overlapping signatures and the decomposition of signatures into components that may represent associated—but distinct—DNA damage, repair and/or replication mechanisms. By estimating the contribution of each signature to the mutational catalogues of individual cancer genomes, we revealed associations of signatures to exogenous or endogenous exposures, as well as to defective DNA-maintenance processes. However, many signatures are of unknown cause. This analysis provides a systematic perspective on the repertoire of mutational processes that contribute to the development of human cancer.
Discussion|There are important constraints, limitations and assumptions in the analytic frameworks used here to characterize mutational signatures. Signatures extracted from sample sets in which multiple processes are operative remain mathematical approximations, with profiles that are potentially influenced by the mathematical approach used and other factors. For conceptual and practical simplicity, we assume that a single signature is associated with each mutational process and provide an average reference signature to represent it. However, we do not discount the possibility that further nuances and variations of signature profiles exist. We have estimated the contributions from each signature to the mutation burden in each sample. However, with increasing numbers of signatures and differences of multiple orders of magnitude in mutation burdens between some signatures, prior knowledge has helped to avoid biologically implausible results. Thus, the further development of methods for deciphering and attributing mutational signatures is warranted, ideally supported by signatures derived from experimental systems in which the causes are known. Nevertheless, signatures with many similarities and some differences can be found by different mathematical approaches, and these can be confirmed in several ways, including experimentally elucidated signatures5,31,39,42,43,54,55,56,57,58,59,60,61,62 and tumours dominated by a single signature (syn12016215).This analysis includes most publicly available exome and whole-genome cancer sequences. Some rare or geographically restricted signatures may not have been captured, signatures conferring limited mutation burdens may have been missed and signatures of therapeutic mutagenic exposures have not been exhaustively explored. Nevertheless, it is likely that a substantial proportion of the naturally occurring mutational signatures found in human cancer have now been described. This comprehensive repertoire provides a foundation for research into the aetiologies of geographical and temporal differences in cancer incidence, the mutational processes that operate in healthy tissues and non-neoplastic disease states, clinical and public health applications of signatures and mechanistic understanding of the mutational processes that underlie carcinogenesis.
Methods|No statistical methods were used to predetermine sample size. The experiments were not randomized and investigators were not blinded to allocation during experiments and outcome assessment.These online methods contain an abridged description of the methodology used in the current manuscript; extensive details about the methodology we used are provided in Supplementary Note 2. Importantly, two independently developed computational frameworks (SigProfiler and SignatureAnalyzer) based on NMF were applied separately to the examined sets of mutational catalogues. SigProfiler and SignatureAnalyzer take different approaches for deciphering mutational signatures and for assigning each signature to each sample. By using two methods, we aimed to provide a perspective on the effect that different methodologies can have on the numbers of signatures generated, signature profiles and attributions. In addition to applying SigProfiler and SignatureAnalyzer to cancer data, the tools were also applied to realistic synthetic data with known solutions.Analysis of mutational signatures with SigProfilerSigProfiler incorporates two distinct steps for identification of mutational signatures, based on the previously described methodology6,11,17 (Extended Data Fig. 8). The first step (SigProfilerExtraction) encompasses a hierarchical de novo extraction of mutational signatures based on somatic mutations and their immediate sequence context, and the second step (SigProfilerAttribution) focuses on accurately estimating the number of somatic mutations associated with each extracted mutational signature in each sample. SigProfilerExtraction is an extension of a previous framework for the analysis of mutational signatures11,17. In brief, for a given set of mutational catalogues, the algorithm deciphers a minimal set of mutational signatures that optimally explains the proportion of each mutation type and estimates the contribution of each signature to each sample. More specifically, for each NMF iteration, SigProfilerExtraction minimizes a generalized Kullback–Leibler divergence constrained for nonnegativity (Supplementary Note 2). The algorithm uses multiple NMF iterations (in most cases 1,024) to identify the matrix of mutational signatures and the matrix of the activities of these signatures, as previously described17. The unknown number of signatures is determined by human assessment of the stability and accuracy of solutions for a range of values, as previously described17. The framework is applied hierarchically to increase its ability to find mutational signatures that generate few mutations or are present in few samples.After signatures are discovered by SigProfilerExtraction, SigProfilerAttribution estimates their contributions to individual samples. For each examined sample, the estimation algorithm involves finding the minimum of the Frobenius norm of a constrained function using a nonlinear convex optimization programming solver using the interior-point algorithm63. See Supplementary Note 2 and Extended Data Fig. 8b for further details.Analysis of mutational signatures with SignatureAnalyzerSignatureAnalyzer uses a Bayesian variant of NMF that infers the number of signatures through the automatic relevance determination technique and delivers highly interpretable and sparse representations for both signature profiles and attributions that strike a balance between data fitting and model complexity. Further details of the actual implementation of the computational approach have previously been published9,27,64. SignatureAnalyzer was applied by using a two-step signature extraction strategy using 1,536 pentanucleotide contexts for SBSs, 83 indel features and 78 DBS features. In addition to the separate extraction of SBS, indel and DBS signatures, we performed a ‘COMPOSITE’ signature extraction based on all 1,697 features (1,536 SBS + 78 DBS + 83 indel). For SBSs, the 1,536 SBS COMPOSITE signatures are preferred; for DBSs and indels, the separately extracted signatures are preferred.In step 1 of the two-step extraction process, global signature extraction was performed for the samples with a low mutation burden (n = 2,624). These excluded hypermutated tumours: those with putative polymerase epsilon (POLE) defects or mismatch repair defects (microsatellite instable tumours), skin tumours (which had intense UV-light mutagenesis) and one tumour with temozolomide (TMZ) exposure. Because the underlying algorithm of SignatureAnalyzer performs a stochastic search, different runs can produce different results. In step 1, we ran SignatureAnalyzer 10 times and selected the solution with the highest posterior probability. In step 2, additional signatures unique to hypermutated samples were extracted (again selecting the highest posterior probability over ten runs) while allowing all signatures found in the samples with low mutation burden, to explain some of the spectra of hypermutated samples. This approach was designed to minimize a well-known ‘signature bleeding’ effect or a bias of hyper- or ultramutated samples on the signature extraction. In addition, this approach provided information about which signatures are unique to the hypermutated samples, which was later used when attributing signatures to samples.A similar strategy was used for signature attribution: we performed a separate attribution process for low- and hypermutated samples in all COMPOSITE, SBS, DBS and indel signatures. For downstream analyses, we preferred to use the COMPOSITE attributions for SBSs and the separately calculated attributions for DBSs and indels. Signature attribution in samples with a low mutation burden was performed separately in each tumour type (for example, Biliary–AdenoCA, Bladder–TCC, Bone–Osteosarc, and so on). Attribution was also performed separately in the combined microsatellite instable tumours (n = 39), POLE (n = 9), skin melanoma (n = 107) and TMZ-exposed samples (syn11738314). In both groups, signature availability (which signatures were active, or not) was primarily inferred through the automatic relevance determination process applied to the activity matrix H only, while fixing the signature matrix W. The attribution in samples with a low mutation burden was performed using only signatures found in the step 1 of the signature extraction. Two additional rules were applied in SBS signature attribution to enforce biological plausibility and minimize a signature bleeding: (i) allow SBS4 (smoking signature) only in lung, head and neck cases; and (ii) allow SBS11 (TMZ signature) in a single GBM sample. This was enforced by introducing a binary, signature-by-sample signature indicator matrix Z (1, allowed; 0, not allowed), which was multiplied by the H matrix in every multiplication update of H. No additional rules were applied to indel or DBS signature attributions, except that signatures found in hypermutated samples were not allowed in samples with a low mutation burden.Application of SigProfiler and SignatureAnalyzer to synthetic dataOur goal was to evaluate SignatureAnalyzer and SigProfiler on realistic synthetic data to identify any potential limitations of these two methods. SignatureAnalyzer and SigProfiler were tested on 11 sets of synthetic data, encompassing a total of 64,400 synthetic samples, in which known signature profiles were used to generate catalogues of synthetic mutational spectra. We operationally defined ‘realistic’ data as those based on the characteristics of either SignatureAnalyzer’s or SigProfiler’s analysis of the PCAWG genome data. SignatureAnalyzer’s reference signature profiles were based on COMPOSITE signatures, consisting of 1,536 types of strand-agnostic SBSs in pentanucleotide context, 78 types of DBSs and 83 types of small indels, for a total of 1,697 mutation types. SigProfiler’s reference analysis was based on strand-agnostic SBSs in the context of one 5′ and one 3′ base. For each test, we generated two sets of realistic data: SigProfiler-realistic (based on SigProfiler’s reference signatures and attributions) and SignatureAnalyzer-realistic (based on SignatureAnalyzer’s reference signatures and attributions), as well as two other types of data that involved using SignatureAnalyzer profiles with SigProfiler attributions and vice versa. A detailed description of each of the 11 sets of synthetic data and the results from applying SigProfiler and SignatureAnalyzer are provided in Supplementary Note 2.Analysis of clustered mutational signaturesSomatic SBSs were considered clustered if they had intermutational distances < 1,000 bp. More specifically, for each sample, an SBS mutational catalogue was generated for substitutions that were <1,000 bp from another substitution. Subsequently, the set of SBS mutational catalogues containing clustered mutations underwent de novo extraction of mutational signatures. Any novel mutational signature (one that was not previously observed in the complete SBS catalogues) was reported as a clustered mutational signature.Better separation compared to COSMIC v.2 signaturesAs described in the manuscript, all mutational signatures previously reported in COSMIC v.2 were confirmed in the new set of analyses with median cosine similarity of 0.95. However, the separation between the COSMIC v.2 mutational signatures (https://cancer.sanger.ac.uk/cosmic/signatures_v2) is much worse than the separation between the mutational signatures reported here. For example, in COSMIC v.2, signatures 5 and 16 had a cosine similarity of 0.90, making them hard to distinguish from one another. By contrast, in the current analysis, SBS5 and SBS16 have a cosine similarity of 0.65. This allows us to unambiguously assign SBS5 and SBS16 to different samples. In the current analysis, the larger number of samples has allowed the reduction of bleeding between signatures and has given more unique and easily distinguishable signatures. One can evaluate the overall separation of a set of mutational signatures by examining the distribution of cosine similarities between the signatures in the set. The signatures in COSMIC v.2 had a median cosine similarity of 0.238. By contrast, the current signatures have a much lower median cosine similarity of 0.098. This twofold reduction in similarity is highly statistically significant (P value 9.1 × 10−25) and indicates a better separation between the signatures in the current analysis.Correlations of mutational signature activity with ageBefore evaluating the association between age and the activity of a mutational signature, all outliers for both age and numbers of mutations attributed to a signature in a cancer type were removed from the data. An outlier was defined as any value outside three standard deviations from the mean value. A robust linear regression model that estimated the slope of the line and whether this slope was significantly different from zero (F test; P value < 0.05) was performed using the MATLAB function robustfit (https://www.mathworks.com/help/stats/robustfit.html) with default parameters. The P values from the F tests were corrected using the Benjamini–Hochberg procedure for false discovery rates. Results are available at syn12030687 and syn20317940.Reporting summaryFurther information on research design is available in the Nature Research Reporting Summary linked to this paper.
Abstract|Pain is an integrative phenomenon that results from dynamic interactions between sensory and contextual (i.e., cognitive, emotional, and motivational) processes. In the brain the experience of pain is associated with neuronal oscillations and synchrony at different frequencies. However, an overarching framework for the significance of oscillations for pain remains lacking. Recent concepts relate oscillations at different frequencies to the routing of information flow in the brain and the signaling of predictions and prediction errors. The application of these concepts to pain promises insights into how flexible routing of information flow coordinates diverse processes that merge into the experience of pain. Such insights might have implications for the understanding and treatment of chronic pain.
Abstract|Investigation of tree growth in Isle Royale National Park in Michigan revealed the influence of herbivores and carnivores on plants in an intimately linked food chain. Plant growth rates were regulated by cycles in animal density and responded to annual changes in primary productivity only when released from herbivory by wolf predation. Isle Royale's dendrochronology complements a rich literature on food chain control in aquatic systems, which often supports a trophic cascade model. This study provides evidence of top-down control in a forested ecosystem.
Abstract|Purpose of ReviewSevere acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the virus responsible for the aggressive coronavirus disease (COVID-19) pandemic. Recently, investigators have stipulated that COVID-19 patients receiving angiotensin-converting-enzyme inhibitors (ACEI) may be subject to poorer outcomes. This editorial presents the available evidence to guide treatment practices during this pandemic.Recent FindingsRecent studies from Wuhan cohorts provide valuable information about COVID-19. A cohort with 52 critically ill patients revealed cardiac injury in 12% of patients. Worse outcomes appear to be more prevalent in patients with hypertension and diabetes mellitus (DM), possibly due to overexpression of angiotensin-converting enzyme 2 (ACE2) receptor in airway alveolar epithelial cells. Investigators suspect that SARS-CoV-2 uses the ACE2 receptor to enter the lungs in a mechanism similar to SARS-CoV. Several hypotheses have been proposed to date regarding the net effect of ACEI/ARB on COVID-19 infections. Positive effects include ACE2 receptor blockade, disabling viral entry into the heart and lungs, and an overall decrease in inflammation secondary to ACEI/ARB. Negative effects include a possible retrograde feedback mechanism, by which ACE2 receptors are upregulated.SummaryEven though physiological models of SARS-CoV infection show a theoretical benefit of ACEI/ARB, these findings cannot be extrapolated to SARS-CoV-2 causing COVID-19. Major cardiology scientific associations, including ACC, HFSA, AHA, and ESC Hypertension Council, have rejected these correlation hypotheses. After an extensive literature review, we conclude that there is no significant evidence to support an association for now, but given the rapid evolvement of this pandemic, findings may change.Keywords: COVID-19, SARS-COV 2, ACEI, ARB, ACE2 receptor
Introduction|Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the virus responsible for COVID-19, a global pandemic with catastrophic consequences for healthcare systems and populations around the world. SARS-CoV-2 was initially described in December 2019 in Wuhan, China [1]. The virus rapidly escalated and on March 11, 2020; the World Health Organization declared it a pandemic. SARS-CoV-2 shares similarities with SARS-CoV, the virus responsible for the 2002–2003 SARS epidemic, and Middle Eastern respiratory syndrome coronavirus (MERS), the virus responsible for MERS [2]. Following the SARS epidemic, researchers extensively investigated the pathophysiologic mechanisms of SARS-CoV infection, including the interaction of the virus with the heart and lungs. Based on these studies, researchers believe that the angiotensin-converting enzyme 2 (ACE2) receptor, located on alveolar epithelial cells, serves as a high affinity receptor and co-transporter for SARS-CoV-2 to enter the lungs [3]. Medications, such as angiotensin-converting enzyme inhibitors (ACEI), block ACE2 receptors, which may predispose or protect against COVID-19 infection. This editorial summarizes the current scientific evidence surrounding this subject in order to guide clinical practice.
Background|The renin-angiotensin-aldosterone system (RAAS) maintains plasma sodium concentration via feedback from blood pressure, baroreceptors, and sodium and potassium levels. First, the kidneys secrete renin, which metabolizes angiotensinogen into angiotensin I. Next, the kidneys and lungs secrete ACE, which converts angiotensin I into angiotensin II. Finally, angiotensin II stimulates vasoconstriction, cardiovascular response, and aldosterone and ADH production; this ultimately increases blood pressure and body fluid volume through sodium, potassium, and free water resorption [3]. ACE2 receptor, a homolog of the angiotensin I-converting enzyme (ACE) receptor, is a type I transmembrane aminopeptidase with high expression in heart and lung tissue [4], but which is also expressed in the endothelium and kidney (see Fig. 1, illustrating the RAAS activation pathway). Discovered in 2000, ACE2 receptor appears to counter-regulate RAAS activation by degrading angiotensin II [5]. The RAAS system is widely implicated in DM, hypertension, and heart failure. ACEI and ARB drugs, based upon strong evidence of efficacy, are commonly used in the management of hypertension, heart failure, post myocardial infarction care, and to slow progression of renal disease associated with diabetes.Open in a separate windowFig. 1RAAS pathway showing ACEI/ARB mechanism of action and SARS and SARS-COV2 infectious mechanism via ACE2 receptors
COVID-19 and Comorbidity|With the exponential rise of COVID-19 cases worldwide, observational studies have identified risk factors for infection and poor outcomes. Three separate studies identified hypertension and DM as highly prevalent among COVID-19 patients:A.According to Yang et al., among 52 critically ill patients, DM was present in 17% of cases [6].B.According to Guan et al., among 1099 patients, DM was present in 16.2% of cases and hypertension was present in 23.7% of cases [7].C.According to Zhang et al., among 140 hospitalized patients, DM was present in 12% of cases and hypertension was present in 30% of cases [8].While both hypertension and DM are treated with ACEI and ARB, medication use was not assessed in any of the three aforementioned studies, leading to an inconclusive hypothesis. However, one study to date has analyzed the effect of ACEI and ARB use on the COVID-19 population. According to Peng et al., among 112 patients, cardiovascular comorbidities led to worse outcomes, with most deaths occurring secondary to fulminant inflammation, lactic acidosis, and thrombotic states [9]. ACEI and ARB use did not influence morbidity or mortality [9].In addition to these observations, there is a well-known association between the viruses, SARS-CoV and MERS-CoV, and deleterious cardiac events, including cardiac injury in SARS-CoV as well as myocarditis and heart failure in MERS-CoV [10]. Furthermore, SARS-CoV-2 was recently associated with cardiac injury, defined by a troponin > 28 pg/ml or electrocardiogram/echocardiogram abnormalities, in 12% of patients from a COVID-19 cohort. These data demonstrate a cardiac affinity with all three viruses [11].
Pros and Cons of ACE Inhibition|The etiology of cardiac damage in patients with COVID-19 is unclear, but ACE2 receptors may play a role, given the high affinity of SARS-CoV for ACE2 receptors [12]. A recent commentary published in the Lancet Respiratory Medicine hypothesizes that the use of ACE2 receptor increasing drugs is at higher risk for severe COVID-19 infection. ACEI initially inhibits ACE leading to decreased angiotensin I levels, causing a possible negative feedback loop that ultimately upregulates more ACE2 receptor to be able to interact with the decreased angiotensin I substrate available [13] (see Fig. ​Fig.1.).1.). This ACE2 receptor upregulation results in increased binding sites for SARS-CoV-2, leading to preferential COVID-19 infection. This is particularly observed in patients with diabetes and/or hypertension, since they are usually taking ACEI or ARB [12]. This comment was released before the findings published by Peng et al. [9]. An editorial response by Sommerstein et al. [14], based on the findings by Ferrario et al. showing a 5-fold increase in ACE2 levels with lisinopril and 3-fold increase in ACE2 levels with losartan [15], was published in the British Medical Journal.Conversely, some investigators argue that ACEI or ARB use may be beneficial in COVID-19 infection prevention. Li et al., for example, proposed that ACEI inhibition of ACE may stimulate a negative feedback (given the lack of angiotensin II, upregulating ACE2 receptors and decreasing overall inflammation [16]). Sun et al. argued that ACEI use im pairs the ACE/angiotensin II/angiotensin-1 receptor pathway, therefore, impairing the integrity of the ACE2/angiotensin 1–7/MAS (MAS-related G protein-coupled receptor). A disruption of the ACE2/angiotensin 1–7/MAS pathway could lead to decreased production of ACE2, decreasing chances of SARS-CoV-2 entering the cell [17]. Some RAAS inactivated animal models demonstrate symptom relief in acute severe pneumonia and respiratory failure, through vasoconstriction mechanisms [17]. Recent findings also demonstrate that patients on the ARB olmesartan had increased secretion of urinary ACE2, likely from an upregulation mechanism, although unclear [18]. Despite this hypothesis of ACE2 upregulation, a causal relationship decreasing mortality has not been demonstrated.Finally, given the contradictory hypotheses, rapidly evolving nature of the disease, and social media-related hysteria, several cardiology associations (HFSA/ACC/AHA and ESC Hypertension Council) released an official statement regarding the continuation of ACEI and ARB for COVID-19 patients [19]. The associations strongly recommend continuing treatment with ACEI/ARB in patients who were previously taking either class of medication.
Conclusions|There is a lack of scientific evidence and clinical data to support discontinuing ACE/ARB use in patients with COVID-19 and co-existing heart failure, hypertension, or ischemic heart disease. The well-studied reduction in mortality conferred by ACE/ARB use and the beneficial effects for patients with diabetes, chronic kidney disease, and proteinuria or albuminuria currently outweigh the theoretical risks. As the COVID-19 pandemic continues to rapidly evolve and affect more patients with cardiovascular comorbidities, further research is needed to clarify the accuracy of existing hypotheses.
Abstract|BackgroundGlobally, sexual violence is prevalent, particularly for adolescent women. This cluster-randomized controlled implementation trial examines empowerment self-defense (ESD) for sexual assault risk reduction among school-age women in Malawi.MethodsThe unit of randomization and analysis was the school (n = 141). Intervention participants received a 12-h intervention over 6 weeks, with refreshers. Primary outcomes were past-year prevalence and incident rate of sexual violence. Secondary outcomes included confidence, self-defense knowledge, and, for those victimized, violence disclosure. Interaction effects on outcomes were evaluated with Poisson models with school-correlated robust variance estimates for risk ratios and incident rate ratios (baseline n = 6644, follow-up n = 4278).ResultsPast-year sexual assault prevalence was reduced among intervention students (risk ratio [RR] 0.68, 95% CI 0.56, 0.82), but not control students (interaction effect p < 0.001). Significant increases in self-defense knowledge were observed solely among intervention students (RR 3.33, 95% CI 2.76, 4.02; interaction effect p < 0.001). Significant changes in sexual violence prevalence and knowledge were observed for both primary and secondary students. Favorable reductions were also observed in sexual violence incident rate among students overall (interaction effect p = 0.01).ConclusionsThis intervention reduced sexual violence victimization in both primary and secondary school settings. Results support the effectiveness of ESD to address sexual violence, and approach the elimination of violence against women and girls set forth with Sustainable Development Goal #5. Implementation within the education system can enable sustainability and reach.Trial registrationPan African Clinical Trials Registry PACTR201702002028911. Registered 09 February 2017. Retrospectively registered.
Methods|Setting, recruitment and data collectionThis study was implemented in the Malawi districts of Lilongwe, Dedza, and Salima, selected for heterogeneity and based on designation as priority, high-need settings for UNICEF’s Safe Schools Program. Lilongwe, the capital, is the most urbanized site. Dedza lies south of Lilongwe, in the mountains on the Mozambique border. Salima is located on the lake. Within districts, primary and secondary schools were selected for stratified randomization from a full listing of schools participating in UNICEF’s Safe Schools program by UNICEF’s field assessment team.Within each selected school, research staff drew a simple random sample of students for activity participation (intervention or control) with the goal of a 20:1 student to instructor ratio. Based on instructor capacity in Lilongwe and Dedza, approximately 60 female students were selected for activity participation per school; in Salima where more staff were available, approximately 100 female students were selected per school. In most settings, participating classes included primary school classes 5, 6, 7, 8 and secondary school forms 1, 2, 3, 4. In Salima, Class 8 was not included in the selection pool based on retention concerns as many Class 8 students graduate. Sessions took place after school and were split as necessary for larger groups. Students within participating schools were blinded to intervention or control status. Within schools, simple random sampling was achieved by gathering participating school classes outside for circulation of an opaque plastic tin containing a pre-determined mix of beads specific to the size of each school. Students each selected a single bead; bead color red indicated random selection into the activities (intervention or control) underway at their school. In several schools, administrator concerns emerged regarding the use of red beads for random selection, and black instructor uniforms, which led to color modification for these procedures.Study population and retentionBaseline data was collected from February to June 2015 (See Fig. 1). Sample size was determined on the basis of implementation capacity. Follow-up data collection occurred from November 2015–May 2016. Follow-up data collection was not attempted in ten schools (5 Lilongwe, 4 Dedza, 1 Salima) based on the aforementioned administrator concerns; additionally students in Class 8 and Form 4 were not followed after completing school examinations; yielding an effective baseline sample of 6644 students. Follow-up data was obtained for 3311 primary school students and 967 secondary school students (total n = 4278).Open in a separate windowFig. 1Participant flow chartIMPower intervention activitiesIMPower consists of weekly, 2-h sessions for 6 weeks for a total of 12 h of interactive, empowerment self-defense training. Because physical interventions can escalate situations of potential violence, IMPower emphasizes early recognition of boundary testing, negotiation, diffusion and distraction tactics, and verbal assertiveness over physical self-defense, with the guidance that physical tactics should only be used if they are the last and best option. IMpower teaches boundary recognition and boundary setting (e.g., name harmful behaviors, warn about consequences), negotiation and diffusion tactics, verbal assertiveness (e.g., yell if threatened), and physical defense skills, with the self-efficacy to implement these skills. The physical skills comprise closed target skills, weapons and targets. After the six weeks, two-hour refresher courses are performed every 3–6 months. IMPower was developed by No Means No Worldwide (NMNW), a US-based NGO. An extensive formative phase was conducted to adapt IMPower for the Malawi context. Following community sensitization and structured discussions with key stakeholders, program adaptations included an emphasis on verbal and negotiation skills, with clarification that a physical response, including physical self-defense, is a last resort in a situation of danger. IMPower instructors are carefully selected with preference for experience with youth and on issues of GBV, and capacity for teaching and community organizing. All instructors attend a 3-week, 126-h intensive training in Lilongwe. After certification, instructors are deployed, most often to their home districts, for a 6-month period of co-teaching with an experienced instructor, followed by independent teaching.Control condition activitiesStudents randomized to the control condition received the standard of care for youth in schools, specifically the Lifeskills program. This standard 2-h Malawian school program covers adolescent health topics including puberty, menstruation, hygiene, sex education, STIs/HIV, and pregnancy prevention. These students also received two-hour refresher classes at 3–6 months also focused on puberty and hygiene at 10.5 months, prior to completing follow-up surveys.Supporting sexual violence survivors: The SASA programIMPower participants who disclosed sexual violence during the program were referred to the Sexual Assault Survivors Anonymous (SASA) support program. This voluntary program offers weekly meetings with the goal of healing. From the IMPower program, 56 students in Lilongwe, 46 in Dedza, and 70 in Salima, were voluntarily linked to SASA support services.RandomizationThe unit of randomization was the school, to minimize risk of contamination given the intensity of the IMPower intervention. Within district, primary and secondary schools were grouped into blocks based on approximate size, and randomized within block to intervention or control conditions using SPSS (allocation ratio 1:1). Participants were assigned to intervention or control condition based on school. For logistical purposes regarding planning program duration, school administrators were aware of assignment to intervention condition prior to study implementation. While every effort was made to blind students and teachers to intervention arm, intervention status could have been inadvertently been revealed to teachers or students.Data collection proceduresInformed consent followed UNICEF guidelines. Teacher consent was undergone during the formative phase to allow classroom participation of students. Written consent was obtained from parents and students prior to participation in the program and evaluation research.Data collection procedures were designed to maximize confidentiality, in accordance with best practices for violence-related research [30]. No names or identifiers were collected. Participants self-administered baseline and follow-up surveys, aided by two instructors present in each class: one for reading the measures aloud and the other for monitoring the class to ensure confidentiality. Each question and answer choice was read aloud to the entire class in both English and Chichewa. Instructors from the local district were present to ensure that the district-specific dialect of Chichewa was used for survey administration, as terms vary by district. The students were given the opportunity to ask questions before selecting their own answer choice. To minimize deductive disclosure related to speed of survey completion, a response was required for each question, including “never had sex” or “never experienced sexual violence.” To maximize confidentiality, following survey completion, participants deposited their survey in a locked ballot box [31] to be taken directly to the research office, thus limiting the risk of instructors or data collectors inadvertently viewing responses. Students in both study arms had the option to self-refer to study staff and/or teachers for violence-related support and connection to local services.MeasuresAll measures were self-reported. They were adapted from prior evaluations of the IMPower program in Kenya [27, 28, 32] for comparability, and piloted in Malawian primary and secondary schools and with Chichewa-speaking instructor groups for feasibility and acceptability. Survey instruments were designed to maximize efficiency and feasibility for this demonstration study, as well as confidentiality. Primary outcomes were past year prevalence and past year incident rate of forced sex; for comparability with past evaluations. The forced sex outcome was assessed with a single item used for Kenya-based evaluations of IMPower [27, 28, 32], specifically, “Since you took the No Means No survey or in the past one year, have you ever been forced against your will to have sex (penetration of your vagina, anus or mouth with a man’s penis or another object)?” with a follow-up question “If so, how many times?”. School-level prevalence reflects the total number of participants who reported any forced sex over the referent period, divided by the total number of participants in the school. Incident rate was computed as mean number of forced sexual incidents per person-time within school, converted to adjust for time frame differences between the baseline (12 months) and follow-up period (10.5 months). Prevalence and disclosure outcomes are presented as proportions for interpretability; statistical inferences were robust in sensitivity analysis with time-adjusted estimates (data not shown).Intermediate outcomes were confidence, knowledge, and sexual violence disclosure. Confidence measures were developed specifically for IMPower, specifically, “If I am attacked by a bigger man I feel confident that I can defend myself” and “Is it okay to use force and even injure anyone who is known to me if he is forcing me to have sex and will not listen to me (e.g., brother, boyfriend, father, cousin)?” Knowledge pertained to the self-defense skills taught, specifically: “If I am grabbed by an attacker what should I use to free myself?” and “The main aim of self-defense is to?” Both confidence and knowledge were then transformed into scores—if students answered both questions correctly, they were considered to have “complete” knowledge or “high” confidence, respectively, with those answering only one or zero serving as the referent group. Disclosure of sexual violence was measured among girls who had experienced violence in the past year/since the baseline survey from the question “Did you tell anyone about it?” and analyzed dichotomously. Participants with past-year forced sex were asked to indicate perpetrator(s), which were aggregated into the following groups: boyfriend, friend, relative (including stepfather/father and brother), neighbor, known adult (comprising teacher, pastor, police, doctor, and imam), stranger (including gangster), and other perpetrator. Formative research documented presence of Chinamwali, a Malawian rite of passage that can include forced sex, undergone by adolescent girls at the start of puberty. Chinamwali experiences were assessed via “In some communities in Malawi girls attend/undergo chinamwali. Have you gone through chinamwali?” These items were the only measures unique to Malawi.AnalysisData were entered by a single individual and independently reviewed for accuracy by a second individual. The unit of analysis is the school level. We attempted anonymous matching of baseline and follow-up surveys for individual-level statistical analysis; participants self-created an identification code based on responses to questions for which only they would know the answer. The elements of the self-created identification codes proved suboptimal; for example, participants had limited knowledge regarding month and year of birth, and matching across time periods was poor. Individual-level baseline characteristics of participants by school type and intervention arm were compared using chi-squared statistics. Subsequent matching analyses and comparisons across time points were all done at the school-level given the suboptimal matching of individual-level data.To support aggregate analysis at the school level, baseline and follow-up data points were collapsed at the school level to obtain means and counts. The primary and intermediate endpoints included past-year sexual violence prevalence, past-year sexual violence incident rate (primary), disclosure of abuse, self-defense knowledge, and confidence (intermediate). Poisson regression with cluster-correlated robust variance estimates was used to estimate either risk ratios or incident rate ratios per student-month comparing intervention arms by time point. The model included arm, time point (baseline vs. endline), and time-arm interaction to test within-arm baseline to endline relative change. School size (or a product of school size by time period) was used as an offset term in the Poisson regression to constrain the coefficient to one and use it as the denominator for the risk and rates. Results were subsequently stratified by type of school (primary/secondary) and district. Additionally, perpetrator groups were compared between baseline and follow-up; p-values were estimated using generalized linear models with binomial distribution and logit link and school-correlated variance, using each type of perpetrator as the outcome. The sample size fluctuates slightly to accommodate small amounts of missing data. All statistical analyses were conducted in STATA 14 (STATCORP. College Station, TX).All procedures were approved by the Malawi National Commission for Science and Technology (Ref. No. NCST/RTT/2/6). Johns Hopkins conducted post-hoc evaluation analysis with support from UNICEF, and received a non-human subject IRB determination given the anonymous nature of data. This trial was registered with Pan African Clinical Trials Registry (PACTR201702002028911) in February 2017; trial registry did not occur in advance of participant enrollment due to ambiguity on eligibility for trial registry given the applied (rather than clinical) nature of the study. Resource constraints prevented data collection on the pregnancy-related school dropouts initially proposed within the protocol as well as implementation of the companion programming for young men; thus the current analysis reports solely on sexual assault outcomes. The intervention promotes boundary setting through voice and action, with physical self-defense as a last resort. There are possible harms that could result from self-defense, including injury. No adverse events have been reported in past trials [27, 32] of this intervention. For this implementation trial, study staff were directed to immediately report any adverse events to the Ujamaa office; no adverse events were detected.
Results|Individual baseline characteristicsTable 1 disaggregates responses of the 5199 the primary school and 1445 secondary school students at baseline by intervention status and school type. Mean age of primary school students was 15.28 (SD = 2.06) and mean age of secondary school students was 19.55 (SD = 2.70). Salima district comprised approximately 50% of primary school students, whereas approximately 70% of secondary school students were from Lilongwe. Sixteen percent of primary school girls and 26% of secondary school girls had ever been forced to have sex. Prevalence of previous year forced sex was slightly lower than ever forced, with 10% of primary school girls and 18% of secondary school girls reporting forced sex victimization in the past year. Of girls who reported forced sex experiences at baseline, 49% of primary school girls reported only one incident within the past year, whereas 63% of secondary school girls reported multiple incidents.Table 1Baseline Characteristics by Intervention Group§Primary SchoolSecondary SchoolIntervention(n = 2491)n (%)Control(n = 2708)n (%)Overall (n = 5199)n (%)Intervention(n = 757)n (%)Control(n = 688)n (%)Overall (n = 1445)n (%)District Lilongwe702 (28.2)569 (21.0)1271 (24.5)*495 (65.4)497 (72.2)992 (68.7)* Dedza484 (19.4)654 (24.2)1138 (21.9)50 (6.6)59 (8.6)109 (7.5) Salima1305 (52.4)1485 (54.8)2790 (53.7)212 (28.0)132 (19.2)344 (23.8)School Class Class 5929 (37.7)1123 (41.8)2052 (39.8)*––– Class 6819 (33.2)849 (31.6)1668 (32.4)––– Class 7718 (29.1)712 (26.5)1430 (27.8)––– Form 1–––286 (37.8)242 (35.5)528 (36.7) Form 2–––225 (29.7)204 (30.0)429 (29.9) Form 3–––245 (32.4)235 (34.5)480 (33.4)Chinamwali Underwent721 (29.3)1022 (38.0)1743 (33.8)*240 (32.5)219 (32.1)459 (32.3) Did not undergo1742 (70.7)1667 (62.0)3409 (66.2)499 (67.5)464 (67.9)963 (67.7)Ever forced to have sex Yes407 (16.4)403 (14.9)810 (15.6)220 (29.3)154 (22.4)374 (26.0)* No2082 (83.7)2304 (85.1)4386 (84.4)531 (70.7)534 (77.6)1065 (74.0)Past year forced sex None2164 (90.3)2375 (90.6)4539 (90.4)584 (80.9)550 (82.8)1134 (81.8) 1 time120 (5.0)112 (4.3)232 (4.6)53 (7.3)41 (6.2)94 (6.8) 2 times47 (2.0)56 (2.1)103 (2.1)35 (4.9)43 (6.5)78 (5.6) 3 times******23 (3.2)15 (2.3)38 (2.7) 4+ times**52 (2.0)**27 (3.7)15 (2.3)42 (3.0)Open in a separate window§ Sample size fluctuates slightly to accommodate small amounts of missing data, *χ2 test significant at the 0.05 level, **cell with < 2% suppressedPrimary outcomesAcross all students, baseline past-year sexual violence was prevalent in both intervention (15.2%) and control arms (13.8%; Table 2). By follow-up, prevalence in the intervention arm dropped to 9.2%(Risk Ratio(RR)Intervention 0.59 [95% CI 0.49–0.72]), while control arm prevalence remained approximately steady at 14.5% (RRControl1.04[0.86–1.26]); interaction effect p-value < 0.001. Overall incident rates followed a similar trend; they were comparable at baseline (19.6 per 1000 student-months vs. 19.3 control arm), decreased significantly by follow-up in the intervention arm (16.3; Rate RatioIntervention = 0.82 [0.67–1.00]), and increased in the control arm (24.0; Rate RatioControl = 1.22[0.95–1.57].The overall interaction effect favored the intervention arm (p-value =0.01).Table 2Primary and secondary outcomes across intervention and control arms and intervention effects on changes from baseline to follow-upBaselineFollow-upDifference Baseline to Follow-upIntervention EffectInterventionControlInterventionControlInterventionControl% (95% CI) (rate) (95% CI) p-valuea% (95% CI) (rate) (95% CI) p-valueaRR(95% CI)p-valuebRR(95% CI)p-valuebp-valuecAll SchoolsPrimary outcomes Past year sexual violence prevalence15.2%(12.8–18.1)13.8%(11.6–16.3)0.419.2%(7.7–11.0)14.5%(12.0–17.6) 0.001 0.59 (0.49–0.72) < 0.001 1.04(0.86–1.26)0.70 < 0.001 School incident rated (19.6) (16.1–23.9) (19.3) (15.7–23.8) 0.92 (16.3) (13.3–20.0) (24.0) (19.5–29.4) 0.01 0.82(0.67–1.00) 0.05 1.22(0.95–1.57)0.12 0.01 Intermediate outcomes High confidence41.2%(35.7–47.6)26.2%(22.8–30.2) < 0.001 55.0%(50.1–60.3)33.2%(28.9–38.2) < 0.001 1.33 (1.17–1.50) < 0.001 1.26 (1.06–1.49) 0.01 0.63 Correct knowledge15.0%(12.0–18.8)7.0%(5.3–9.2) < 0.001 50.4%(45.6–55.7)8.9%(6.8–11.8) < 0.001 3.33 (2.76–4.02) < 0.001 1.27(0.95–1.70)0.11 < 0.001 Sexual violence disclosuree73.0%(67.4–79.1)68.2%(63.4–73.2)0.2183.7%(78.4–89.3)74.8%(70.8–79.1) 0.01 1.14 (1.04–1.26) 0.007 1.10 (1.01–1.19) 0.02 0.50Primary School Past year sexual violence prevalence13.0%(10.5–16.0)12.1%(10.5–14.1)0.627.5%(6.1–9.3)11.8%(9.9–14.1) 0.001 0.58 (0.45–0.74) < 0.001 0.97(0.78–1.20)0.78 0.002 School incident rated (15.3) (12.3–19.0) (16.9) (13.9–20.6) 0.50 (13.6) (10.7–17.4) (20.2) (16.2–25.4) 0.02 0.88(0.71–1.11).291.20(0.88–1.62)0.250.12 High confidence37.1%(31.4–43.8)23.5%(19.9–27.8) < 0.001 50.9%(45.5–57.0)29.2%(25.1–34.1) < 0.001 1.37 (1.18–1.59) < 0.001 1.24(1.00–1.54)0.050.46 Correct knowledge11.6%(8.9–15.1)5.3%(3.9–7.2) < 0.001 45.7%(40.6–51.5)5.8%(4.2–8.0) < 0.001 3.97 (3.15–5.00) < 0.001 1.09(0.72–1.65)0.68 < 0.001 Sexual violence disclosuree74.0%(67.2–81.6)69.1%(64.6–73.8)0.2584.7%(78.3–91.7)75.4%(70.2–81.0) 0.03 1.14 (1.01–1.28) 0.03 1.09 (1.00–1.19) 0.04 0.58Secondary School Past year sexual violence prevalence22.7%(17.5–29.4)20.1%(13.3–30.3)0.6215.1%(11.6–19.8)23.1%(16.5–32.4) 0.06 0.70 (0.54–0.91) 0.01 1.13(0.78–1.67)0.51 0.04 School incident rated (33.9) (24.7–46.6) (28.8) (17.7–46.9) 0.59 (26.2) (18.8–36.5) (35.8) (25.4–50.3) 0.200.82(0.56–1.19)0.291.23(0.78–1.94)0.380.18 High confidence54.8%(43.1–69.7)36.9%(30.6–44.5) 0.01 69.9%(63.5–77.0)46.0%(36.6–57.8) 0.001 1.30 (1.01–1.66) 0.04 1.24(0.96–1.61)0.100.82 Correct knowledge26.4%(19.8–35.3)13.5%(9.2–19.8) 0.01 67.8(61.9–74.3)18.9%(13.7–26.0) < 0.001 2.57 (1.91–3.45) < 0.001 1.40(0.93–2.09)0.11 0.02 Sexual violence disclosuree71.1%(61.7–81.9)65.9%(54.5–79.8)0.5481.7%(72.7–91.8)73.9%(67.2–81.3)0.191.13(0.93–1.38)0.221.12(0.92–1.36)0.250.94Open in a separate windowaBetween-arm difference within time point; b Between-time point within arm ratio; c Between-arm between-time point ratio of ratios; d Per 1000 person-months; eAmong those affected (past year); boldface illustrates statistical significance at p < 0.05Secondary outcomesSubstantial differences in baseline self-defense confidence were seen across intervention and control groups (full confidence 41.2% vs. 26.2%, p < 0.001). Modest increases in confidence were observed for both intervention (RR = 1.33 [1.17–1.50]) and control (RR = 1.26[1.06–1.49]) groups; the interaction effect was not significant (p = 0.63). While baseline self-defense knowledge was slightly higher for the intervention than control group (full knowledge 15.0% vs. 7.0%, p < 0.001), knowledge increased three-fold in the intervention group by follow-up(RRintervention = 3.33[2.76–4.02]), with no significant difference was observed in the control arm (RRcontrol = 1.27[0.95–1.70]; interaction effect p-value < 0.001.Among those who experienced sexual violence in the past year, violence disclosure increased in both intervention and control groups (Intervention 73.0% vs 83.7%; RRIntervention = 1.14 [1.04–1.26], p = 0.007; Control 68.2% vs. 74.8%; RRControl = 1.10 [1.01–1.19], p = 0.02; resulting in an insignificant interaction effect (p = 0.50).Stratification by primary vs. secondary school and districtIn primary schools, past-year sexual violence prevalence decreased significantly in the intervention arm (baseline 13.0% vs. follow-up 7.5%; RRIntervention 0.58 [95% CI 0.45–0.74]), and remained steady in the control arm (12.1% vs. 11.8%; RRControl 0.97 [95% CI 0.78–1.20]); intervention effect p-value = 0.002. In primary schools, differences in sexual violence incident rates were not statistically significant, but in the direction of a decrease in the intervention arm and an increase in the control arm (Rate RatioIntervention = 0.88[0.71–1.11]; Rate RatioControl = 1.20[0.88–1.62]; overall interaction effect p-value = 0.12. Significant interaction effects were observed for self-defense knowledge (p < 0.001), while confidence and disclosure increased for both intervention and control arms with nonsignificant intervention interaction effects.In secondary schools, where past-year sexual violence was most prevalent at baseline (22.7% intervention, 20.1% control), prevalence similarly decreased significantly in the intervention arm (RRintervention = 0.70[0.54–0.91]) and remained unchanged in the control arm (RRcontrol = 1.13[0.78–1.67]); intervention effect p-value = 0.04). Differences in sexual violence incident rates were not statistically significant, though trended towards a reduction in intervention schools and an increase in control schools (Rate RatioIntervention = 0.82[0.56–1.19]; Rate RatioControl = 1.23[0.78–1.94]; overall interaction effect p-value = 0.18. Similar to primary schools, an interaction effect was observed for self-defense knowledge (p = 0.02), whereas confidence and disclosure increased for both arms with nonsignificant intervention interaction effects.In district-stratified analysis (data not shown), significant interaction effects (p < 0.05) were observed for sexual violence prevalence in Lilongwe and Salima districts. In Dedza, intervention impact was concentrated among secondary students (interaction effect p = 0.03). The lack of significant effect observed for primary students in Dedza may reflect statistical power issues stemming from the low baseline prevalence of sexual violence among Dedza primary students (8.3% intervention arm; 11.5% control arm), coupled with their somewhat smaller overall sample size.Perpetrator mixOverall, across intervention arms, time periods, and school levels, most sexual violence perpetrators were known to victims (Table 3). Boyfriends were the most common perpetrators for primary (34.5–45.4%) and secondary students (41.0–68.6%) across both intervention and control groups, followed by relatives, friends and neighbors. Only 9–15% of primary students affected by violence indicated strangers as perpetrators; < 1–9.5% among secondary students. Amongst the intervention group, the proportion of sexual assaults perpetrated by relatives decreased significantly from 21.0% at baseline to 9.8% at follow-up (p = 0.01). Perpetration by boyfriends, other known adults, and strangers held steady. Several changes in the control group were also noted during this time, including decreases in the proportion of assaults perpetrated by neighbors among those victimized. When stratified by school level, the decrease in relative-perpetrated violence was found concentrated among primary school students in the intervention arm (26.0 to 8.9%, p = 0.001). Among secondary school intervention arm students, there were no significant changes in sexual violence perpetrators, however in the control arm, a higher proportion of relatives and strangers were indicated as violence perpetrators at follow-up, while the proportion of boyfriends as perpetrators decreased from baseline to follow-up (68.6 to 41.0%; p = 0.004).Table 3Perpetrators* of past-year forced sex among those who experienced sexual violence at baseline and follow-up, respectively, by intervention statusInterventionControlBaseline (n = 452)n (%)Follow-Up (n = 194)n (%)p-valueaBaseline (n = 423)n (%)Follow-Up (n = 268)n (%)p-valueaAll Students Boyfriend203 (44.9)90 (46.4)0.46188 (44.4)117 (43.7)0.75 Friend51 (11.3)30 (15.5)0.1041 (9.7)33 (12.3)0.33 Relative95 (21.0)19 (9.8) 0.01 79 (18.7)57 (21.3)0.47 Neighbor37 (8.2)23 (11.9)0.1546 (10.9)13 (4.9) 0.01 Known Adult24 (5.3)10 (5.2)0.9529 (6.9)16 (6.0)0.66 Stranger32 (7.1)21 (10.8)0.1028 (6.6)28 (10.5)0.19 Other10 (2.2)**0.1912(2.8)**0.21Primary School Only Boyfriend103 (35.6)45 (36.6)0.57103 (34.5)74 (45.4) 0.05 Friend32 (11.1)24 (19.5) 0.02 32 (10.7)26 (16.0)0.15 Relative75 (26.0)11 (8.9) 0.001 62 (20.7)26 (16.0)0.08 Neighbor25 (8.7)18 (14.6)0.0839 (13.0)5 (3.1) < 0.001 Known Adult20 (6.9)6 (4.9)0.5925 (8.4)12 (7.4)0.68 Stranger29 (10.0)18 (14.6)0.1427 (9.0)18 (11.0)0.63 Other****0.5311 (3.7)**0.12Secondary School Only Boyfriend100 (61.4)45 (63.4)0.6185 (68.6)43 (41.0) 0.004 Friend19 (11.7)6 (8.5)0.549 (7.3)7 (6.7)0.90 Relative20 (12.3)8 (11.3)0.9517 (13.7)31 (29.5) 0.001 Neighbor12 (7.4)5 (7.0)0.997 (5.7)8 (7.6)0.42 Known Adult4 (2.5)4 (5.6)0.144 (3.2)4 (3.8)0.82 Stranger**3 (4.2)0.37**10 (9.5) 0.03 Other5 (3.1)** < 0.001 ****0.32Open in a separate windowboldface illustrates statistical significance at p < 0.05*not mutually exclusive**cell with < 2% suppressedageneralized linear models with binomial distribution and logit link and school-correlated varianceSkills mixWithin the intervention group, 43% of girls said that they had used skills to stop forced sex since IMPower ESD training. Of the girls that used the skills, 49% used verbal only, 13% used physical only, and 38% used a combination of verbal and physical skills. Of those, 52% reported using the learned skills more than once to stop forced sex. The skills learned within IMPower extended beyond sexual violence prevention, with an additional 53% of girls reporting using the skills to stop harassment and 52% reporting using the skills to stop physical violence since the ESD training (data not shown).DisclosureThose who disclosed sexual violence were most likely to turn to friends (approximately 50% at both baseline and follow up), with fewer than 7% reporting to law enforcement (data not shown). While approximately 80% of girls disclosed their violence experience to someone, they chose to disclose to friends and other members of their informal networks, instead of police or medical professionals.
Discussion|Results from this cluster-randomized controlled implementation trial support the effectiveness of school-based, empowerment self-defense training as a promising strategy in reducing risk for sexual violence for girls in both primary and secondary schools in Malawi. Compared with control participants, students assigned to IMPower intervention reported reductions in past-year sexual violence from baseline to follow-up, and increases in self-defense-related knowledge. IMpower has been previously found effective in reducing sexual violence in the densely-populated urban settlements of Nairobi, Kenya [27, 28]; current findings provide a critical replication and extension of this work by demonstrating similar reductions in sexual assault prevalence, as well as increases in knowledge in self-defense, in the far more diffuse setting of Malawi (197.5 people/kmsq; 2017) [33],across a heterogeneity of districts, and across both primary and secondary school levels. Results provide timely direction for addressing the epidemic of sexual violence against young women in Malawi, where an estimated one in four young women experience sexual violence by the time of secondary school.Several potential mechanisms may underlie the reductions observed in sexual violence among ESD participation in this study, and in past evaluations [27, 28, 32]. The observed increases in self-defense knowledge in the intervention arm relative to controls, may in part explain the reductions in sexual violence. Empowerment and social cognitive theory would suggest knowledge as necessary but insufficient to change in health-related behavior, and would suggest power, self-efficacy, or confidence as necessary to implement change. Surprisingly, confidence in responding to threatening situations increased in both intervention and control arms with no significant intervention effect observed. It is possible that our current measures, while tailored to IMpower content, were insufficiently sensitive to capture meaningful changes in confidence and empowerment resulting from the program. It is also possible that natural maturation is responsible for the observed increases in confidence across both arms. Our findings should be interpreted with caution given measurement limitations in assessing self-confidence (2 items), particularly given recent evidence from Nairobi, which showed increases in self-efficacy as assessed on a generalized scale, when IMpower was implemented in concert with programming for young men [32]. Finally, it is possible that IMpower shifted participants’ interpersonal power in ways that were perceived by potential sexual violence perpetrators and changed their behavior. Evaluating this mechanism would require community-level research, in addition to the participant data that has served as the basis for this and related studies.The vast majority of sexual violence perpetrators were known to victims, with boyfriends the most commonly reported perpetrators at baseline and follow-up, across primary and secondary schools and intervention and control groups. Results are consistent with past research in Malawi and globally indicating boyfriends, intimate partners and other acquaintances as leading perpetrators of violence against women [15, 34]. In the control arm, among secondary students, proportion of violence survivors reporting boyfriends as perpetrators decreased while proportion of survivors reporting relatives as perpetrators increased by follow-up. Intervention arm primary schools saw decreases in the proportion of survivor reporting relatives as perpetrators by follow-up. Further research, including qualitative inquiry, can help inform the ways in which this intervention may function differentially for different types of perpetrators. Differences observed in perpetrator mix at baseline and follow-up, however results should be interpreted with caution in that they represent the portion of survivors reporting a given perpetrator type rather than prevalence or incidents by perpetrator.Results should be considered in light of several additional limitations. As in past evaluations of this program [27, 28, 32], the inability to link individual surveys across baseline and follow-up constrains evaluation of mechanisms responsible for changes observed, and discernment of primary prevention (preventing new incidents among unaffected individuals) vs secondary prevention (reducing recurrent exposure among those who have experienced sexual violence). The evaluation’s emphasis on efficient survey data collection for this demonstration project prompted some measurement limitations. Most notably, we lack data on demographic characteristics that could be used to explore effects by specific groupings. Sexual violence assessment was limited to forced sex, which offers specificity yet overlooks non-consensual sexual experiences resulting from coercion, threats or pressure that did not rise to the level of physical force, and thus may underestimate the extent of sexual violence. Future research should also explore intervention effects on sexual harassment and physical and emotional abuse, which were not assessed for time considerations. The relatively short follow-up duration limits our ability to understand the durability of findings. The self-reported nature of data, single-blind design with students, but not school or study staff, blind to intervention assignment, and instructor-led survey administration could introduce error or bias, despite the measures in place to enhance confidentiality and comfort. Our use of a past-year prevalence outcome for sexual violence is a limitation in precision for follow-up estimates given the 10.5 month length to follow-up; this imprecision is evenly distributed across intervention and control arms and is not likely to introduce bias. Statistical inferences were consistent in sensitivity analysis converted to adjust for time difference. While the IMpower program targets girls specifically, we note that sexual violence also affects young men [15]. Finally we note that while IMPower is designed to increase skills, confidence, and safety behavior in responding to sexual violence and potentially threatening situations, the ultimate behavioral responsibility for sexual violence prevention rests with perpetrators, not victims, of violence.
Abstract|It has been reported that ACE2 is the main host cell receptor of 2019-nCoV and plays a crucial role in the entry of virus into the cell to cause the final infection. To investigate the potential route of 2019-nCov infection on the mucosa of oral cavity, bulk RNA-seq profiles from two public databases including The Cancer Genome Atlas (TCGA) and Functional Annotation of The Mammalian Genome Cap Analysis of Gene Expression (FANTOM5 CAGE) dataset were collected. RNA-seq profiling data of 13 organ types with para-carcinoma normal tissues from TCGA and 14 organ types with normal tissues from FANTOM5 CAGE were analyzed in order to explore and validate the expression of ACE2 on the mucosa of oral cavity. Further, single-cell transcriptomes from an independent data generated in-house were used to identify and confirm the ACE2-expressing cell composition and proportion in oral cavity. The results demonstrated that the ACE2 expressed on the mucosa of oral cavity. Interestingly, this receptor was highly enriched in epithelial cells of tongue. Preliminarily, those findings have explained the basic mechanism that the oral cavity is a potentially high risk for 2019-nCoV infectious susceptibility and provided a piece of evidence for the future prevention strategy in dental clinical practice as well as daily life.
Introduction|Since December 2019, an increasing number of patients with pneumonia occurred in Wuhan, Hubei province, China, which attracted much attention not only within China but across the world1,2. The novel pneumonia was named as Corona Virus Disease 19 (COVID-19) by World Health Organization (WHO) (https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200211-sitrep-22-ncov.pdf?sfvrsn=fb6d49b1_2), the common symptoms of COVID-19 at illness onset were fever, fatigue, dry cough, myalgia, and dyspnea3. In addition, some patients might suffer from headache, dizziness, abdominal pain, diarrhea, nausea, and vomiting3. Onset of disease may lead to progressive respiratory failure due to alveolar damage and even death4.Scientists then isolated a novel coronavirus from human airway epithelial cells, which was named 2019-nCoV5. Lu et al.6 found that 2019-nCoV was closer to bat-SL-CoVZC45 and bat-SL-CoVZXC21 at the whole-genome level, and the external subdomain of the 2019-nCoV receptor-binding domain (RBD) was more similar to that of severe acute respiratory syndrome (SARS) coronavirus (SARS-CoV). Study of Zhou et al.4 indicated that the angiotensin-converting enzyme II (ACE2) is likely the cell receptor of 2019-nCoV, which were also the receptor for SARS-CoV and HCoV-NL637,8. Zhou et al.4 also proved that 2019-nCoV does not use other coronavirus receptors, aminopeptidase N, and dipeptidyl peptidase 4. The study of Xu et al.9 found that the RBD domain of the 2019-nCoV S-protein supports strong interaction with human ACE2 molecules. These findings suggest that the ACE2 plays an important role in cellular entry, thus ACE2-expressing cells may act as target cells and are susceptible to 2019-nCoV infection10.The expression and distribution of the ACE2 in human body may indicate the potential infection routes of 2019-nCoV. Through the developed single-cell RNA sequencing (scRNA-Seq) technique and single-cell transcriptomes based on the public database, researchers analyzed the ACE2 RNA expression profile at single-cell resolution. High ACE2 expression was identified in type II alveolar cells (AT2) of lung10,11,12, esophagus upper and stratified epithelial cells, absorptive enterocytes from ileum and colon12, cholangiocytes13, myocardial cells, kidney proximal tubule cells, and bladder urothelial cells10. These findings indicated that those organs with high ACE2-expressing cells should be considered as potential high risk for 2019-nCoV infection10.In order to investigated the potential routes of 2019-nCov infection on the mucosa of oral cavity, we explored whether the ACE2 is expressed and the ACE2-expressing cell composition and proportion in oral cavity based on the public bulk RNA-seq profiles from two public databases and single-cell transcriptomes from an independent data generated in-house. The result showed that the ACE2 could be expressed in the oral cavity, and was highly enriched in epithelial cells. Moreover, among different oral sites, ACE2 expression was higher in tongue than buccal and gingival tissues. These findings indicate that the mucosa of oral cavity may be a potentially high risk route of 2019-nCov infection.
Results|Public bulk RNA-seq dataset analysisNA-seq profile data of 13 organs including 695 para-carcinoma normal tissues as control from public TCGA were obtained for our analysis, and Fig. 1a showed that ACE2 could be expressed in various organs, the mean expression of different organs could be found in Table 1. According to the mean expression of ACE2, the mucosa of oral cavity could express ACE2, and the results were validated by the data of normal tissues from the FANTOM5 CAGE dataset (Fig. 1b).Fig. 1Bulk RNA-seq analysis of public datasets. a Violin plot of ACE2 expression in para-carcinoma normal tissues from TCGA, colored by organs. b Bar plot of ACE2 expression in normal tissues from FANTOM5 CAGE dataset, colored by organs. c Bar plot of ACE2 expression of para-carcinoma normal tissues from TCGA in different oral sites, colored by oral sites; d Boxplot of ACE2 in two kinds of oral sites from TCGA, colored by oral sitesFull size imageTable 1 Sample size and ACE2 expression of para-carcinoma normal tissues in different organsFull size tableTo investigate the ACE2 expression on mucosa of oral cavity, we looked into the ACE2 expression in different oral sites. According to the site information provided by the TCGA, among the 32 adjacent normal tissues, 13 tissues located in the oral tongue, 2 tissues located in the base of tongue, 3 tissues located in the floor of mouse, and 14 tissues did not definite the site and were just put into the category of oral cavity. The mean expression distribution of different sites was shown in Fig. 1c. When we combined the base of tongue, floor of mouth and oral cavity as other sites, and compared them with oral tongue, we found the obvious tendency that the mean expression of ACE2 was higher in oral tongue (13 tissues) than others (19 tissues) (Fig. 1d), while may due to the limitation of the sample size, the p value was not significant (P = 0.062).Single cell RNA-seq analysis of oral tissuesSingle cell RNA-seq was utilized for four oral tissues, and the data was analyzed to confirm the above results and assess the cell type-specific expression of ACE2. After the data preprocessing (shown in section “Materials and methods”), 22 969 cells were acquired and 7 cell types were identified (Fig. 2a), including epithelial cells (marker genes including SFN, KRT6A, and KRT10), fibroblasts (marker genes including FAP, PDPN, COL1A2, DCN, COL3A1, COL6A1), T cells (marker genes including CD2, CD3D, CD3E, and CD3G), macrophages (marker genes including CD163, CSF1R, CD68, and FCGR2A), mast cells (marker genes including CMA1, MS4A2, TPSAB1, TPSB2), B cells (marker genes including SLAMF7, FCRL5, and CD79A) and endothelial cells (marker genes including PECAM1, VWF, and ENG). The heatmap of main cell markers across the cell types can be found in Fig. 2b.Fig. 2Single cell RNA-seq analysis of oral tissues from independent dataset. a Seven-cell types were identified by the cell markers; cells were clustered by UMAP method. b Heatmap of cell markers for identifying cell types. c Scatter plots of all the cells with ACE2 expression. d Violin plot of ACE2 expression across different oral sites. e Violin plot of ACE2 expression across different cell types. f Scatter plots of tongue epithelial cells with ACE2 expressionFull size imageAccording to Fig. 2c, d, we confirmed the ACE2 was expressed in oral tissues (0.52% ACE2-positive cells), and higher in oral tongue than buccal and gingival tissues (95.86% ACE2-positive cells located in oral tongue). Figure 2e shows that the ACE2-positive cells could be found in oral tissues including epithelial cells (1.19% ACE2-positive cells), T cells (<0.5%), B cells (<0.5%), and fibroblast (<0.5%), and the ACE2 was highly enriched in epithelial cells, of which 93.38% ACE2-positive cells belong to epithelial cells (Fig. 2f). The above results indicated that the ACE2 could be expressed on the epithelial cells of the oral mucosa and highly enriched in tongue epithelial cells.
Discussion|In the last two decades, coronavirus has caused two large-scale pandemics, SARS in 2002 and the Middle East respiratory syndrome (MERS) in 201214. In December 2019, a novel coronavirus (2019-nCov) induced an outbreak of pneumonia in Wuhan, China, restated the risk of coronaviruses posed to public health15. The infection routes and pathogenesis of 2019-nCov are not fully understood by far, and the study of 2019-nCoV host cell receptor ACE2 could be valuable for the prevention and treatment of the COVID-19.In this study, the analysis of public bulk-seq RNA datasets showed that the mucosa of oral cavity could express the ACE2 and was higher in tongue than other oral sites. The results of this study were consistent with the study of Zou et al.10 in general, many organs with higher expression of ACE2 than lung, such as intestine, heart, and kidney. According to the study of Zhao et al.11, the ACE2 expression in lung is concentrated in a small population of type II alveolar cells (AT2), that may cause the relatively low ACE2 expression of lung in bulk-seq RNA datasets analysis. Even though, the result of Zou et al. indicated that the respiratory tract should also be considered as a vulnerable target to 2019-nCoV infection10.The results of our single cell RNA-seq profiles validated the ACE2 expression in oral cavity, and the level of ACE2 expression in oral tissues was higher in tongue than buccal or gingival tissues. Furthermore, we have also demonstrated that the ACE2-positive cells were enriched in epithelial cells, which was also reported by previous study10,11,12,16. These findings indicated that oral cavity could be regarded as potentially high risk for 2019-nCov infectious susceptibility.Interestingly, we found that the ACE2 also expressed in lymphocytes within oral mucosa, and similar results were found in various organs of the digestive system and in lungs11,12. Whether those facts have reminded the 2019-nCoV attacks the lymphocytes and leads to the severe illness of patients needs more in vitro and in vivo evidence and validations, though the proportion of ACE2-positive lymphocytes is quite small.Previous studies have investigated the ACE2 mRNA and protein expression in various tissues by bulk samples17,18, however, the distribution of ACE2 through bulk data could not indicate the cell type-specific expression of ACE2. Recently developed single-cell RNA-sequencing technology enabled the generation of vast amounts of the transcriptomic data at cellular resolution19. The ACE2 expression profile in various organs, tissues, and cell types, provides the bioinformatics evidence for the potential infection routes of 2019-nCov, which might also be associated with presented symptoms.Although studies have reported multiple symptoms of hospitalized patients with 2019-nCoV infection3,20, some cases at home might be asymptomatic. It is worth noting that, a previous study showed that 99% of the patients had no clinical manifestation of oral human papillomavirus (HPV), but HPV DNA was detected in 81% of oral mucosa samples, and anti-HPV IgA was detected in the saliva of 44% of the patients21. Likewise, although 2019-ncov infection hardly presented oral symptoms, the ACE2 expression in the oral cavity indicated that the oral infection route of 2019-nCoV cannot be excluded. Moreover, a latest pilot experiment showed that 4 out of 62 stool specimens tested positive to 2019-nCoV, and another four patients in a separate cohort who tested positive to rectal swabs had the 2019-nCoV being detected in the gastrointestinal tract, saliva, or urine20. Thus, our results support that in addition to the respiratory droplets and direct contact, fecal–oral transmission might also be the route of transmission of 2019-nCoV.Our results are mainly based on public datasets and single cell RNA-sequencing data of in-house oral tissues with minimal diseased lesion which from our previous project found no significant expression difference among the common epithelial markers in our past study and other previous study22,23,24. It is warrant that further histological methods are used to confirm our results and enhance the persuasion of the conclusion.The ACE2-expressing cells in oral tissues, especially in epithelial cells of tongue, might provide possible routes of entry for the 2019-nCov, which indicate oral cavity might be a potential risk route of 2019-nCov infection. Those preliminary findings have explained the basic mechanism that the oral cavity is a potentially high risk for 2019-nCoV infectious susceptibility and provide a piece of evidence for the future prevention strategy in clinical practice as well as daily life.
Abstract|The vagaries of history lead to the prediction that repeated instances of evolutionary diversification will lead to disparate outcomes even if starting conditions are similar. We tested this proposition by examining the evolutionary radiation of Anolis lizards on the four islands of the Greater Antilles. Morphometric analyses indicate that the same set of habitat specialists, termed ecomorphs, occurs on all four islands. Although these similar assemblages could result from a single evolutionary origin of each ecomorph, followed by dispersal or vicariance, phylogenetic analysis indicates that the ecomorphs originated independently on each island. Thus, adaptive radiation in similar environments can overcome historical contingencies to produce strikingly similar evolutionary outcomes.
Abstract|BackgroundPosttraumatic stress disorder is a prevalent mental health condition with substantial impact on daily functioning that lacks sufficient treatment options. Here we evaluate six phase 2 trials in a pooled analysis to determine the study design for phase 3 trials of MDMA-assisted psychotherapy for PTSD.MethodsSix randomized, double-blind, controlled clinical trials at five study sites were conducted from April 2004 to February 2017. Active doses of MDMA (75–125 mg, n = 72) or placebo/control doses (0–40 mg, n = 31) were administered to individuals with PTSD during manualized psychotherapy sessions in two or three 8-h sessions spaced a month apart. Three non-drug 90-min therapy sessions preceded the first MDMA exposure, and three to four followed each experimental session.ResultsAfter two blinded experimental sessions, the active group had significantly greater reductions in CAPS-IV total scores from baseline than the control group [MMRM estimated mean difference (SE) between groups − 22.0 (5.17), P < 0.001]. The between-group Cohen’s d effect size was 0.8, indicating a large treatment effect. After two experimental sessions, more participants in the active group (54.2%) did not meet CAPS-IV PTSD diagnostic criteria than the control group (22.6%). Depression symptom improvement on the BDI-II was greatest for the active group compared to the control group, although only trended towards significant group differences [MMRM, estimated mean difference (SE) between groups − 6.0 (3.03), P = 0.053]. All doses of MDMA were well tolerated, with some expected reactions occurring at greater frequency for the active MDMA group during experimental sessions and the 7 days following.ConclusionsMDMA-assisted psychotherapy was efficacious and well tolerated in a large sample of adults with PTSD. These studies supported expansion into phase 3 trials and led to FDA granting Breakthrough Therapy designation for this promising treatment.Trial registrationClinicalTrials.gov Identifier: {"type":"clinical-trial","attrs":{"text":"NCT00090064","term_id":"NCT00090064"}}NCT00090064, {"type":"clinical-trial","attrs":{"text":"NCT00353938","term_id":"NCT00353938"}}NCT00353938, {"type":"clinical-trial","attrs":{"text":"NCT01958593","term_id":"NCT01958593"}}NCT01958593, {"type":"clinical-trial","attrs":{"text":"NCT01211405","term_id":"NCT01211405"}}NCT01211405, {"type":"clinical-trial","attrs":{"text":"NCT01689740","term_id":"NCT01689740"}}NCT01689740, {"type":"clinical-trial","attrs":{"text":"NCT01793610","term_id":"NCT01793610"}}NCT01793610.Electronic supplementary materialThe online version of this article (10.1007/s00213-019-05249-5) contains supplementary material, which is available to authorized users.Keywords: MDMA, MDMA-assisted psychotherapy, Posttraumatic stress disorder, Anxiety, Psychedelic
Introduction|Posttraumatic stress disorder (PTSD) is a serious debilitating disorder with lifetime prevalence estimated at nearly 4% globally and over 8% in the USA (Kilpatrick et al. 2013; Koenen et al. 2017). Symptoms of PTSD include intrusive thoughts and memories, negative effects on cognition and mood, hyperarousal and reactivity, and avoidance that do not remit for at least 1 month subsequent to exposure to a traumatic event (Koenen et al. 2017). Individuals with PTSD may experience a substantial reduction in quality of life and relationships, and the disability resulting from PTSD can have further negative consequences such as obesity (Scott et al. 2008), hypertension (Kibler et al. 2009), comorbid mental health conditions, and suicidality (Dorrington et al. 2014; Tarrier and Gregg 2004). In addition to these profound costs to individuals with PTSD, the disorder also exerts a substantial economic toll through lost productivity and treatment costs (Marshall et al. 2000).Widely used treatments for PTSD include psychotherapies and medications. A recent review identified trauma-focused psychotherapies as first-line treatments for PTSD (Lee et al. 2016); however, while a substantial proportion of individuals with PTSD respond to psychotherapies [e.g., cognitive processing therapy (Monson et al. 2006; Resick et al. 2008) and prolonged exposure therapy (Foa et al. 2007)], these therapies may be difficult to access and are ineffective for many (Koenen et al. 2017; Steenkamp et al. 2015). A variety of medications have also been used to address PTSD symptoms, but only two drugs—sertraline and paroxetine—are approved by the FDA for PTSD. Extant pharmacotherapies, however, are ineffective for many individuals with PTSD, with an estimated 40–60% of patients not responding adequately (Bradley et al. 2005; Brady et al. 2000; Steenkamp et al. 2015). They may have problematic side effects and generally require long-term use to maintain effectiveness (Lee et al. 2016). In sum, the sizable proportion of cases of PTSD are persistent (Koenen et al. 2017) and the shortcomings of currently available treatments make the development of novel PTSD treatments a research priority.A promising approach to the treatment of PTSD is the combination of psychotherapy with pharmacotherapy using 3,4-methylenedioxymethamphetamine (MDMA). Interest in the therapeutic potential of MDMA for trauma-related psychopathology developed in the context of the broader potential for MDMA to catalyze psychotherapeutic processes by facilitating communication and connection between therapists and patients (Nichols 1986). MDMA was first synthesized in 1912 by Merck, but it was not until the early 1970s that MDMA was first used in combination with psychotherapy. Case reports from that period described therapeutic benefits, although no clinical trials were conducted at that time. Recreational use of “Ecstasy,” tablets purported to contain MDMA, became popular in the 1980s, leading to its classification as a Schedule 1 controlled substance in 1985. The scheduling of MDMA made its use in therapy illegal and created obstacles to clinical research. A non-profit organization, the Multidisciplinary Association for Psychedelic Studies (MAPS), filed a Drug Master File (DMF) application in 1986, followed by an Investigational New Drug (IND) application in 2001, embarking on the FDA drug development process to study the safety and efficacy of MDMA as an adjunct to psychotherapy for PTSD (Greer and Tolbert 1986; Grof 2001; Mithoefer 2011, 2017; Mithoefer et al. 2018).After nonclinical toxicity studies and an investigator-initiated phase 1 study of MDMA were completed (Frith et al. 1987; Grob et al. 1996, 1998), six phase 2 randomized trials of MDMA-assisted psychotherapy for treatment of PTSD were conducted from 2004 to 2017. Active doses of MDMA (75–125 mg) or control doses of inactive placebo or low-dose MDMA (25–40 mg) were combined with manualized inner-directed psychotherapy (Mithoefer 2017) in which participants were supported by a male and female therapy team (Mithoefer et al. 2011, 2018). The therapeutic model described in the Treatment Manual was based upon initial work with classic psychedelics (Grof 2001; Mithoefer 2017) and early reports of MDMA in a therapeutic setting (Greer and Tolbert 1986). Four of these MAPS-sponsored studies have been published (Mithoefer et al. 2011, 2013, 2018; Oehen et al. 2013; Ot’alora et al. 2018), and all six studies demonstrated acceptable safety and promising efficacy results. The MDMA doses selected for phase 2 trials (control—0 mg, 25 mg, 30 mg, 40 mg; active—75 mg, 100 mg, 125 mg) were based on tolerability and subjective effects reported in several prior phase 1 studies (Cami et al. 2000; de la Torre et al. 2000; Grob et al. 1998; Harris et al. 2002; Liechti et al. 2001). Low doses (25 mg, 30 mg, 40 mg) produce some changes in subjective effects that could presumably enhance blinding as an active placebo but would be inadequate for a therapeutic response (Harris et al. 2002). The FDA, after reviewing all available data in 2016, granted Breakthrough Therapy Designation in 2017 and approved the designs of two phase 3 trials that started in 2018.To optimize the design of the phase 3 trials, we pooled data from six phase 2 trials that had similar study objectives and designs. We aimed to determine how many MDMA sessions are needed to achieve a clinically significant response, what demographic and other baseline variables might impact outcomes, which safety parameters are essential, the optimal dose, and how best to minimize bias and enhance blinding. To that end, the aim of this paper is to present pooled data from randomized clinical trials at different study sites that evaluated the efficacy and safety profile of MDMA-assisted psychotherapy among individuals with PTSD from a range of causes.
Methods|SettingSix randomized, double-blind phase 2 studies took place at five sites. The sites were located in the USA (MP-1, MP-8, MP-12), Canada (MP-4), Switzerland (MP-2), and Israel (MP-9). Five sites were private practices and one was a psychiatric clinic. Data were collected from April 2004 to March 2017. Studies were approved by the Western-Copernicus Institutional Review Board (Research Triangle or Cary, NC; MP-1, MP-8, MP-12), IRB Services/Chesapeake (Aurora ON; MP-4), Ethics Committee of Solothurn (Switzerland; MP-2), and Helsinki Committee of Beer Yaakov Hospital (Israel; MP-9).The authors assert that all procedures contributing to this work comply with the ethical standards of the relevant national and institutional committees on human experimentation and with the Helsinki Declaration of 1975, as revised in 2008.ParticipantsParticipants were recruited through internet advertisements, referrals by health professionals, and by word of mouth. Candidates had chronic PTSD with symptoms lasting longer than 6 months and Clinician-Administered PTSD Scale for DSM-IV (CAPS-IV) scores ≥ 50 (all studies except MP-4) or ≥ 60 (MP-4) upon enrollment (see eTable 1 for individual study criteria). Studies enrolled men and women, including civilians and veterans/first responders, aged 18 and older with previous inadequate response to at least one pharmacotherapy and/or psychotherapy. An inadequate response to previous treatment was concluded if participants had a CAPS-IV total score indicating moderate to extreme PTSD at screening.Participants underwent extensive screening by independent examiners, including psychological assessments, physical examinations, laboratory testing, and ECG to identify any possible contraindications to receiving MDMA. The Structured Clinical Interview for DSM-IV Axis I Disorders—Research Version (SCID-I-RV) or the SCID-II was used during screening to detect comorbid disorders, and medical and therapy records from outside providers were reviewed. Participants were not excluded for meeting criteria for anxiety disorders or depression but were excluded if they met criteria for past or current psychotic disorder or Bipolar Disorder 1, or for current borderline personality disorder, or eating disorder with active purging. Other exclusion criteria included significant medical diagnoses (contraindications for MDMA), pregnancy or lactation, and weight under 48 kg. Cardiovascular or cerebrovascular disease was excluded, except in one study where candidates with well-controlled hypertension and no other evidence of vascular disease could enroll after additional screening with nuclear stress test and carotid ultrasound. All therapists maintained a Basic Life Support certification, and a study physician was available by telephone throughout the study. In order to be enrolled, individuals had to meet all inclusion/exclusion criteria and agree to comply with all planned study visits. All participants confirmed comprehension of study procedures and gave written informed consent.Participants could not have a diagnosis of substance abuse disorders within 60 days of screening for five studies and within 6 months for one study. Psychiatric medications were tapered and discontinued prior to commencing experimental sessions. Anxiolytics and sedative hypnotics were used as-needed between experimental sessions.Protocols and treatmentsAfter screening and enrollment, participants were randomized through a web-based system (MP-8, MP-12) or a list generated by a blinded randomization monitor (MP-1, MP-2, MP-4, MP-9) to receive blinded doses of placebo/control (0 mg placebo; 25 mg, 30 mg, or 40 mg MDMA) or active doses of MDMA (75 mg, 100 mg, or 125 mg MDMA) at approximately 1:2 ratio. Doses were administered during two 8-h psychotherapy sessions spaced 3–5 weeks apart. The initial dose was followed approximately 1.5–2.5 h later by an optional supplemental dose equal to half the initial dose. Participants could accept or decline the supplemental dose, and could discuss the choice with the therapy team. The team could withhold the supplemental dose if there were contraindicating circumstances. Participants underwent two to three non-drug 90-min therapy sessions prior to the first experimental session. Fifty participants who received 100 mg or 125 mg had a third experimental session, either open label or blinded depending on the study, and one 75 mg participant had a blinded third session before a protocol amendment changed the crossover to occur after two sessions. The control groups subsequently had the option to receive two to three open-label sessions with active dose MDMA in a crossover segment (data not shown). MDMA was synthesized by David Nichols at Purdue University. Gelatin capsules were compounded with lactose to produce equivalent-weight capsules across dose groups.The same male/female therapy team was present for all therapy sessions for a given participant. There were 18 therapy teams across the six studies. All but one team (MP-2) were trained in the MAPS Therapy Training Program based on the method described in the MDMA-assisted Psychotherapy Treatment Manual (Mithoefer 2017). The method includes periods of introspection alternating with periods of communication between therapists and the participant. The method is aimed at allowing participants to revisit traumatic experiences while staying emotionally engaged even during intense feelings of anxiety, pain, or grief without feeling overwhelmed. The relatively non-directive approach is intended to allow for processing of other psychological, interpersonal, or behavioral aspects of the participants’ lives that are likely to arise spontaneously in addition to processing the traumatic memories that led to PTSD.Experimental sessions took place in a designated area that contained a futon or sofa, as well as artwork or other objects intended to make the space esthetically pleasing. Participants had the option of wearing eye shades and listening to mostly instrumental music during the parts of experimental sessions when they were focused inward. After the 8-h experimental sessions, participants remained at the study site overnight with a supportive attendant. On the following day, they met with the therapists in a 90-min integration session to address and process material that arose during the experimental session. Two to three more integration sessions occurred during the month after each experimental session. For 7 days following each experimental session, the therapy team checked in with the participants in brief telephone calls to assess wellbeing and safety.AssessmentsAssessments were administered at baseline and at follow-up visits occurring 1 to 2 months after the second and third experimental sessions and at additional time points in some studies. Blinded independent raters not present during therapy sessions administered the CAPS-IV. Safety data were collected throughout treatment. Here, we present a limited set of assessments to support the rationale of this paper.Primary outcome The CAPS-IV is a semi-structured interview addressing PTSD symptom clusters (re-experiencing, avoidance, negative mood or cognition, and increased arousal) as recognized by DSM-IV (Blake et al. 1995; Nagy et al. 1993; Weathers et al. 2001). The CAPS-IV contains frequency and intensity scores for each of the three symptom clusters that are summed to produce a total severity score, the primary outcome for these studies. The CAPS-IV has a dichotomous diagnostic score assigned on the basis of meeting PTSD diagnostic criteria.Secondary outcome The Beck Depression Inventory—II (BDI-II), an established 21-item measure of self-reported depression symptoms (Beck et al. 1996), was administered in four of the six studies (MP-4, MP-8, MP-9, and MP-12). Responses are made on a four-point Likert scale and summed to produce an overall score.Safety outcomes Safety was assessed by tracking the rates of spontaneously reported reactions (subset of adverse events (AEs) that could be expected based on findings from published studies in healthy volunteers) during experimental sessions and 7 days following, and by recording treatment-emergent AEs (TEAEs), which were not collected on the spontaneously reported reactions list, or were reactions that continued for 7 days or more after experimental sessions. Blood pressure and heart rate were measured in intervals of 15 to 30 min, and body temperature every 60 to 90 min during experimental sessions. Suicidal ideation and behavior were collected at all visits and twice during the 7 days of contact in four of the six studies (MP-4, MP-8, MP-9, and MP-12) using the clinician-administered Columbia Suicide Severity Rating Scale (C-SSRS) (Posner et al. 2007, 2011), a structured interview addressing presence and intensity of suicidal ideation and behavior. Participants completed the Paced Auditory Serial Addition Task (PASAT) (Gronwall 1977) and Repeatable Battery for the Assessment of Neuropsychological Status (RBANS) (Randolph 1998) at baseline and 2-month follow-up to determine whether changes in cognitive function had occurred after two sessions with placebo dose or active dose MDMA in specific studies (MP-1, MP-4, and MP-12).Statistical analysisData were pooled across the six studies. Participants who received MDMA (75, 100, 125 mg) were combined into an active dose group; participants who received MDMA (0, 25, 30, 40 mg) were combined for the control group. The modified intent-to-treat set included randomized participants who completed at least one blinded experimental session and a post-baseline assessment. Missing data were not imputed. The safety set included all participants exposed to at least one dose of study drug or placebo.Group differences in baseline characteristics and demographics were evaluated with χ2 tests or independent-samples t tests. The primary efficacy evaluation was made with a mixed-effect repeated measure model (MMRM) on change in CAPS-IV total score from baseline to post second experimental session endpoint, and the post third experimental endpoint. The base model included treatment (active/control), baseline CAPS-IV score, and study as a fixed effect, and participant was specified as a random effect. To assess the relationship between outcome measures and age, PTSD duration, sex, race, and prior self-reported “ecstasy” use (substances assumed to contain MDMA), these variables were added to the base model one at a time. BDI-II scores were analyzed the same way. AEs were categorized with the Medical Dictionary for Regulatory Activities (MedDRA) in System Organ Classes and preferred terms. AEs, reactions, and suicidal ideation and behavior were summarized descriptively. Independent-samples t tests compared peak vital signs during experimental sessions between groups. Between-group effect size was calculated with Cohen’s d (Kadel and Kip 2012). SAS software version 9.3 (SAS Institute Inc., Cary, NC) was used for analyses.
Results|SampleeFigure 1 illustrates flow of participants for these studies. From the 488 telephone-screened, 105 were enrolled and randomized [mean (SD) age, 40.5 (10.5); the majority were white/Caucasian participants (87.6%); and nearly sex balanced (females 58.1%)]. Table ​Table11 displays the characteristics of the sample. Demographic characteristics were approximately matched between treatment arms, and no significant differences were found between groups for demographic and baseline characteristics presented in Table ​Table1.1. The mean (SD) duration of PTSD was 215.3 (190.3) months, with trauma from various causes. Many participants had a lifetime history of positive suicidal ideation (86.8%) and/or behavior (30.9%). The optional supplemental dose was taken in 179/197 (90.9%) of blinded experimental sessions. The dropout rate was 7.6% (8/105), with six participants terminating early, but having completed at least one experimental session and follow-up assessment.Table 1Demographics and baseline characteristicsaControl(n = 31)Active(n = 74)Total(n = 105)Age, mean (SD), years40.4 (8.5)40.5 (11.4)40.5 (10.6)Sex, n (%) Male12 (38.7)32 (43.2)44 (41.9) Female19 (61.3)42 (56.8)61 (58.1)Race, n (%) White/Caucasian27 (87.1)65 (87.8)92 (87.6) Latino/Hispanic1 (3.2)2 (2.7)3 (2.9) Native American1 (3.2)1 (1.4)2 (1.9) Middle Eastern1 (3.2)1 (1.4)2 (1.9) Other/biracial1 (3.2)5 (6.8)6 (5.7)BMI, mean (SD)26.2 (6.1)26.1 (5.4)26.1 (5.6)Duration of PTSD, mean (SD), months197.9 (139.1)222.6 (208.5)215.3 (190.3)Pre-study PTSD medicationsb, n (%)Sertraline10 (32.3)25 (33.8)35 (33.3)Paroxetine4 (12.9)14 (18.9)18 (17.1)Pre-study therapy, n (%)CPT, IPT04 (5.4)4 (3.8)Other CBT24 (77.4)34 (45.9)68 (64.8)EMDR11 (35.5)22 (39.7)33 (31.4)Group therapy4 (12.9)18 (24.3)22 (21.0)PE3 (9.7)5 (6.8)8 (7.6)Psychodynamic9 (29.0)14 (18.9)23 (21.9)Insight6 (19.4)15 (20.3)21 (20.0)Other18 (58.1)49 (66.2)67 (63.8)None02 (2.7)2 (1.9)Prior ecstasy use, n (%) Yes7 (22.6)24 (32.4)31 (29.5) No24 (77.4)50 (67.6)74 (70.5)Lifetime C-SSRSc, n (%) Positive ideation14 (77.8)45 (90.0)59 (86.8) Serious ideation4 (22.2)21 (42.0)25 (36.8) Positive behavior6 (33.3)15 (30.0)21 (30.9)CAPS-IV total score Baseline, mean (SD)81.3 (15.9)85.8 (19.3)84.5 (18.4)BDI-II total scored Baseline, mean (SD)26.1 (10.6)30.2 (11.6)29.1 (11.4)Open in a separate windowBMI, body mass index; CPT, cognitive processing therapy; IPT, interpersonal therapy; CBT, cognitive-behavioral therapy; EMDR, eye movement desensitization and reprocessing therapy; PE, prolonged exposure therapy; C-SSRS, Columbia–Suicide Severity Rating Scale; CAPS-IV, Clinician-Administered PTSD Scale; BDI-II, Beck Depression InventoryaThere were no significant group differences (χ2 or independent t tests) for any variables presented in this tablebSertraline and paroxetine are the only two FDA-approved medications for PTSD. Participants took many other medications for symptom management pre-study that are not presented here. Twelve participants took both sertraline and paroxetinecLifetime accounts for all suicidal ideation and behavior prior to study, according to participant recall and medical records. According to the C-SSRS scoring guide, scores of four or five on the suicidal ideation category are considered serious ideation, and scores of one or greater are considered positive behavior or ideation. Four phase 2 studies administered the C-SSRS (control group n = 18 and active MDMA group n = 50)dFor BDI-II, active group (n = 50) and control group (n = 18)Primary outcomeThe change in CAPS-IV total score (Fig. ​(Fig.1)1) from Baseline to after the second experimental session was significantly different [t(95) = − 4.25, P < 0.0001] between control (0–40 mg) and active (75–125 mg) groups (Table ​(Table2).2). The active group had the greatest estimated mean (SE) drop in scores − 30.4 (3.20) compared to the control group − 10.5 (4.46). The between-group Cohen’s d effect size was 0.8, indicating a large treatment effect. Study, age, PTSD duration, sex, race, and prior “ecstasy” use did not predict outcome in this model.Open in a separate windowFigure 1CAPS-IV total score least squared mean estimates at endpoints. The change in scores from baseline to post two experimental sessions were significantly different between MDMA and control groups (***P < 0.0001). After the third MDMA session, the active dose group showed further improvement compared to post two MDMA sessions (***P < 0.0001)Table 2Outcome measuresaControl(n = 31)ActiveMDMA(n = 72)Mean difference (control vs. active)CAPS-IV total scorePost 2 experimental sessions, LS (SE) changeb− 10.47 (4.46)− 32.43 (3.20)–Difference (active − control)––− 21.95 (5.17)P value0.0208< 0.0001< 0.0001Post 3 experimental sessions, LS (SE) changeb–− 45.39 (3.61)c–P value–< 0.0001–Difference post 3 − post 2, LS (SE) change–− 12.97 (2.89)c–P value–< 0.0001–CAPS-IV PTSD diagnostic criteria met, n (%)Post 2 experimental sessions Yes24 (77.4%)33 (45.8%)– No7 (22.6%)39 (54.2%)–Post 3 experimental sessionsc Yes–24 (47.1%) No–27 (52.9%)–BDI-II total scorePost 2 experimental sessions, LS (SE) changeb− 6.46 (2.69)− 12.44 (1.84)–Difference (active − control)––− 5.97 (3.03)P value0.019< 0.00010.0534Post 3 experimental sessions, LS (SE) changeb–− 17.36 (1.89)–P value–< 0.0001–Difference post 3 − post 2, LS (SE) change–− 9.40 (5.66)–P value–0.1019–Open in a separate windowCAPS-IV, Clinician Administered PTSD Scale; BDI-II, Beck Depression Inventory-II; LS, least square mean estimates; SE, standard erroraAll outcomes are based on intent-to-treat setbCompared to baselinecActive MDMA group (n = 51 for CAPS) post 3 experimental sessions, control group crossed over after 2 blinded sessions, except for MP2 study (data not included)Secondary outcomesAccording to CAPS-IV assessment at the endpoint 1–2 months post two experimental sessions (Table ​(Table2),2), more participants in the active group (54.2%) did not meet PTSD diagnostic criteria than the control group (22.6%). Depression symptom improvement on the BDI-II was greatest for the active dose group, estimated mean (SE) change active − 12.4 (1.84) versus control group − 6.5 (2.69), with the difference between groups trending toward significance [t(61) = − 1.97, P = 0.053].Depending on the study, after two blinded experimental sessions, most participants in the active dose group had one additional open-label (MDMA 100–125 mg, n = 42) or blinded session (MDMA 75–125 mg, n = 9). The estimated mean change (SE) from baseline to post third session on CAPS-IV for the active dose group was − 45.4 (3.61) with a significant further decline from second to third session [t(95) = − 12.58, P < 0.0001]. The within-participant pre-test (baseline) to post-test Cohen’s d effect size increased from 1.4 (post two sessions) to 1.9 (post three sessions). Due to the crossover, there is no between-group comparison for the post third session time point.Safety and tolerabilityTreatment-emergent adverse events (TEAEs) during the blinded treatment segment most commonly reported across all doses included events in the following MedDRA System Organ Classes (SOC): psychiatric disorders, gastrointestinal disorders, and general disorders (eTable 2). The most frequently reported psychiatric TEAEs (Table ​(Table3)3) were anxiety, depressed mood, irritability, and panic attack. On the day of blinded experimental sessions, reactions reported by ≥ 40% of participants in either group were anxiety, dizziness, fatigue, headache, jaw clenching/tight jaw, lack of appetite, and nausea. The majority of expected reactions were rated mild or moderate, and the frequency of reports decreased over the 7 days following an experimental session (eTables 5 and 6). No changes in neurocognitive function were detected (eTable 4).Table 3Treatment-emergent adverse events during the blinded treatment segment and expected reactions during two blinded MDMA sessionsControl(n = 31)ActiveMDMA(n = 72)Total(n = 103)Top reactions during experimental sessions, n (%)a Anxiety15 (48.39)52 (72.22)67 (65.05) Dizziness6 (19.35)29 (40.28)35 (34.00) Fatigue18 (58.06)35 (48.61)53 (51.46) Headache22 (70.97)38 (52.78)60 (58.25) Jaw clenching, tight jaw6 (19.35)46 (63.89)52 (50.49) Lack of appetite7 (22.58)35 (48.61)42 (40.78) Nausea6 (19.35)29 (40.28)35 (33.98)Psychiatric TEAEs, n (%)b Anxiety3 (9.7)17 (23.6)20 (19.4) Depressed mood1 (3.2)6 (8.3)7 (6.8) Irritability03 (5.6)3 (2.9) Panic attack03 (5.6)3 (2.9)Open in a separate windowTEAE, treatment-emergent adverse eventaFrequency of subjects who reported an expected, spontaneously reported reaction collected during blinded experimental sessions 1 and 2 (only reactions reported by ≥ 40% of participants in any group are displayed; see supplemental for full list of reactions)bFrequency of subjects who self-reported psychiatric adverse events after first drug administration until the day before experimental session 3 (only AEs reported by three or more subjects in either group displayed)There were no unexpected MDMA-related SAEs. Four SAEs were reported during the blinded treatment period, including one instance of suicidal ideation (30 mg) (Mithoefer et al. 2018); one SAE of exacerbation of ventricular extrasystoles was reported during an open-label session (125 mg) (Mithoefer et al. 2018) and one SAE of suicidal behavior prior to MDMA exposure in the first experimental session.There was no suicidal behavior during the treatment period after dosing (eTable 3). At baseline, prior to any drug dosing, the active dose group (46%) had much higher rates of positive suicidal ideation than the control group (16.7%), but the lifetime reports (Table ​(Table1)1) were similar between groups. During the treatment phase, suicidal ideation transiently increased in some participants and was more common in the active MDMA group (eTable 3), although the causal relationship to the psychotherapeutic processing of traumatic memories or to MDMA itself, or to random group differences could not be determined.
Discussion|By pooling data across six phase 2 trials, we found significant symptom reductions in a large sample of participants with PTSD treated with active doses of MDMA combined with psychotherapy. The results informed the design of two phase 3 trials (one now ongoing the other to follow) that were approved through a Special Protocol Assessment by the FDA. The reproducible findings attained by various therapy teams in participants with PTSD arising from different types of traumatic experiences demonstrate the generalizability of this manualized drug-therapy approach and the applicability of the MAPS MDMA Therapy Training Program. Overall, the treatment was safe and efficacious for civilians and veterans/first responders with chronic PTSD who had previously failed to respond to pharmacotherapies and/or psychotherapy. More than half of the participants had previously undergone first-line trauma-focused psychotherapies, and all but two participants had received some type of psychotherapy prior to study enrollment. MDMA-assisted psychotherapy was effective for these individuals, suggesting a different mechanism of action for MDMA for reducing PTSD symptoms.The data show that when using the “gold standard” measure of PTSD (CAPS-IV) as a primary outcome measure, with blinded raters, for participants with highly refractory PTSD (mean duration 215.3 months), there was a significant effect after two blinded active doses of MDMA adjunctive with psychotherapy versus psychotherapy with control doses. Notably, more participants in the active dose group (54.2%) no longer met PTSD diagnostic criteria compared to the control group (22.6%). The between-group effect size was large with Cohen’s d equal to 0.8. The effect size was used in power calculations for phase 3 trials. Planned enrollment is 100 participants in each phase 3 trial, with an interim analysis and option for sample size adjustment after 60% of participants have completed the primary endpoint. In addition, depression symptoms trended toward greater improvement in participants receiving active MDMA compared with the control group.After a third experimental session, symptoms on average improved further for the active dose group. The interpretation is limited because the third session was open label for most participants, and there was no control group for comparison due to the open-label crossover after two blinded sessions for most participants. However, it appears that while many people respond adequately after two MDMA sessions, an additional session leads to more participants reaching clinically significant symptom reductions and greater drops in CAPS-IV scores. For this reason, phase 3 trials will include three blinded experimental sessions to maximize response at the primary endpoint (2 months post third experimental session, i.e., 18 weeks post baseline).Phase 2 data showed that 75 mg MDMA produced significant improvement (Mithoefer et al. 2018), yet the sample was quite small (n = 7); therefore, we do not know what the optimal dose is, 75 mg, 100 mg, or 125 mg. There is individual variation in subjective effects of MDMA, and fixed-dose regimens do not account for differences in body weight. To gather more information about optimal dosing, phase 3 trials from 15 sites in the USA, Canada, and Israel will employ a flexible dose regimen. Participants will be randomized to receive equal-weight blinded capsules of inactive placebo or MDMA (80 mg) plus supplemental half-dose unless contraindicated in experimental session one, and then have the choice to escalate the dose to 120 mg with optional supplemental dose (or stay at 80 mg) in the next two sessions. An inactive placebo plus the same psychotherapy will be used as the control group, with the same option to escalate the dose. To minimize bias, a blinded independent rater (IR) pool will administer the primary outcome measure (CAPS-5) to participants across all sites based on availability of IRs. Consecutive assignments to the same IR will not be permitted. Independent raters will remain blinded to the number or timing of CAPS measurements in the study; therefore, we cannot reveal this information until the trials are complete.The safety and tolerability of limited doses of MDMA in highly controlled therapeutic settings in a PTSD population was adequate, consistent with previous phase 1 studies. There was a dose effect for mean increase in vital signs during MDMA sessions (eTable 7), with values returning or trending toward baseline by the end of the 8-h session. Because vital sign increases did not reach clinically concerning ranges, the frequency of required vital measurements will be reduced in phase 3 trials to baseline, pre-supplemental dose, and session end. Vital signs at the pre-supplemental dose reading will be taken into consideration before administering the additional half-dose. Neurocognitive measures will not be employed during phase 3 because phase 2 studies showed no evidence of cognitive impairment after two doses of MDMA (eTable 4).MDMA at all doses tested was well tolerated, as demonstrated by the low rate of TEAEs and expected reactions. Most were mild to moderate, resolving as MDMA effects dissipated or during the week following (eTables 5 and 6). During experimental sessions, the active MDMA group had higher incidences of some reactions, including anxiety, dizziness, jaw clenching/tight jaw, lack of appetite, and nausea. Whether reactions are due to the pharmacological effects of MDMA or from augmented trauma processing catalyzed by MDMA effects cannot be determined from the data collected in these studies, but phase 1 studies in healthy individuals report similar reactions to MDMA. During the 7 days following experimental sessions, some reactions occurred more often in the active dose group for the first few days before declining by the end of the week. For this reason, the number of telephone contacts after an experimental session will be less frequent for phase 3 trials, which will require four telephone contacts over 7 days. In accordance with FDA guidance for all psychiatric drugs under development, the C-SSRS will be given at each in-person visit. In phase 2 trials, there were no related deaths or incidence of suicidal behavior after MDMA. The low dropout rate (7.6%) in MDMA trials compared to other PTSD treatments (approximately 17–36%) (Bradley et al. 2005; Marshall et al. 2001; Steenkamp et al. 2015) could be related to the propensity of MDMA to make trauma processing more tolerable with rapid symptom improvements in the days and weeks following. Participants in the placebo/control group had the opportunity to cross over to receive three open-label (100–125 mg) sessions of MDMA-assisted psychotherapy if they complete the blinded segment which likely motivated participants to complete treatment.MDMA in the context of psychotherapy was found to have a low potential for abuse. There were no AEs or treatment discontinuation related to “ecstasy” seeking or craving, and no reports of use outside the study through the post third session endpoint. Indeed, many participates anecdotally reported that the experimental sessions were not particularly pleasurable experiences, but rather difficult therapeutic work delving into their traumatic memories. Overall, safety outcomes were favorable for use of MDMA in individuals with PTSD in a supportive environment with trained mental health professionals.
Limitations|There are limitations of these trials and the associated pooled data analyses. The sample was nearly gender balanced, but participants and therapists were predominantly White/Caucasian. Phase 3 studies will evaluate the generalizability to individuals from more diverse ethnic and cultural backgrounds. Across the six trials, there were variations in study design, such as differences in timing of outcome measures, doses tested, number of blinded experimental sessions, and number of participants in each dose group. Drawbacks of pooled data analyses are that multiple doses tested were combined into two groups—control group and active dose group—and that the third experimental session was blinded or open-label full-dose MDMA, depending on the study. Also, there was no control group for a between-group comparison of the post third session; therefore, response after three sessions was limited to a within-subject analysis. Due to small sample sizes, reliability of effect size estimates from individual studies is unknown. Blinding of treatment assignment for psychoactive substances is a recognized challenge. Both psychological and vital sign changes during experimental sessions can be clues to the group assignment. To reduce bias, blinded independent raters who were not present during therapy sessions administered the CAPS-IV. However, participants and therapists often, but not always, accurately guessed dose assignment (Mithoefer et al. 2011, 2018; Oehen et al. 2013; Ot’alora et al. 2018)—a recognized limitation in clinical trials of all drugs with perceivable effects and in all psychotherapy studies where there is no possibility of effective blinding.
Abstract|A series of dipyridodiazepinones have been shown to be potent inhibitors of human immunodeficiency virus-1 (HIV-1) reverse transcriptase (RT). One compound, BI-RG-587, had a Ki of 200 nanomolar for inhibition of HIV-1 RT that was noncompetitive with respect to deoxyguanosine triphosphate. BI-RG-587 was specific for HIV-1 RT, having no effect on feline and simian RT or any mammalian DNA polymerases. BI-RG-587 inhibited HIV-1 replication in vitro as demonstrated by in situ hybridization, inhibition of protein p24 production, and the lack of syncytia formation in cultured human T cell lines and freshly isolated human peripheral blood lymphocytes. Cytotoxicity studies of BI-RG-587 on human cells showed a high therapeutic index (greater than 8000) in culture.
Abstract|A complementary DNA for the Aequorea victoria green fluorescent protein (GFP) produces a fluorescent product when expressed in prokaryotic (Escherichia coli) or eukaryotic (Caenorhabditis elegans) cells. Because exogenous substrates and cofactors are not required for this fluorescence, GFP expression can be used to monitor gene expression and protein localization in living organisms.
Abstract|BackgroundCoronavirus disease 2019 (Covid-19) occurs after exposure to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). For persons who are exposed, the standard of care is observation and quarantine. Whether hydroxychloroquine can prevent symptomatic infection after SARS-CoV-2 exposure is unknown. MethodsWe conducted a randomized, double-blind, placebo-controlled trial across the United States and parts of Canada testing hydroxychloroquine as postexposure prophylaxis. We enrolled adults who had household or occupational exposure to someone with confirmed Covid-19 at a distance of less than 6 ft for more than 10 minutes while wearing neither a face mask nor an eye shield (high-risk exposure) or while wearing a face mask but no eye shield (moderate-risk exposure). Within 4 days after exposure, we randomly assigned participants to receive either placebo or hydroxychloroquine (800 mg once, followed by 600 mg in 6 to 8 hours, then 600 mg daily for 4 additional days). The primary outcome was the incidence of either laboratory-confirmed Covid-19 or illness compatible with Covid-19 within 14 days. ResultsWe enrolled 821 asymptomatic participants. Overall, 87.6% of the participants (719 of 821) reported a high-risk exposure to a confirmed Covid-19 contact. The incidence of new illness compatible with Covid-19 did not differ significantly between participants receiving hydroxychloroquine (49 of 414 [11.8%]) and those receiving placebo (58 of 407 [14.3%]); the absolute difference was −2.4 percentage points (95% confidence interval, −7.0 to 2.2; P=0.35). Side effects were more common with hydroxychloroquine than with placebo (40.1% vs. 16.8%), but no serious adverse reactions were reported. ConclusionsAfter high-risk or moderate-risk exposure to Covid-19, hydroxychloroquine did not prevent illness compatible with Covid-19 or confirmed infection when used as postexposure prophylaxis within 4 days after exposure. (Funded by David Baszucki and Jan Ellison Baszucki and others; ClinicalTrials.gov number, NCT04308668.)
Methods|Trial Design and Oversight We conducted a randomized, double-blind, placebo-controlled trial to evaluate postexposure prophylaxis with hydroxychloroquine after exposure to Covid-19.12 We randomly assigned participants in a 1:1 ratio to receive either hydroxychloroquine or placebo. Participants had known exposure (by participant report) to a person with laboratory-confirmed Covid-19, whether as a household contact, a health care worker, or a person with other occupational exposures. Trial enrollment began on March 17, 2020, with an eligibility threshold to enroll within 3 days after exposure; the objective was to intervene before the median incubation period of 5 to 6 days. Because of limited access to prompt testing, health care workers could initially be enrolled on the basis of presumptive high-risk exposure to patients with pending tests; however, on March 23, eligibility was changed to exposure to a person with a positive polymerase-chain-reaction (PCR) assay for SARS-CoV-2, with the eligibility window extended to within 4 days after exposure. This trial was approved by the institutional review board at the University of Minnesota and conducted under a Food and Drug Administration Investigational New Drug application. In Canada, the trial was approved by Health Canada; ethics approvals were obtained from the Research Institute of the McGill University Health Centre, the University of Manitoba, and the University of Alberta. Participants We included participants who had household or occupational exposure to a person with confirmed Covid-19 at a distance of less than 6 ft for more than 10 minutes while wearing neither a face mask nor an eye shield (high-risk exposure) or while wearing a face mask but no eye shield (moderate-risk exposure). Participants were excluded if they were younger than 18 years of age, were hospitalized, or met other exclusion criteria (see the Supplementary Appendix, available with the full text of this article at NEJM.org). Persons with symptoms of Covid-19 or with PCR-proven SARS-CoV-2 infection were excluded from this prevention trial but were separately enrolled in a companion clinical trial to treat early infection. Setting Recruitment was performed primarily with the use of social media outreach as well as traditional media platforms. Participants were enrolled nationwide in the United States and in the Canadian provinces of Quebec, Manitoba, and Alberta. Participants enrolled themselves through a secure Internet-based survey using the Research Electronic Data Capture (REDCap) system.13 After participants read the consent form, their comprehension of its contents was assessed; participants provided a digitally captured signature to indicate informed consent. We sent follow-up e-mail surveys on days 1, 5, 10, and 14. A survey at 4 to 6 weeks asked about any follow-up testing, illness, or hospitalizations. Participants who did not respond to follow-up surveys received text messages, e-mails, telephone calls, or a combination of these to ascertain their outcomes. When these methods were unsuccessful, the emergency contact provided by the enrollee was contacted to determine the participant’s illness and vital status. When all communication methods were exhausted, Internet searches for obituaries were performed to ascertain vital status. Interventions Randomization occurred at research pharmacies in Minneapolis and Montreal. The trial statisticians generated a permuted-block randomization sequence using variably sized blocks of 2, 4, or 8, with stratification according to country. A research pharmacist sequentially assigned participants. The assignments were concealed from investigators and participants; only pharmacies had access to the randomization sequence. Hydroxychloroquine sulfate or placebo was dispensed and shipped overnight to participants by commercial courier. The dosing regimen for hydroxychloroquine was 800 mg (4 tablets) once, then 600 mg (3 tablets) 6 to 8 hours later, then 600 mg (3 tablets) daily for 4 more days for a total course of 5 days (19 tablets total). If participants had gastrointestinal upset, they were advised to divide the daily dose into two or three doses. We chose this hydroxychloroquine dosing regimen on the basis of pharmacokinetic simulations to achieve plasma concentrations above the SARS-CoV-2 in vitro half maximal effective concentration for 14 days.14 Placebo folate tablets, which were similar in appearance to the hydroxychloroquine tablets, were prescribed as an identical regimen for the control group. Rising Pharmaceuticals provided a donation of hydroxychloroquine, and some hydroxychloroquine was purchased. Outcomes The primary outcome was prespecified as symptomatic illness confirmed by a positive molecular assay or, if testing was unavailable, Covid-19–related symptoms. We assumed that health care workers would have access to Covid-19 testing if symptomatic; however, access to testing was limited throughout the trial period. Covid-19–related symptoms were based on U.S. Council for State and Territorial Epidemiologists criteria for confirmed cases (positivity for SARS-Cov-2 on PCR assay), probable cases (the presence of cough, shortness of breath, or difficulty breathing, or the presence of two or more symptoms of fever, chills, rigors, myalgia, headache, sore throat, and new olfactory and taste disorders), and possible cases (the presence of one or more compatible symptoms, which could include diarrhea).15 All the participants had epidemiologic linkage,15 per trial eligibility criteria. Four infectious disease physicians who were unaware of the trial-group assignments reviewed symptomatic participants to generate a consensus with respect to whether their condition met the case definition.15 Secondary outcomes included the incidence of hospitalization for Covid-19 or death, the incidence of PCR-confirmed SARS-CoV-2 infection, the incidence of Covid-19 symptoms, the incidence of discontinuation of the trial intervention owing to any cause, and the severity of symptoms (if any) at days 5 and 14 according to a visual analogue scale (scores ranged from 0 [no symptoms] to 10 [severe symptoms]). Data on adverse events were also collected with directed questioning for common side effects along with open-ended free text. Outcome data were measured within 14 days after trial enrollment. Outcome data including PCR testing results, possible Covid-19–related symptoms, adherence to the trial intervention, side effects, and hospitalizations were all collected through participant report. Details of trial conduct are provided in the protocol and statistical analysis plan, available at NEJM.org. Sample Size We anticipated that illness compatible with Covid-19 would develop in 10% of close contacts exposed to Covid-19.9 Using Fisher’s exact method with a 50% relative effect size to reduce new symptomatic infections, a two-sided alpha of 0.05, and 90% power, we estimated that 621 persons would need to be enrolled in each group. With a pragmatic, Internet-based, self-referral recruitment strategy, we planned for a 20% incidence of attrition by increasing the sample size to 750 participants per group. We specified a priori that participants who were already symptomatic on day 1 before receiving hydroxychloroquine or placebo would be excluded from the prophylaxis trial and would instead be separately enrolled in the companion symptomatic treatment trial. Because the estimates for both incident symptomatic Covid-19 after an exposure and loss to follow-up were relatively unknown in early March 2020,9 the protocol prespecified a sample-size reestimation at the second interim analysis. This reestimation, which used the incidence of new infections in the placebo group and the observed percentage of participants lost to follow-up, was aimed at maintaining the ability to detect an effect size of a 50% relative reduction in new symptomatic infections. Interim Analyses An independent data and safety monitoring board externally reviewed the data after 25% and 50% of the participants had completed 14 days of follow-up. Stopping guidelines were provided to the data and safety monitoring board with the use of a Lan–DeMets spending function analogue of the O’Brien–Fleming boundaries for the primary outcome. A conditional power analysis was performed at the second and third interim analysis with the option of early stopping for futility. At the second interim analysis on April 22, 2020, the sample size was reduced to 956 participants who could be evaluated with 90% power on the basis of the higher-than-expected event rate of infections in the control group. At the third interim analysis on May 6, the trial was halted on the basis of a conditional power of less than 1%, since it was deemed futile to continue. Statistical Analysis We assessed the incidence of Covid-19 disease by day 14 with Fisher’s exact test. Secondary outcomes with respect to percentage of patients were also compared with Fisher’s exact test. Among participants in whom incident illness compatible with Covid-19 developed, we summarized the symptom severity score at day 14 with the median and interquartile range and assessed the distributions with a Kruskal–Wallis test. We conducted all analyses with SAS software, version 9.4 (SAS Institute), according to the intention-to-treat principle, with two-sided type I error with an alpha of 0.05. For participants with missing outcome data, we conducted a sensitivity analysis with their outcomes excluded or included as an event. Subgroups that were specified a priori included type of contact (household vs. health care), days from exposure to enrollment, age, and sex.
Results|Participants Figure 1. Figure 1. Screening and Randomization. Of the 821 participants who underwent randomization, 96 did not complete the day 14 follow-up survey, of whom 8 formally withdrew from the trial (4 in each group). Investigators confirmed the vital status and lack of infection in 19 participants (10 in the hydroxychloroquine group and 9 in the control group); 17 completed some follow-up surveys without symptoms before being lost to follow-up (13 in the hydroxychloroquine group and 4 in the control group). A total of 52 participants never completed any surveys after enrollment and did not respond to investigators e-mails, text messages, or telephone calls (23 in the hydroxychloroquine group and 29 in the control group). SARS-CoV-2 denotes severe acute respiratory syndrome coronavirus 2. Table 1. Table 1. Demographic and Clinical Characteristics of the Participants at Baseline. We recruited 821 asymptomatic adult participants who were randomly assigned to the hydroxychloroquine group (414 participants) or the placebo group (407 participants) (Figure 1). The demographic and clinical characteristics of the participants are provided in Table 1. The median age was 40 years (interquartile range, 33 to 50). Women accounted for 51.6% of the trial participants (424 of 821). A total of 27.4% of the participants (225 of 821) reported chronic health conditions, with hypertension being the most common (99 of 821 [12.1%]), followed by asthma (62 of 821 [7.6%]). Health care workers accounted for 66.4% of the participants (545 of 821), the majority being physicians or physician assistants (342 of 545 [62.8%]) and nurses or nursing assistants (128 of 545 [23.5%]). In the case of health care workers, exposure was predominantly from patients (418 of 545 [76.7%]) or ill coworkers (107 of 545 [19.6%]). Among the 29.8% of the participants (245 of 821) who enrolled as a household contact, the majority reported that their Covid-19 contact exposure was either a spouse or partner (114 of 245 [46.5%]) or a parent (43 of 245 [17.6%]). Overall, 87.6% of the participants (719 of 821) had high-risk exposures without eye shields and surgical masks or respirators. Of those, 365 received hydroxychloroquine and 354 received placebo. Approximately 60% of the participants reported not wearing any element of personal protective equipment during their Covid-19 exposure. Primary Outcome Table 2. Table 2. Outcomes of Hydroxychloroquine Therapy for Postexposure Prophylaxis against Covid-19. Figure 2. Figure 2. Cumulative Incidence of Illness Compatible with Coronavirus Disease (Covid-19). The cumulative incidence of illness compatible with Covid-19 was 11.8% in the hydroxychloroquine group (49 of 414 participants) and 14.3% in the placebo group (58 of 407) (P=0.35). The difference equates to a number needed to treat to prevent one infection of 42 persons (lower boundary of the 95% confidence interval for the number needed to treat to prevent one infection, 14 persons; upper boundary of the 95% confidence interval for the number needed to treat to harm 1 person, 50 persons). When we excluded participants who were lost to follow-up, who withdrew, or who were not fully adherent to the trial intervention, the results were similar. When we excluded 13 persons with possible Covid-19 cases who had only one symptom compatible with Covid-19 and no laboratory confirmation, the incidence of new Covid-19 still did not differ significantly between the two groups: 10.4% in the hydroxychloroquine group (43 of 414 participants) and 12.5% in the placebo group (51 of 407) (P=0.38). The vertical bars represent 95% confidence intervals. (Details on symptoms and the adjudication of cases are provided in the Supplementary Appendix.) Overall, new Covid-19 (either PCR-confirmed or symptomatically compatible) developed in 107 of 821 participants (13.0%) during the 14 days of follow-up (Table 2). The incidence of new illness compatible with Covid-19 did not differ significantly between those receiving hydroxychloroquine (49 of 414 [11.8%]) and those receiving placebo (58 of 407 [14.3%]) (P=0.35). The absolute difference was −2.4 percentage points (95% confidence interval, −7.0 to 2.2). Figure 2 shows the development of Covid-19 over time. Two hospitalizations were reported (one in each group). No arrhythmias or deaths occurred. There was no meaningful difference in effectiveness according to the time of starting postexposure prophylaxis or in any of the prespecified subgroups (Fig. S1 in the Supplementary Appendix). Overall, 10.7% of the participants (46 in the hydroxychloroquine group and 42 in the placebo group) did not complete the day 14 survey; among these participants, vital status was unknown for 36 in the hydroxychloroquine group and 33 in the control group. In sensitivity analyses, exclusion of these persons from the denominator or inclusion of them as having had an event did not affect the trial conclusions (Table S1). Of 113 persons in whom symptomatic illness developed, 16 had PCR-confirmed disease, 74 had illness that was compatible with probable Covid-19 per the U.S. case definition, 13 had possible Covid-19 with compatible symptoms and epidemiologic linkage, and 10 were adjudicated as not having Covid-19 on the basis of the symptom complex (Table S2). Four additional participants had positive PCR tests and were asymptomatic during the 14-day trial period; symptoms eventually developed in 3 of these participants. The median number of symptoms was 4 (interquartile range, 2 to 5) among participants with Covid-19. The most frequent symptoms were cough (44.9% of the 107 participants with Covid-19), fever (34.6%), shortness of breath (18.7%), fatigue (49.5%), sore throat (40.2%), myalgia (37.4%), and anosmia (23.4%). Among participants who were symptomatic at day 14, the median symptom-severity score (on a scale from 0 to 10, with higher scores indicating greater severity) was 2.8 (interquartile range, 1.6 to 5.0) in those receiving hydroxychloroquine and 2.7 (interquartile range, 1.4 to 4.8) in those receiving placebo (P=0.34). Adherence and Safety Table 3. Table 3. Participant-Reported Adherence and Side Effects. Adherence among the trial participants was moderate. Full adherence to the trial intervention differed according to trial group, with 75.4% of participants in the hydroxychloroquine group (312 of 414) and 82.6% of those in the placebo group (336 of 407) having taken all 19 prescribed tablets over a period of 5 days (P=0.01). The most common reason that participants stopped taking the assigned hydroxychloroquine or placebo was side effects (17 participants in the hydroxychloroquine group and 8 in the placebo group). Side effects were more frequent with hydroxychloroquine than with placebo (Table 3). Among the participants who took any hydroxychloroquine, 40.1% (140 of 349) reported a side effect by day 5, as compared with 16.8% (59 of 351) receiving placebo (P<0.001). Nausea, loose stools, and abdominal discomfort were the most common side effects. There were no serious intervention-related adverse reactions or cardiac arrhythmias. On day 14, we assessed how well the masking of the trial interventions was maintained. Of the 344 participants in the hydroxychloroquine group who completed the day 14 survey question, 160 (46.5%) correctly identified that they received hydroxychloroquine, 151 (43.9%) were unsure, and 33 (10%) believed that they received placebo. Of the 353 participants in the control group who completed the day 14 survey question, 126 (35.7%) correctly identified that they received placebo, 168 (47.6%) were unsure, and 59 (16.7%) believed that they received hydroxychloroquine. Participants who reported any side effect (regardless of trial group) at day 5 were 3.7 times as likely to believe that they received hydroxychloroquine as participants who did not report side effects (122 of 179 participants [68.2%] reporting side effects and 94 of 504 participants [18.7%] not reporting side effects; P<0.001). In the absence of side effects, blinding was well maintained.
Abstract|College students encounter unique challenges leading to poor mental health in the wake of the COVID-19 outbreak. Before the pandemic started, one in five college students experienced one or more diagnosable mental disorders worldwide. The fact that the COVID-19 pandemic affects collegiate mental health underscores the urgent need to understand these challenges and concerns in order to inform the development of courses of action and public health messaging that can better support college students in this crisis. This article provides recommendations that prepare higher education institutions and health professionals for addressing collegiate mental health needs and challenges posed by COVID-19.Keywords: COVID-19, Collegiate mental health, Higher education
1. The impact of COVID-19 on collegiate mental health|Many universities decided to suspend in-person classes and evacuate students in responding to the intensifying concerns surrounding COVID-19. This action can lead to negative psychological consequences among college students. For example, college students often experience compounded negative emotions during the school “closure” (Van Bortel et al., 2016). Some students who find the campus homelike and welcoming harbor intense feelings such as frustration, anxiety, and betrayal. Some may struggle with loneliness and isolation while sheltering in place because of disconnections from friends and partners. For those who receive counseling services on campus, they can no longer access counseling services, which exacerbates their psychological symptoms and increase some students’ risk for suicide and substance abuse.College students experience distress contributed by the uncertainty and abrupt disruption of the semester in addition to the anxiety caused by school closure. As more universities transitioning to remote learning after the spring break, some students suffer from poor mental health due to the disruption of academic routine (Agnew et al., 2019). Many students have to cease their research projects and internships when universities evacuated them from campus. Moreover, disruptions of their research projects and internships jeopardize their program of study, delay their graduation, and undermine their competitiveness on the job market, which in turn fuels anxiety among college students. They may also struggle with the cost of returning home and managing belongings.Many college students have lost their on-campus jobs due to the evacuation, and the pending issue of room and board fees can aggravate their financial hardship and mental health outcomes. They also have concerns and fears of infection and transmission of COVID-19 to their family members when they return home. Given that youth can be asymptomatic carriers (Pan et al., 2020), students may be worried about putting their elder family members at increased risk for infection with severe complications from COVID-19. The fact that the COVID-19 pandemic affects collegiate mental health only underscores the urgent need to understand these challenges and concerns in order to inform the development of courses of action and public health messaging that will support college students during this difficult time.
3. Recommendations|Notwithstanding that some universities have responded to the public health emergency, universities should continue to develop courses of action and public health messaging to better address collegiate mental health issues caused by the disruptions of education and career trajectory. First, in addition to remote education, student advising should continue and transition to telecommunication (e.g., phone call, online meeting) in order to provide academic support for students. Faculty and staff should consider offering virtual office hours to students, and they need to work together to maintain the connection and help students process and address academic concerns caused by the disruption of the semester. Second, for students whose internships or research projects were affected by the pandemic, internship site supervisors and research advisors should actively engage in helping students seek alternative plans, enabling them to work from home to maximize internship and research experiences. Third, universities should work on innovative methods to support students to move research projects and capstones forward so that students can fulfil graduation requirements; meanwhile, university career centers should switch to virtue services, continuing to facilitate career development for college students.Of importance is that university counseling centers should set up options to continue to provide college students with counseling services at a distance (i.e., telemental health counseling) within the constraints of the pandemic outbreak. Telemental health has been found effective in treating anxiety and depressive symptoms (Brenes et al., 2015), and implementing telemental health will facilitate the delivery of counseling services to address students’ pressing mental health concerns (Dorsey and Topol, 2020). University counseling centers can also provide options for students to join online support groups that enable them to share common concerns and receive social support (Rollman et al., 2018). Further, university counseling centers and other departments should rally to develop and pass public health messaging onto students, sharing coping resources, and encouraging them to take action to protect their mental health.
4. Conclusion|COVID-19 and its accompanying effects will continue impacting collegiate mental health and wellbeing profoundly; meanwhile, mental health serves a crucial role in combating the epidemic. It is thus imperative for universities to build awareness of students’ mental health needs and concerns, and to empower their students to seek help and support during this biological disaster. College students should tailor coping strategies to meet their specific needs and promote their psychological resilience. Considerable efforts made by universities should be dedicated to helping students thrive in this crisis. With the experience attained supporting students in this pandemic, universities will be well positioned to help college students stay well in mind, body, and spirit during other challenging times.
Abstract|Many filamentous cyanobacteria grow as multicellular organisms that show a developmental pattern of single nitrogen-fixing heterocysts separated by approximately 10 vegetative cells. Overexpression of a 54-base-pair gene, patS, blocked heterocyst differentiation in Anabaena sp. strain PCC 7120. A patS null mutant showed an increased frequency of heterocysts and an abnormal pattern. Expression of a patS-gfp reporter was localized in developing proheterocysts. The addition of a synthetic peptide corresponding to the last five amino acids of PatS inhibited heterocyst development. PatS appears to control heterocyst pattern formation through intercellular signaling mechanisms.
Abstract|Retrotransposition of processed mRNAs is a common source of novel sequence acquired during the evolution of genomes. Although the vast majority of retroposed gene copies, or retrogenes, rapidly accumulate debilitating mutations that disrupt the reading frame, a small percentage become new genes that encode functional proteins. By using a multibreed association analysis in the domestic dog, we demonstrate that expression of a recently acquired retrogene encoding fibroblast growth factor 4 (fgf4) is strongly associated with chondrodysplasia, a short-legged phenotype that defines at least 19 dog breeds including dachshund, corgi, and basset hound. These results illustrate the important role of a single evolutionary event in constraining and directing phenotypic diversity in the domestic dog.
Methods|An ongoing outbreak of pneumonia associated with the severe acute respiratory coronavirus 2 (SARS-CoV-2) started in December, 2019, in Wuhan, China. Information about critically ill patients with SARS-CoV-2 infection is scarce. We aimed to describe the clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia.
Challenges in treating COVID-19|Coronavirus disease (COVID-19), which appeared in December 2019, presents a global challenge, particularly in the rapid increase of critically ill patients with pneumonia and absence of definitive treatment. To date, over 81,000 cases have been confirmed, with over 2700 deaths. The mortality appears to be around 2%; early published data indicate 25.9% with SARS-CoV-2 pneumonia required ICU admission and 20.1% developed acute respiratory distress syndrome [1].There is presently no vaccine or specific anti-viral drug regime used to treat critically ill patients. The management of patients mainly focuses on the provision of supportive care, e.g., oxygenation, ventilation, and fluid management. Combination treatment of low-dose systematic corticosteroids and anti-virals and atomization inhalation of interferon have been encouraged as part of critical COVID-19 management [2]. Other reported therapeutic agents that are used for the treatment of seriously ill patients have been noted in Table 1. Table 1Potential treatment options of COVID-19ClassesPotential treatment optionsReferenceAnti-viral> 85% of patients received anti-viral agents, including oseltamivir (75 mg every 12 h orally), ganciclovir (0.25 g every 12 h intravenously), and lopinavir/ritonavir tablets (400/100 mg twice daily). Remdesivir is currently under trials at more than ten medical institutions in Wuhan and has been known to prevent MERS-CoV.[1]Anti-malarialAn old anti-malarial, chloroquine phosphate, has been effective in inhibiting the exacerbation of pneumonia due to its anti-viral and anti-inflammatory activities.[3]Herbal treatmentsThere was widespread use of Traditional Chinese Medicine during the last SARS-COV outbreak and it is currently being used in China. The five most commonly used herbs were Astragali Radix (Huangqi), Glycyrrhizae Radix Et Rhizoma (Gancao), Saposhnikoviae Radix (Fangfeng), Atractylodis Macrocephalae Rhizoma (Baizhu), and Lonicerae Japonicae Flo.[4]Open in a separate window
Convalescent plasma: one of the forgotten immunologically based strategies|Passive immunization has been successfully used to treat infectious diseases. A meta-analysis demonstrated a significant reduction in mortality and viral load in studies using convalescent plasma for the treatment of severe acute viral respiratory infections, including those caused by related coronaviruses (SARS-CoV and MERS-CoV) [5]. Serious adverse events were not reported. Eighty SARS patients were treated with convalescent plasma during the last major outbreak. A significantly better outcome was obtained with earlier transfusion (before day 14), and no immediate adverse events were observed.A feasibility intervention study of convalescent plasma for MERS-CoV infection treatment failed to identify sufficient high-titer plasma from patients with confirmed/suspected MERS, their close family members, or healthcare workers exposed to MERS (n = 12 reactive ELISA/443 serum tested). Two fresh-frozen plasma units (250–350 mL/unit) would be required for each enrolled MERS patient ({"type":"clinical-trial","attrs":{"text":"NCT02190799","term_id":"NCT02190799"}}NCT02190799). Encouragingly, anti-MERS CoV titers measured by ELISA correlated with microneutralization (MN) assays [6].There are currently over 30,000 recovered COVID-19 patients, who could present a valuable resource of convalescent plasma for health care providers. China National Biotec Group Co has claimed that 10 seriously ill patients receiving this immunoglobulin therapy demonstrated improved oxygenation and reduced inflammation and viral load [7]. High-titer specific antibodies should be able to bind to SARS-CoV-2 and neutralize the viral particles, block access to uninfected cells, and activate potent effector mechanisms (e.g., complement activation and phagocytosis).
Conclusion|In the absence of definitive management protocols, many treatment regimes have been explored in the treatment of COVID-19. Some of these treatments may have been tried out of desperation, and among these, some show initial promise. However, it is too early to see any published results of rigorous clinical trials.Using the serum of recovered patients is a tried and tested approach, and trials are underway to study its effectiveness. This treatment appears to be helpful in the short term until definitive and effective treatments are found.
Abstract|Background: Remdesivir, a nucleotide analogue prodrug that inhibits viral RNA polymerases, has shown in vitro activity against SARS-CoV-2. Methods: We provided remdesivir on a compassionate-use basis to patients hospitalized with Covid-19, the illness caused by infection with SARS-CoV-2. Patients were those with confirmed SARS-CoV-2 infection who had an oxygen saturation of 94% or less while they were breathing ambient air or who were receiving oxygen support. Patients received a 10-day course of remdesivir, consisting of 200 mg administered intravenously on day 1, followed by 100 mg daily for the remaining 9 days of treatment. This report is based on data from patients who received remdesivir during the period from January 25, 2020, through March 7, 2020, and have clinical data for at least 1 subsequent day. Results: Of the 61 patients who received at least one dose of remdesivir, data from 8 could not be analyzed (including 7 patients with no post-treatment data and 1 with a dosing error). Of the 53 patients whose data were analyzed, 22 were in the United States, 22 in Europe or Canada, and 9 in Japan. At baseline, 30 patients (57%) were receiving mechanical ventilation and 4 (8%) were receiving extracorporeal membrane oxygenation. During a median follow-up of 18 days, 36 patients (68%) had an improvement in oxygen-support class, including 17 of 30 patients (57%) receiving mechanical ventilation who were extubated. A total of 25 patients (47%) were discharged, and 7 patients (13%) died; mortality was 18% (6 of 34) among patients receiving invasive ventilation and 5% (1 of 19) among those not receiving invasive ventilation. Conclusions: In this cohort of patients hospitalized for severe Covid-19 who were treated with compassionate-use remdesivir, clinical improvement was observed in 36 of 53 patients (68%). Measurement of efficacy will require ongoing randomized, placebo-controlled trials of remdesivir therapy. (Funded by Gilead Sciences.).
Abstract|The emergence of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2; previously provisionally named 2019 novel coronavirus or 2019-nCoV) disease (COVID-19) in China at the end of 2019 has caused a large global outbreak and is a major public health issue. As of 11 February 2020, data from the World Health Organization (WHO) have shown that more than 43 000 confirmed cases have been identified in 28 countries/regions, with >99% of cases being detected in China. On 30 January 2020, the WHO declared COVID-19 as the sixth public health emergency of international concern. SARS-CoV-2 is closely related to two bat-derived severe acute respiratory syndrome-like coronaviruses, bat-SL-CoVZC45 and bat-SL-CoVZXC21. It is spread by human-to-human transmission via droplets or direct contact, and infection has been estimated to have mean incubation period of 6.4 days and a basic reproduction number of 2.24-3.58. Among patients with pneumonia caused by SARS-CoV-2 (novel coronavirus pneumonia or Wuhan pneumonia), fever was the most common symptom, followed by cough. Bilateral lung involvement with ground-glass opacity was the most common finding from computed tomography images of the chest. The one case of SARS-CoV-2 pneumonia in the USA is responding well to remdesivir, which is now undergoing a clinical trial in China. Currently, controlling infection to prevent the spread of SARS-CoV-2 is the primary intervention being used. However, public health authorities should keep monitoring the situation closely, as the more we can learn about this novel virus and its associated outbreak, the better we can respond.
Abstract|This article introduces a new and unobtrusive wearable monitoring device based on electrodermal activity (EDA) to be used in health-related computing systems. This paper introduces the description of the wearable device capable of acquiring the EDA of a subject in order to detect his/her calm/distress condition from the acquired physiological signals. The lightweight wearable device is placed in the wrist of the subject to allow continuous physiological measurements. With the aim of validating the correct operation of the wearable EDA device, pictures from the International Affective Picture System are used in a control experiment involving fifty participants. The collected signals are processed, features are extracted and a statistical analysis is performed on the calm/distress condition classification. The results show that the wearable device solely based on EDA signal processing reports around 89% accuracy when distinguishing calm condition from distress condition.Keywords: wearable, electrodermal activity, arousal, valence, distress, calmness
1. Introduction|Early mental stress detection can prevent many health problems related to distress (negative stress). Therefore, there is an urgent need to create new technologies to monitor the physical and mental health of people during their daily life. Fortunately, some efforts are being carried out towards monitoring and regulating people’s arousal state [1,2,3], which is indicative of stress or mental illness [4,5,6,7]. Thus, the lack of human–machine interfaces in interpreting the subjects’ emotional states is being faced with the important aim of understanding and managing personal well-being regarding mental health state.In this regard, a number of physiological features have been widely used in the literature [8,9]. These features use to measure alterations in the central nervous system [10,11,12]. One of these physiological markers corresponds to electrodermal activity (EDA). The utilization of EDA is excellent in assessing the arousal level, as it is able to quantify changes in the sympathetic nervous system. In order to continuously measure the EDA signal from the subjects, wearable sensors are the most appropriate in real mobility situations, given their performance in providing detailed user-specific information. Moreover, wearable sensors are greatly valued due to their light weight and their wireless communication capacities with either a computer or other wearable sensors [13].In this sense, there are several low-cost solutions for wearable long-term EDA monitoring. For instance, the similarity of signals between a prototype of the wearable Moodmetric EDA Ring is compared with a laboratory-grade skin conductance sensor in a psycho-physiological experiment [14]. Recently, a pilot study of EDA measurements conducted during a trade fair has been presented [15].This paper describes and assesses the performance of a new wearable electrodermal activity-based device in classifying distress or calm conditions. Section 2 introduces the hardware description of the wearable. Section 3 introduces an experiment that has been designed in order to assess the validity of the proposed wearable. The experimental design and the description of the study population are presented in Section 3.1 and Section 3.2, respectively. Then, in Section 4, the segmentation and processing of the several signals (Section 4.1), as well as the feature extraction process (see Section 4.2) and the statistical analysis of the classification capabilities (see Section 4.3), are described in extensive. Afterwards, the results are offered in Section 5, and Section 6 includes the most outstanding discussion and conclusions related to this work.
4. Methodology|After the experimentation, all signals are segmented and processed according to the time duration of the stimuli. Next, the most significant time, morphological and frequency features are extracted. Finally, diverse statistical analysis and classification techniques are applied with the objective of maximizing the performance.4.1. Signal ProcessingThe EDA raw signals are acquired from the wearable at a sampling rate of 10 Hz and a 12-bit resolution. These specifications are chosen to remain the EDA shape unaltered and to prevent distortions [19]. EDA signals result from the superposition of two different components, SCR and SCL. The sympathetic nervous system fires the sudomotor nerve, provoking the phasic response. Traditionally, the SCR intensity has been quantified after each elicited stimulus by detecting peaks directly on the EDA signal [19]. Next, the difference between a found peak and its preceding local minimum is assessed.However, depending on the stimuli, it is frequent that SCRs appear as bursts, such that an EDA signal is represented as a sequence of consecutive SCRs. In this case, the SCR boundaries remain masked by the preceding response. Indeed, SCRs may occur at the rise or decay of existing stimuli, making it very difficult to determine if these responses correspond to a new stimulus or are part of previous events. In this regard, some works have defined different strategies for all possible overlapping cases [30]. Nevertheless, the through-to-peak standard method may result in an underestimation of the amplitude of consecutive SCRs, depending on the closeness among responses [31]. Recently, new studies have addressed this problem by decomposing the EDA signal into its two components by using a deconvolution operation [31]. Despite this approach requires more intensive signal processing, it has reported better performance than others that process directly the EDA raw data. Consequently, a similar methodology is applied in this work. Thus, considering the raw data from the wearable as y[n], a new 1.5 Hz cut-off low-pass FIR filter with order N = 32 is applied over this signal in order to decrease the possible noise produced in the different electronic stages. The resulting filtered signal y^[n] is calculated by means of a discrete convolution as:y^[n]=C0y[n]+C1y[n−1]+...+CNy[n−N]=∑i=0NCiy[n−i](3)Next, a deconvolution operation is carried out to separate SCR and SCL components. The deconvolution is an algorithm-based process used to reverse the effects of combining signals by finding the solution to the convolution equation, such that:y^[n]=(r×l)[n]=∑i=0Nr[n−i]l[i](4) being × the convolution operator in the time domain, y^[n] the filtered EDA signal, r[n] the required SCR and l[n] the SCL components. It is worth noting that l[n] corresponds to the transfer function in Equation (4), such that, if l[n] is known or estimated, a deterministic deconvolution could be used to compute the desired component r[n].In this regard, three different assumptions have been considered in this work. Firstly, the exact moments when the stimulus (picture) starts have been recorded as events, as can be observed in Figure 3b. Secondly, notice that the SCR component takes a while since the stimulus is fired until the sympathetic system reacts through filling the sweat glands, thus increasing the skin conductivity. Indeed, the exact time occurrence of SCR varies depending on skin type and genetic aspects [22]. Furthermore, the SCR duration varies subject to the stimulus’ nature and the participant’s reaction against such stimulus. Consequently, a fixed temporal window (5 s) is used as the time segment where the SCR response may occur, following the recommendations of a previous work [17]. Thus, a period from + 1 to + 6 s after the onset of each stimulus is considered. In third place, l[n] corresponds to y^[n] when no stimulus is elicited [19].Open in a separate windowFigure 3Different stages in EDA signal processing. (a) Raw EDA signal before filtering. (b) Raw EDA signal after low-pass filtering and stimuli onset. (c) Estimation of EDA baseline using a cubic spline approximation. (d) Skin conductivity response (SCR) obtained after the convolution process.Considering the aforementioned assumptions, the time intervals occurring before and after each phasic impulse are used to estimate the SCL gaps between the different phasic impulses. In this work, a cubic spline fit is used to approximate l[n] at the gaps produced in the SCR temporal window, as you may observe in Figure 3c. Once l[n] is known, r[n] can be computed by following the inverse process defined in Equation (4). Nevertheless, given the complexity of this operation in time, it is preferable to work in the frequency domain, where convolution and deconvolution become in simple multiplications and divisions. Thus, y^[n] and l[n] are transformed into the frequency domain by using the Fourier Fast Transforms (FFT), such that r[n] can be calculated as:r[n]F=y^[n]Fl[n]F(5) being y^[n]F, r[n]F and l[n]F the FFTs of EDA signal, and SCR and SCL components, respectively. The original r[n] component alone is estimated by computing the inverse Fourier transform over r[n]F. As it can be observed in Figure 3d, r[n] corresponds to a signal with zero baseline, where each impulse reflects the activation of the sudomotor nerve.4.2. Feature SetsIn the present section, all the features are estimated. The characteristics related to time domain, frequency domain, statistics and morphological analysis are computed for each physiological variable. In this work, thirty six features are used, as you may observe in Table 1.Table 1Temporal, morphological and frequency features computed for EDA signals.AnalysisFeaturesTemporalMSC, SDSC, MASC, MISC, DRSC, FMSC, FDSC, SMSC, SDSCMorphologicalALSC, INSC, APSC, RMSC, ILSC, ELSC, KUSC, SKSC, KUSC, MOSCFrequencyF1SC, F2SC, F3SCOpen in a separate windowDifferent time-domain and frequency-domain markers are computed over the phasic component SCR to characterize the EDA signal. The SCL tonic component is out of scope of this study, since it uses to vary among different people due to physical and genetic aspects [19]. Firstly, a number of temporal metrics are computed over the SCR component. Thus, the mean (MSC), standard deviation (SDSC), maximum (MASC), minimum (MISC) and the dynamic range (DRSC), defined as the difference between MASC and MISC, are estimated. In order to highlight the sudden changes in the skin conductivity, the first and second derivative of SCR are also computed. Then, the mean (FMSC) and standard deviation (FDSC) of the first derivative and the mean (SMSC) and standard deviation (SDSC) of the second derivative are calculated.In addition, several morphological features are defined for SCR characterization. Thus, in order to identify the morphological alterations produced when SCRs are present in the EDA signal, the arc length (ALSC) is computed as:ALSC=∑n=2N1+(r[n]−r[n−1])2.(6)This parameter discerns between the presence or absence of SCR components, and it has been used previously in the morphological characterization of Gaussian responses [32]. Moreover, some parameters related to the SCR amplitude are also used to assess the activation of the sympathetic nervous system. Thus, the integral (INSC), normalized average power (APSC) and normalized root mean square (RMSC) of SCR are calculated as:INSC=∑n=1N|r[n]|,(7) APSC=1N∑n=1Nr[n]2,(8) RMSC=1N∑n=1Nr[n]2.(9)Furthermore, possible relationships between the amplitude and energy of the SCR signal with its arc length are studied. Thus, the area-perimeter (ILSC) and the energy-perimeter (ELSC) relationships are estimated as the ratio between INSC and RMSC with ALSC, respectively. Finally, high order skewness (SKSC) and kurtosis (KUSC) statistics, as well as the central moment (MOSC) are calculated on the SCR component. These metrics assess the symmetry and shape of a probability distribution, and can therefore be also considered as geometrical descriptors. Indeed, if an SCR is considered as a Gaussian distribution, SKSC indicates if the tail distribution is longer at the left or at the right relative to the normal distribution. Similarly, the SKSC shows the degree of peakedness or flatness of a distribution relative to the normal distribution.In regards to frequency aspects, the SCR component is transformed into the frequency domain by using a nonparametric FFT algorithm. Then, the spectral power in bandwidths 0.1 to 0.2 (F1SC), 0.2 to 0.3 (F2SC) and 0.3 to 0.4 (F3SC) Hz are estimated. These bands have been previously used in other studies [33].4.3. Statistical AnalysisShaphiro-Wilks and Levene tests have proved that the distributions are normal and homoscedastic for all the features studied. Consequently, the results are expressed in terms of the mean ± standard deviation for all the samples belonging to a same class. The statistical differences between both classes, calm and distress, are assessed by a one-way ANOVA test. A value of statistical significance ρ < 0.05 has been considered as significant.Moreover, a 10-fold stratified cross-validation is used to assess the discriminant ability of each feature. This kind of cross-validation allows to obtain a highly reliable performance generalization of the metric under study [34]. Indeed, this approach makes use of all available data both for training and testing. This avoids the possibility of the classification results to be highly dependent on the choice of a given training-test segmentation. Thus, the database is firstly partitioned into 10 equally sized folds, rearranging the data to ensure that each fold is a good representative of the whole. Then, 10 training and validation iterations are performed, such that a fold of the data is held out for test, whereas the other ones are used for learning within each iteration. For each learning set, a receiver operating characteristic (ROC) curve is used to obtain the optimal discriminant threshold between calm and distress condition. The ROC curve is created by plotting the true positive (TP) rate against the false positive (FP) rate at various threshold settings. Here, the TP rate (or sensitivity) is considered as the percentage of distress condition correctly classified. On the other hand, the FP rate (or one-specificity) corresponds to the rate of calm individuals improperly identified. The optimal threshold is selected as the value which provides the highest accuracy. Finally, the global precision is obtained by averaging this procedure 10 times.Additionally, the relationships among the different temporal, morphological and frequency features are analysed by means of decision trees. In brief, the algorithm starts taking into account all the input data, examining all the possible splits on each feature. Then, the split with the best optimization criterion, based on the Gini diversity index [35], is chosen. In this respect, the Gini index is commonly computed by using the Lorenz curve, and it is expressed mathematically as:G=aa+b(10) where G corresponds to the Gini index, a represents the area that lies between the line of equality and the Lorenz curve and b represents the area under the Lorenz curve. Thus, a node containing observations from just one group (pure node) has Gini index 0, while if the node contains observations from both groups (impure node), the Gini index is a positive number, ranging between 0 and 1. Finally, when the split has been executed, the aforementioned process is repeated recursively for the two child nodes using the remaining data. It is worth noting that some stopping rules are imposed to prevent tree overgrowth. Thus, the growth of every tree is always stopped when any node only contains samples from a group of subjects (pure node) or less than 20% of all samples.
5. Results|Table 2 shows the mean and standard deviation of the features calculated. Only those markers reporting statistical differences throughout a one-way ANOVA test are presented.Table 2Results obtained from skin conductance response (SCR). Mean and standard deviation values for emotional states of calm and distress, and statistical significance (ρ), for all parameters are presented.FeatureCalm ConditionDistress Condition ρ AcronymMean ± StdMean ± Std MSC5.5339 ± 4.222813.0193 ± 8.62011.03 × 10−5SDSC4.4618 ± 4.897612.5249 ± 9.03401.33 × 10−5MASC28.6079 ± 27.4469.4104 ± 48.03102.68 × 10−5DRSC28.5653 ± 27.466069.3719 ± 48.01452.67 × 10−5FDSC0.9932 ± 0.96652.2660 ± 1.67561.50 × 10−4ALSC1.4049 × 104 ± 99.68091.4153 × 104 ± 279.79890.0175INSC193.9833 ± 148.3517457.2628 ± 304.90611.11 × 10−5APSC4.6324 ± 9.218123.8873 ± 36.43450.0026RMSC7.3106 ± 6.347618.0970 ± 12.42651.20 × 10−5ILSC5.5067 ± 4.148012.8120 ± 8.20067.31 × 10−6ELSC0.0065 ± 0.01290.0330 ± 0.04840.002SKSC1.8838 ± 1.18823.1146 ± 0.71590.0031MOSC2.2337 ± 5.069411.7973 ± 19.79300.0057F1SC2.9219 ± 5.438014.1513 ± 20.29890.0018F2SC0.1631 ± 0.29841.4143 ± 2.17678.99 × 10−4F3SC0.1391 ± 0.32311.2288 ± 2.39070.0076Open in a separate windowConcerning the study of the SCR component, 16 out of 22 features report significant differences. In this respect, higher values of skin conductivity are observed in the distress class regarding the calm class for all temporal parameters. From a statistical point of view, all temporal parameters achieve a comparable power discrimination between both classes. On the other hand, a number of morphological markers are defined and used in this analysis. Most of them show an important discriminatory power. It is worth noting that ILSC achieves the most remarkable trends, while ALSC reports the lowest significance. Just like in the temporal parameters, the average values of the morphological markers show an increasing pattern, such that higher skin conductivities are observed in distress compared to calm condition. Regarding the frequency parameters, all the markers report a power increase when the subjects are elicited with stressing stimuli. In this regard, F2SC experiences the highest difference between the classes, achieving a considerable statistical significance.In order to estimate a reliable and robust classification accuracy for each studied parameter, the stratified 10-fold cross-validation is run five times. Thus, the average values of sensitivity, specificity and accuracy, reported by each marker and for both training and test subsets iterations, are shown in Table 3.Table 3Sensitivity (Se), specificity (Sp) and accuracy (Ac) for all the parameters under study and for training and test subsets.FeatureLearningTestAcronymSe (%)Sp (%)Ac (%)Se (%)Sp (%)Ac (%)MSC75.9583.1079.5269.0076.7872.95SDSC85.0678.9582.4378.5776.0777.38MASC81.4576.3578.9174.0772.2873.34DRSC82.9275.4279.1876.6471.0073.62FDSC74.5880.4377.5069.2174.8571.81ALSC72.1576.8674.5270.9275.6473.19APSC84.9879.8882.4379.9278.7179.28RMSC85.9278.9282.4281.5778.8579.94ILSC85.1279.7182.4281.7878.1479.99ELSC71.7873.1472.2563.0075.3569.16SKSC77.4481.0879.2973.2875.7174.53MOSC86.1978.7582.5075.2875.2175.18F1SC77.1577.9477.5774.8575.8575.38F2SC85.2985.9385.6184.9278.5081.61F3SC85.1279.7182.4280.3576.4278.30Open in a separate windowThe classification results are in agreement with the discriminatory power obtained previously. Thus, all the temporal parameters computed over SCR component achieve a similar global accuracy, ranging from 71% to 73%. Similarly, morphological parameters computed over SCR component reach similar performance with global accuracy varying between 73% and 75%. Nevertheless, the energetic parameters APSC and RMSC show a very notable classification accuracy, reaching a correctness of 79.28% and 79.94%, respectively. In agreement with the discriminatory power shown before, the arc length (ALSC) achieves the worst performance in the classification. On the contrary, the area-perimeter marker ILSC reports the highest global accuracy among the morphological features, achieving a precision of 79.99%. Finally, frequency markers show a very notable performance, where F2SC reaches the highest global accuracy among all the studied parameters. It is important to remark that almost all markers show a good balance between sensitivity and specificity. Nevertheless, some remarkable parameters state a slight increase in sensibility, i.e. an increase in the ability of detecting distress condition. Hence, RMSC, ILSC, F2SC and F3SC report sensibility values higher than 80%.Considering this context, a series of tree-based classification models are programmed in order to study the possible relationships among the different parameters. Here we present the tree-based discriminant model that reaches the highest performance. This model is constructed by exclusively considering temporal and morphological features. In this case, the frequency parameters are excluded, thinking in a lighter algorithmic model only focused in analyzing the time domain. In this model, a threshold value of 4.4830 on SDSC parameter in this model serves to divide into two subgroups. Then, one of the subgroups is assessed by means of SKSC parameter, taking value 3.1102 as classification threshold. This tree-based model achieves a sensitivity, specificity and accuracy of 93.9%, 85.36% and 89.18%, respectively. In this regard, the model improves the global correctness more than 8%, regarding the performance of the best single parameter F2SC. It is also important to highlight that sensitivity, that is, the ability to discriminate distress condition, improves over 9%.
6. Conclusions|A recent paper [36] says that “Stress is a very complex subject and measuring stress is not an easy task. There are many markers that could be used, many algorithms that could be applied, and many forms of stress which could be observed”. Moreover, an important number of works can be found in the literature based in distress detection. Although comparison among works should always be considered with caution, since different ways to provoke the stress may trigger several cognitive processes [22], it is worth noting that the present study has achieved a comparable performance to other systems aimed at classifying calm and distress emotional states. Thus, while this work has reported a global accuracy around 89% when classifying calm and distress condition, other works based on electrodermal activity as reported in very interesting reviews on wearable sensors for remote health monitoring [37,38,39], state stress detection rates ranging from 75% to 95% [40,41,42,43]. Nevertheless, complex classifiers have been used to improve their performance.For instance, an approach throws a precision of 75% using EDA after classifying with k-means, Gaussian mixture models, SVM and decision trees [44].On the other hand, it is important to highlight that these and/or other approaches have used an important number of features from more than one physiological sensor to calculate the stress degree. For instance, electroencephalogram, electrocardiogram and facial cameras complement EDA signals to reach an accuracy of 68% [45]. The classification is performed by means of support vector machines (SVM) and naive Bayes classifier. In another approach [46], heart rate variability (HRV) supplements EDA to obtain 78% of precision, also using SVM classifiers. Two other papers using EDA and HRV reach an accuracy of 97.3% [10] and 80% [47], respectively. However, in the first article [10] a driving task is analysed with a series of sensors: hand and foot EDA, together with three other physiological measurements, namely HRV, Trapezius muscle electromyography and respiration. Moreover, the other work [47] uses EDA, blood pressure, HRV, eye gaze and pupil dilation. Another work with three physiological measurements [48], which are HRV, skin temperature and EDA, gets a precision of 91.2%. In another work [49], a portable embedded device to measure the accumulated stress level is designed. This device uses EDA and HRV with electrocardiograph or photoplethysmograph signals to obtain an accurate stress level.On the contrary, our proposal uses only features from SRC to achieve a high performance that is comparable to the most remarkable works. In this sense, the most outstanding aspect of our contribution is the development of the necessary hardware, signal processing and classification model to deploy a wearable device with a high ability to discriminate between the two considered states. The simplicity of the classification model and the lightness of the signal processing approach enables this device to work in real-time and long-term. Another relevant aspect is that almost all the features computed on the SCR component show some degree of relevance in the classification.For the sake of comparability and reproducibility of the experiment described in this study, a DC-EXM method has been chosen for the acquisition of EDA signals. Indeed, most of the works using EDA, and found in scientific literature, have been performed by means of this methodology [19]. Although ESM approaches may provide some advantages over the EXM methods, as, for instance, no need of additional amplification and coupling circuitry, the output signal generated is biphasic or even triphasic, which difficult considerably the interpretation and further processing of the data [19]. From a point of view of electronic design, many efforts have been made to standardize the use of constant voltage versus constant current sources when using DC-EXM methodology. Despite constant voltage directly providing conductance values, there is no consensus in the literature about its generalized use, because the conductance value can be obtained easily with constant current sources if SCL is also measured. In this aspect, the design of constant current sources are preferred because they are easier to design and present less tolerance than constant voltage variants.Finally, some limitations should be considered. First of all, the wearable prototype has been tested in laboratory conditions enrolling exclusively young subjects. Therefore, the results of this work can not be generalized directly to the entire population. With the aim of validating the results obtained, people of all ages will be enrolled in future studies. Furthermore, a sequence of 10 consecutive pictures with lengths of 6 s have been used as stimuli in this work. Considering such duration for visualization, the cognitive dimension of exploration of images during the 6 s may affect the outcomes. In this line, there is no consensus about which type of stimulus (image, sound, or video clip) and duration is the most adequate to elicit certain emotional states [22]. Moreover, it should be noted that the same order of condition high arousal-low valence—low arousal-high valence could affect, which could suppose an experimental bias.
Abstract|A key mutational process in cancer is structural variation, in which rearrangements delete, amplify or reorder genomic segments that range in size from kilobases to whole chromosomes1,2,3,4,5,6,7. Here we develop methods to group, classify and describe somatic structural variants, using data from the Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium of the International Cancer Genome Consortium (ICGC) and The Cancer Genome Atlas (TCGA), which aggregated whole-genome sequencing data from 2,658 cancers across 38 tumour types8. Sixteen signatures of structural variation emerged. Deletions have a multimodal size distribution, assort unevenly across tumour types and patients, are enriched in late-replicating regions and correlate with inversions. Tandem duplications also have a multimodal size distribution, but are enriched in early-replicating regions—as are unbalanced translocations. Replication-based mechanisms of rearrangement generate varied chromosomal structures with low-level copy-number gains and frequent inverted rearrangements. One prominent structure consists of 2–7 templates copied from distinct regions of the genome strung together within one locus. Such cycles of templated insertions correlate with tandem duplications, and—in liver cancer—frequently activate the telomerase gene TERT. A wide variety of rearrangement processes are active in cancer, which generate complex configurations of the genome upon which selection can act.
Discussion|We have described the patterns and signatures of structural variation in a large cohort of uniformly analysed cancer genomes. A major grouping of patterns in structural variants that emerges from our study is one in which extra copies of genomic templates are inserted during the rearrangement process. This includes simple events such as tandem duplications, as well as a range of more-complex events with duplications and triplications that are rearranged locally as well as inserted distantly. Our signature analysis grouped a large proportion of these more-complex events together with tandem duplications, which suggests that they represent a continuum of processes that share underlying properties. A replication-based mechanism has previously been proposed to explain local two-jumps4,23,24, in which stalled replication forks or other DNA lesions cause the DNA polymerase to switch templates and continue replication in a new location. Studies in experimental models are now revealing that a wide range of mechanisms and DNA lesions can result in templated insertions: these mechanisms include tandem duplications in BRCA1 deficiency10, translocations with templated insertions caused by dysregulated strand invasion38 and distant templated insertions in the absence of replication helicases39.Genomic instability in cancer is not a single phenomenon. Instead, many different mutational processes can act to restructure the genome and, in doing so, generate a notably flexible array of possible structures. Any given tumour draws on a subset of the available processes, shaped by the cell of origin, germline predisposition and other, unknown, factors: selection then does the rest, promoting the clone that has chanced on the structure that increases its potential for self-determination.
Methods|No statistical methods were used to predetermine sample size. The experiments were not randomized and investigators were not blinded to allocation during experiments and outcome assessment.A detailed description of the methods used in this paper and many additional results are described in Supplementary Information. Here, we summarize the key aspects of the analysis.Generation of the structural-variant call setThe final set of structural variants used in this Article was generated by the Technical Working Group of the PCAWG Consortium and is described in the main PCAWG paper8. In brief, four variant callers were used to identify somatically acquired structural variants from matched tumour and germline whole genome sequencing data: SvABA (Broad pipeline), DELLY (DKFZ pipeline), BRASS (Sanger pipeline) and dRanger (Broad pipeline). These were merged into a final call set using a graph-based algorithm to identify overlapping breakpoint junctions across algorithms. Detailed visual inspection of structural-variant calls suggested that a simple approach of accepting all structural-variant calls made by two or more of the four algorithms gave the best trade-off between sensitivity and specificity.Structural-variant clustering and annotationTo identify clusters of structural variants, we developed a method for grouping structural variants into clusters and footprints to allow structural and mechanistic inferences to be made systematically. In parallel, we processed the somatic copy-number data and merged it with structural-variant junctions to enable us produce rearrangement patterns from the generated structural-variant clusters and footprints. We produced normalized representations of structural-variant cluster patterns, which enable us to tabulate the number of different cluster and footprint patterns and analyse their features. Finally, we performed manual and simulation-assisted interpretation of the recurrently observed cluster and footprint patterns. The individual steps of the structural-variant classification pipeline are outlined below and detailed in the subsequent subsections: (1) computing the exact breakpoint coordinates from clipped reads; (2) removing redundant ‘segment-bypassing’ structural variants; (3) merging rearrangement breakpoints with copy-number data to yield structural-variant breakpoint-demarcated, normalized, absolute copy-number data; (4) clustering individual structural variants into structural-variant clusters and footprints; (5) heuristically refining structural-variant clusters and footprints; (6) filtering artefactual fold-back-type structural variants with insufficient support; (7) determining balanced overlapping breakpoints (this step is to distinguish very short templated insertions from mutually overlapping balanced breakpoints); and (8) computing rearrangement patterns and categories.Distribution of structural variants across the genomeWe divided the hg19 human reference genome (autosomes and chromosome X) into 3,036,315 pixels of 1 kb, and calculated a suite of metrics per pixel to summarize a variety of genome properties with potential relevance to the distribution of rearrangements, as listed in the Supplementary Information. Properties were matched as closely as possible to the tissue of origin for cancer samples from the PCAWG data. All other genome properties were held fixed across all tissues. To test for associations between structural-variant event classes and the library of genome properties, the genome property metrics were compared between real structural-variant positions (randomly choosing one side of each breakpoint junction to reduce dependence between observations) and one million uniform random positions from the callable genome space. To compare the tissue-specific properties, each random position was assigned a random tissue type, drawing from the observed tissue-type distribution in the structural-variant call set. For each genome property and each event class, the real observations were pooled amongst the random ones, and then rank-transformed and normalized on a scale from 0 to 1. Under the null hypothesis of no event-versus-property association, the ranks of the real observations would follow a uniform distribution. We tested this in each case with a Kolmogorov–Smirnov test then applied a Benjamini–Yekutieli correction for false-discovery rate across the entire suite of tests and set the threshold for significance reporting at 0.01.Structural-variant-signature analysisWe used two algorithms for extracting structural-variant signatures. Both used the same input files, comprising a matrix of counts per patient (across all patients) of structural-variant clusters falling into a number of mutually exclusive categories. These categories included the major classes of structural variants, with the more-common events (deletions, tandem duplications and inversions) split by size and/or replication timing. The two algorithms that were used for extracting the signatures were (1) a hierarchical Dirichlet process and (2) non-negative matrix factorization. Further details on the implementation of these algorithms are available in the Supplementary Information.Reporting summaryFurther information on research design is available in the Nature Research Reporting Summary linked to this paper.
Abstract|Recent announcements indicated, without sharing any distinct published set of results, that the corticosteroid dexamethasone may reduce mortality of severe COVID-19 patients only. The recent Coronavirus [severe acute respiratory syndrome (SARS)-CoV-2]-associated multiorgan disease, called COVID-19, has high morbidity and mortality due to autoimmune destruction of the lungs stemming from the release of a storm of pro-inflammatory cytokines. Defense against this Corona virus requires activated T cells and specific antibodies. Instead, cytokines are responsible for the serious sequelae of COVID-19 that damage the lungs. Dexamethasone is a synthetic corticosteroid approved by the FDA 1958 as a broad-spectrum immunosuppressor and it is about 30 times as active and with longer duration of action (2-3 days) than cortisone. Dexamethasone would limit the production of and damaging effect of the cytokines, but will also inhibit the protective function of T cells and block B cells from making antibodies, potentially leading to increased plasma viral load that will persist after a patient survives SARS. Moreover, dexamethasone would block macrophages from clearing secondary, nosocomial, infections. Hence, dexamethasone may be useful for the short-term in severe, intubated, COVID-19 patients, but could be outright dangerous during recovery since the virus will not only persist, but the body will be prevented from generating protective antibodies. Instead, a pulse of intravenous dexamethasone may be followed by administration of nebulized triamcinolone (6 times as active as cortisone) to concentrate in the lungs only. These corticosteroids could be given together with the natural flavonoid luteolin because of its antiviral and anti-inflammatory properties, especially its ability to inhibit mast cells, which are the main source of cytokines in the lungs. At the end, we should remember that "The good physician treats the disease; the great physician treats the patient who has the disease" [Sir William Osler's (1849-1919)].
Abstract|In immigration enforcement, many undocumented immigrants with children are often detained and deported. But it is their US-born citizen-children that have been overlooked in immigration debates and enforcement policies and practices. Citizen-children are at risk for negative psychological outcomes when families are fractured and destabilized by arrest, detention, and deportation. The children risk being torn from their parents and, often, their undocumented siblings. To add to the small but growing empirical base on the effects of living under the threat of deportation and actual deportation of parents, we compared the psychological status of three groups of citizen-children: (1) a group living in Mexico with their deported parents; (2) a group in the US with parents affected by detention or deportation; and (3) a comparison group of citizen-children whose undocumented parents were not affected by detention or deportation. We compared children on self-report and parent-report measures of behavioral adjustment, depression, anxiety, and self-concept. Across the three groups we found elevated levels of distress, and differences between children who had experienced a parent's detention or deportation and those who had not. We discuss findings in the context of children's clinical needs, future research, and implications for immigration enforcement policy and practices.
Abstract|Cancer is driven by genetic change, and the advent of massively parallel sequencing has enabled systematic documentation of this variation at the whole-genome scale1,2,3. Here we report the integrative analysis of 2,658 whole-cancer genomes and their matching normal tissues across 38 tumour types from the Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium of the International Cancer Genome Consortium (ICGC) and The Cancer Genome Atlas (TCGA). We describe the generation of the PCAWG resource, facilitated by international data sharing using compute clouds. On average, cancer genomes contained 4–5 driver mutations when combining coding and non-coding genomic elements; however, in around 5% of cases no drivers were identified, suggesting that cancer driver discovery is not yet complete. Chromothripsis, in which many clustered structural variants arise in a single catastrophic event, is frequently an early event in tumour evolution; in acral melanoma, for example, these events precede most somatic point mutations and affect several cancer-associated genes simultaneously. Cancers with abnormal telomere maintenance often originate from tissues with low replicative activity and show several mechanisms of preventing telomere attrition to critical levels. Common and rare germline variants affect patterns of somatic mutation, including point mutations, structural variants and somatic retrotransposition. A collection of papers from the PCAWG Consortium describes non-coding mutations that drive cancer beyond those in the TERT promoter4; identifies new signatures of mutational processes that cause base substitutions, small insertions and deletions and structural variation5,6; analyses timings and patterns of tumour evolution7; describes the diverse transcriptional consequences of somatic mutation on splicing, expression levels, fusion genes and promoter activity8,9; and evaluates a range of more-specialized features of cancer genomes8,10,11,12,13,14,15,16,17,18.
Conclusions and future perspectives|The resource reported in this paper and its companion papers has yielded insights into the nature and timing of the many mutational processes that shape large- and small-scale somatic variation in the cancer genome; the patterns of selection that act on these variations; the widespread effect of somatic variants on transcription; the complementary roles of the coding and non-coding genome for both germline and somatic mutations; the ubiquity of intratumoral heterogeneity; and the distinctive evolutionary trajectory of each cancer type. Many of these insights can be obtained only from an integrated analysis of all classes of somatic mutation on a whole-genome scale, and would not be accessible with, for example, targeted exome sequencing.The promise of precision medicine is to match patients to targeted therapies using genomics. A major barrier to its evidence-based implementation is the daunting heterogeneity of cancer chronicled in these papers, from tumour type to tumour type, from patient to patient, from clone to clone and from cell to cell. Building meaningful clinical predictors from genomic data can be achieved, but will require knowledge banks comprising tens of thousands of patients with comprehensive clinical characterization83. As these sample sizes will be too large for any single funding agency, pharmaceutical company or health system, international collaboration and data sharing will be required. The next phase of ICGC, ICGC-ARGO (https://www.icgc-argo.org/), will bring the cancer genomics community together with healthcare providers, pharmaceutical companies, data science and clinical trials groups to build comprehensive knowledge banks of clinical outcome and treatment data from patients with a wide variety of cancers, matched with detailed molecular profiling.Extending the story begun by TCGA, ICGC and other cancer genomics projects, the PCAWG has brought us closer to a comprehensive narrative of the causal biological changes that drive cancer phenotypes. We must now translate this knowledge into sustainable, meaningful clinical treatments.
Methods|SamplesWe compiled an inventory of matched tumour–normal whole-cancer genomes in the ICGC Data Coordinating Centre. Most samples came from treatment-naive, primary cancers, although a small number of donors had multiple samples of primary, metastatic and/or recurrent tumours. Our inclusion criteria were: (1) matched tumour and normal specimen pair; (2) a minimal set of clinical fields; and (3) characterization of tumour and normal whole genomes using Illumina HiSeq paired-end sequencing reads.We collected genome data from 2,834 donors, representing all ICGC and TCGA donors that met these criteria at the time of the final data freeze in autumn 2014 (Extended Data Table 1). After quality assurance (Supplementary Methods 2.5), data from 176 donors were excluded as unusable, 75 had minor issues that could affect some analyses (grey-listed donors) and 2,583 had data of optimal quality (white-listed donors)(Supplementary Table 1). Across the 2,658 white- and grey-listed donors, whole-genome sequences were available from 2,605 primary tumours and 173 metastases or local recurrences. Matching normal samples were obtained from blood (2,064 donors), tissue adjacent to the primary tumour (87 donors) or from distant sites (507 donors). Whole-genome sequencing data were available for tumour and normal DNA for the entire cohort. The mean read coverage was 39× for normal samples, whereas tumours had a bimodal coverage distribution with modes at 38× and 60× (Supplementary Fig. 1). The majority of specimens (65.3%) were sequenced using 101-bp paired-end reads. An additional 28% were sequenced with 100-bp paired-end reads. Of the remaining specimens, 4.7% were sequenced with read lengths longer than 101 bp, and 1.9% with read lengths shorter than 100 bp. The distribution of read lengths by tumour cohort is shown in Supplementary Fig. 11. Median read length for whole-genome sequencing paired-end reads was 101 bp (mean = 106.2, s.d. = 16.7; minimum–maximum = 50–151). RNA-sequencing data were collected and re-analysed centrally for 1,222 donors, including 1,178 primary tumours, 67 metastases or local recurrences and 153 matched normal tissue samples adjacent to the primary tumour.Demographically, the cohort included 1,469 men (55%) and 1,189 women (45%), with a mean age of 56 years (range, 1–90 years) (Supplementary Table 1). Using population ancestry-differentiated single nucleotide polymorphisms, the ancestry distribution was heavily weighted towards donors of European descent (77% of total) followed by East Asians (16%), as expected for large contributions from European, North American and Australian projects (Supplementary Table 1).We consolidated histopathology descriptions of the tumour samples, using the ICD-0-3 tumour site controlled vocabulary89. Overall, the PCAWG dataset comprises 38 distinct tumour types (Extended Data Table 1 and Supplementary Table 1). Although the most common tumour types are included in the dataset, their distribution does not match the relative population incidences, largely owing to differences among contributing ICGC/TCGA groups in the numbers of sequenced samples.Uniform processing and somatic variant callingTo generate a consistent set of somatic mutation calls that could be used for cross-tumour analyses, we analysed all 6,835 samples using a uniform set of algorithms for alignment, variant calling and quality control (Extended Data Fig. 1, Supplementary Fig. 2, Supplementary Table 3 and Supplementary Methods 2). We used the BWA-MEM algorithm90 to align each tumour and normal sample to human reference build hs37d5 (as used in the 1000 Genomes Project91). Somatic mutations were identified in the aligned data using three established pipelines, which were run independently on each tumour–normal pair. Each of the three pipelines—labelled ‘Sanger’92,93,94,95, ‘EMBL/DKFZ’96,97 and ‘Broad’98,99,100,101 after the computational biology groups that created or assembled them—consisted of multiple software packages for calling somatic SNVs, small indels, CNAs and somatic SVs (with intrachromosomal SVs defined as those >100 bp). Two additional variant algorithms102,103 were included to further improve accuracy across a broad range of clonal and subclonal mutations. We tested different merging strategies using validation data, and choses the optimal method for each variant type to generate a final consensus set of mutation calls (Supplementary Methods S2.4).Somatic retrotransposition events, including Alu and LINE-1 insertions72, L1-mediated transductions73 and pseudogene formation104, were called using a dedicated pipeline73. We removed these retrotransposition events from the somatic SV call-set. Mitochondrial DNA mutations were called using a published algorithm105. RNA-sequencing data were uniformly processed to quantify normalized gene-level expression, splicing variation and allele-specific expression, and to identify fusion transcripts, alternative promoter usage and sites of RNA editing8.Integration, phasing and validation of germline variant call-setsCalls of common (≥1% frequency in PCAWG) and rare (<1%) germline variants including single-nucleotide polymorphisms, indels, SVs and mobile-element insertions (MEIs) were generated using a population-scale genetic polymorphism-detection approach91,106. The uniform germline data-processing workflow comprised variant identification using six different variant-calling algorithms96,107,108 and was orchestrated using the Butler workflow system109.We performed call-set benchmarking, merging, variant genotyping and statistical haplotype-block phasing91 (Supplementary Methods 3.4). Using this strategy, we identified 80.1 million germline single-nucleotide polymorphisms, 5.9 million germline indels, 1.8 million multi-allelic short (<50 bp) germline variants, as well as germline SVs ≥ 50 bp in size including 29,492 biallelic deletions and 27,254 MEIs (Supplementary Table 2). We statistically phased this germline variant set using haplotypes from the 1000 Genomes Project91 as a reference panel, yielding an N50-phased block length of 265 kb based on haploid chromosomes from donor-matched tumour genomes. Precision estimates for germline SNVs and indels were >99% for the phased merged call-set, and sensitivity estimates ranged from 92% to 98%.Core alignment and variant calling by cloud computingThe requirement to uniformly realign and call variants on nearly 5,800 whole genomes (tumour plus normal) presented considerable computational challenges, and raised ethical issues owing to the use of data from different jurisdictions (Extended Data Table 2). To process the data, we adopted a cloud-computing architecture26 in which the alignment and variant calling was spread across 13 data centres on 3 continents, representing a mixture of commercial, infrastructure-as-a-service, academic cloud compute and traditional academic high-performance computer clusters (Supplementary Table 3). Together, the effort used 10 million CPU-core hours.To generate reproducible variant calling across the 13 data centres, we built the core pipelines into Docker containers28, in which the workflow description, required code and all associated dependencies were packaged together in stand-alone packages. These heavily tested, extensively validated workflows are available for download (Box 1).Validation, benchmarking and merging of somatic variant callsTo evaluate the performance of each of the mutation-calling pipelines and determine an integration strategy, we performed a large-scale deep-sequencing validation experiment (Supplementary Notes 1). We selected a pilot set of 63 representative tumour–normal pairs, on which we ran the 3 core pipelines, together with a set of 10 additional somatic variant-calling pipelines contributed by members of the PCAWG SNV Calling Methods Working Group. Sufficient DNA remained for 50 of the 63 cases for validation, which was performed by hybridization of tumour and matched normal DNA to a custom RNA bait set, followed by deep sequencing, as previously described29. Although performed using the same sequencing chemistry as the original whole-genome sequencing analyses, the considerably greater depth achieved in the validation experiment enabled accurate assessment of sensitivity and precision of variant calls. Variant calls in repeat-masked regions were not tested, owing to the challenge of designing reliable validation probes in these areas.The 3 core pipelines had individual estimates of sensitivity of 80–90% to detect a true somatic SNV called by any of the 13 pipelines; with >95% of SNV calls made by each of the core pipelines being genuine somatic variants (Fig. 1a). For indels—a more-challenging class of variants to identify in short-read sequencing data—the 3 core algorithms had individual sensitivity estimates in the range of 40–50%, with precision 70–95% (Fig. 1b). Validation of SV calls is inherently more difficult, as methods based on PCR or hybridization to RNA baits often fail to isolate DNA that spans the breakpoint. To assess the accuracy of SV calls, we therefore used the property that an SV must either generate a copy-number change or be balanced, whereas artefactual calls will not respect this property. For individual SV-calling algorithms, we estimated precision to be in the range of 80–95% for samples in the 63-sample pilot dataset.Next, we examined multiple methods for merging calls made by several algorithms into a single definitive call-set to be used for downstream analysis. The final consensus calls for SNVs were based on a simple approach that required two or more methods to agree on a call. For indels, because methods were less concordant, we used stacked logistic regression110,111 to integrate the calls. The merged SV set includes all calls made by two or more of the four primary SV-calling algorithms96,100,112,113. Consensus CNA calls were obtained by joining the outputs of six individual CNA-calling algorithms with SV consensus breakpoints to obtain base-pair resolution CNAs (Supplementary Methods 2.4.3). Consensus purity and ploidy were derived, and a multi tier system was developed for consensus copy-number calls (Supplementary Methods 2.4.3, and described in detail elsewhere7).Overall, the sensitivity and precision of the consensus somatic variant calls were 95% (90% confidence interval, 88–98%) and 95% (90% confidence interval, 71–99%), respectively, for SNVs (Extended Data Fig. 2). For somatic indels, sensitivity and precision were 60% (90% confidence interval, 34–72%) and 91% (90% confidence interval, 73–96%), respectively. Regarding SVs, we estimate the sensitivity of the merging algorithm to be 90% for true calls generated by any one calling pipeline; precision was estimated to be 97.5%. That is, 97.5% of SVs in the merged SV call-set had an associated copy-number change or balanced partner rearrangement. The improvement in calling accuracy from combining different pipelines was most noticeable in variants that had low variant allele fractions, which are likely to originate from subclonal populations of the tumour (Fig. 1c, d). There remains much work to be done to improve indel calling software; we still lack sensitivity for calling even fully clonal complex indels from short-read sequencing data.
Abstract|Objective: To investigate the therapeutic effect of CBD-ointment administered on severe skin chronic diseases and/or on their outcome scars. Methods: A spontaneous, anecdotal, retrospective study of 20 patients with two most frequent skin disorders: psoriasis (n: 5 patients), atopic dermatitis (n: 5) and resulting outcome scars (n: 10). The subjects were instructed to administer topical CBD-enriched ointment to lesioned skin areas twice daily for three months treatment. Results: Based on skin evaluations (hydration, TEWL, elasticity), clinical questionnaires (SCORAD, ADI, PASI), and supported by photographic data and investigators' clinical assessment, the results showed that topical treatment with CBD-enriched ointment significantly improved the skin parameters, the symptoms and also the PASI index score. No irritant or allergic reactions were documented during the period treatment. Conclusions: The topical administration of CBD ointment, without any THC, is a safe and effective non-invasive alternative for improve the quality of life in patients with some skin disorders, especially on inflammatory background.
Abstract|Objective: To evaluate trends by race in Agency for Healthcare Research and Quality obstetric-related quality and safety indicators and their relationships to trends in inpatient maternal and neonatal mortality. Methods: We used the Nationwide Inpatient Sample from 2000 through 2009 and calculated obstetric hospital quality and patient safety indicators and inpatient maternal and neonatal mortality stratified by race. We examined differences in age and comorbidity-adjusted trends in black compared with white women over time in the United States and by geographic region. Proportions were analyzed by χ2 and trends by regression analysis. Results: Obstetric quality indicators varied by geographic region, but changes over time were consistent for both races. Cesarean deliveries increased similarly for black and white women, and vaginal births after cesarean delivery declined for both races but more rapidly for white women than for black women. Obstetric safety indicators improved over the study period for black and white women, with obstetric trauma decreasing significantly for both groups (28% compared with 35%, respectively) and birth trauma-injury to neonates declining for both, but changes were not significant. In striking contrast, inpatient maternal and neonatal mortality remained relatively constant during the study period, with persistently higher rates of both seen among black compared with white women (12.0 compared with 4.6 per 100,000 deliveries, P<.001 and 6.6 compared with 2.5 per 1,000 births, P<.001, respectively, in 2009). Conclusion: Improvements in Agency for Healthcare Research and Quality quality indicators for obstetrics are not reflected in improvements in maternal and neonatal morbidity and mortality and do not explain continued racial disparities for outcomes in pregnancies in black and white women. Quality measures that are related to pregnancy outcomes are needed and these should elucidate obstetric health disparities.
Abstract|Background: Previous reports of lower triage acuity scores and longer Emergency Department (ED) wait times for African Americans compared to Caucasians had insufficient information to determine if this was due to bias or appropriately based on medical history and clinical presentation. Objective: (1) Determine if African Americans are assigned lower triage acuity scores (TAS) after adjusting for a number of demographic and clinical variables likely to affect triage scores. (2) Determine if lower TAS translate into clinically significant longer wait times to assignment to a treatment area. Methods: This was a retrospective matched cohort design analysis of de-identified data extracted from the ED electronic medical record system, which included demographic and clinical information, as well as TAS, and ED process times. Triage scores were assigned using a 5-point scale (ESI), with 1 being most urgent and 5 being least urgent. Mean TAS and wait times to a treatment area for specific chief complaints were compared by race; after adjusting for age, gender, insurance status, time of day, day of week, presence of co-morbidities, and abnormal vital signs using a 1:1 matched case analysis. Results: The overall mean TAS for African Americans was 2.97 vs. 2.81 for Caucasians (difference of 0.18; p<0.001), translating to a lower acuity rating. African Americans had a significantly longer wait time to a treatment area compared to case-matched Caucasians (10.9min; p<0.001), with much larger differences in wait times noted within certain specific chief complaint categories. Conclusion: Our current study supports the hypothesis that racial bias may influence the triage process.
Abstract|Background: Versus whites, blacks with diabetes have poorer control of hemoglobin A1c (HbA1c), higher systolic blood pressure (SBP), and higher low-density lipoprotein (LDL) cholesterol as well as higher rates of morbidity and microvascular complications. Objective: To examine whether several mutable risk factors were more strongly associated with poor control of multiple intermediate outcomes among blacks with diabetes than among similar whites. Design: Case-control study. Subjects: A total of 764 blacks and whites with diabetes receiving care within 8 managed care health plans. Measures: Cases were patients with poor control of at least 2 of 3 intermediate outcomes (HbA1c > or =8.0%, SBP > or =140 mmHg, LDL cholesterol > or =130 mg/dL) and controls were patients with good control of all 3 (HbA1c <8.0%, SBP <140 mmHg, LDL cholesterol <130 mg/dL). In multivariate analyses, we determined whether each of several potentially mutable risk factors, including depression, poor adherence to medications, low self-efficacy for reducing cardiovascular risk, and poor patient-provider communication, predicted case or control status. Results: Among blacks but not whites, in multivariate analyses depression (odds ratio: 2.28; 95% confidence interval: 1.09-4.75) and having missed medication doses (odds ratio: 1.96; 95% confidence interval: 1.01-3.81) were associated with greater odds of being a case rather than a control. None of the other risk factors were associated for either blacks or whites. Conclusions: Depression and missing medication doses are more strongly associated with poor diabetes control among blacks than in whites. These 2 risk factors may represent important targets for patient-level interventions to address racial disparities in diabetes outcomes.
Abstract|Flexible transparent electrodes are in significant demand in applications including solar cells, light-emitting diodes, and touch panels. The combination of high optical transparency and high electrical conductivity, however, sets a stringent requirement on electrodes based on metallic materials. To obtain practical sheet resistances, the visible transmittance of the electrodes in previous studies is typically lower than the transparent substrates the electrode structures are built on, namely, the transmittance relative to the substrate is <100%. Here, we demonstrate a flexible dielectric-metal-dielectric-based electrode with ~88.4% absolute transmittance, even higher than the ~88.1% transmittance of the polymer substrate, which results in a relative transmittance of ~100.3%. This non-trivial performance is achieved by leveraging an optimized dielectric-metal-dielectric structure guided by analytical and quantitative principles described in this work, and is attributed to an ultra-thin and ultra-smooth copper-doped silver film with low optical loss and low sheet resistance.
Introduction|Transparent electrodes are widely used in photovoltaics (PVs)1,2, light-emitting diodes (LEDs)3,4,5, touch panels6,7, and other optoelectronic devices. Indium tin oxide (ITO) is the conventional selection for the transparent electrode because of its high visible transmittance and electrical conductivity. However, the low abundance of the indium element on earth is a limiting factor of this material. In addition, its applications in emerging flexible optoelectronic devices are significantly hindered by both the poor mechanical flexibility and the high annealing temperature needed to reduce its resistivity8. Graphene9,10, carbon nanotubes (CNTs)11,12, and conductive polymers13 have been investigated to replace ITO. Unfortunately, their limited conductivity and high cost of mass-production remains a challenge in large-area applications. To overcome these limitations, transparent electrodes employing metal networks have been proposed14,15,16,17,18,19,20,21,22,23. Although these novel structures exhibit decent optical and electrical performance and can also be scaled up easily via different roll-to-roll (R2R) techniques24,25, the extruded or non-uniform surfaces may cause shorting problems when being used in LED and PV devices14. In addition, the high optical haze resulting from the scattering of the nanowires and mesh patterns is undesired in high-resolution displays14. In recent years, dielectric–metal–dielectric (DMD)-based transparent electrodes have been noted as potential alternatives. In this type of electrode, a thin metallic film is sandwiched between two antireflection dielectrics to induce high transparency. They feature high transparency and conductivity, low haze, excellent flexibility, facile fabrication, and great compatibility with different substrates2,26,27,28,29,30,31,32,33,34,35,36,37. The trade-off between electrical conductivity and optical transmittance has been a major challenge for metallic-material-based transparent electrodes. To achieve a practical conductivity (e.g., sheet resistance (Rs) < 20 Ω sq−1), the absolute visible transmittance of previously reported metallic-material-based transparent electrodes, including both metal network and DMD structures, is typically lower than that of the substrate itself, which has been taken for granted without serious questioning.Therefore, we are motivated to conduct a rigorous investigation and explore the limit of transmittance at a sufficiently low sheet resistance suitable for practical applications. In this work, we demonstrate a DMD-based transparent electrode with ~88.4% absolute transmittance averaged over the entire visible spectrum (400–700 nm) on polyethylene terephthalate (PET) polymeric substrate, which surpasses the transmittance of the substrate itself (~88.1%), leading to a relative transmittance >100%. This counter-intuitive, yet achievable performance is obtained by (i) quantitative design principles that are generalized in this work, particularly, analytic expression of the optimal bottom dielectric thickness and analytical result showing that different materials should be used for the two dielectrics, (ii) the use of an ultra-thin (~6.5 nm thick) and ultra-smooth (roughness < 1 nm) copper-doped silver (Cu-doped Ag) film providing low optical loss and high electrical conductivity at the same time. The proposed design principles and electrode structures have resolved the problems faced by other existing transparent electrodes and may have the potential to replace traditional ITO counterparts for flexible optoelectronics, thus facilitating high-performance flexible displays and optoelectronic devices.
Results|Strategy to achieve relative transmittance surpassing 100%Relative transmittance, Tr, is defined as the ratio of the absolute transmittance of the structure where a transparent substrate is coated with a metallic electrode, TD (see Fig. 1a), to the absolute transmittance of the bare substrate, TS (Fig. 1b):$$T_{\mathrm{r}} = \frac{{T_{\mathrm{D}}}}{{T_{\mathrm{S}}}} \approx \frac{{1 - \left( {R_1 + A} \right) - R_2}}{{1 - R_2 - R_2}}.$$ (1) where R1 is the reflection at the top side of DMD and A is the absorption of the metallic film, and R2 is the reflection at the substrate/air interface. For simplicity, the multiple round-trip reflections between the front and bottom surfaces of the substrate are ignored due to the negligible reflection intensity at both interfaces. Intuitively, Tr is thought to be smaller than 100% because a metallic electrode is usually reflective. However, in this section, we will provide guidelines built around the antireflection principles to reduce R1 and A using the DMD structure, and show that with optimized design (R1 + A) can be made smaller than the single side reflection of the substrate, R2, leading to relative transmittance surpassing 100%.Fig. 1: The definition of relative transmission. a Transparent substrate coated with a DMD structure. b Bare substrate.Full size imageAs shown in Fig. 2a, seven parameters (the refractive index, n2, and thickness, d2, of the transparent Dielectric 1, the refractive index, n3, extinction coefficient, κ3, and thickness, d3, of the metallic film, and the refractive index, n4, and thickness, d4, of the transparent Dielectric 2) should all be considered in order to optimize the transmittance over a broad spectrum in the visible, which makes it challenging to provide quantitative design guidelines33,34,35. Our design strategy is to maximize the transmittance for the condition where the metallic film is thick enough to achieve a practical sheet resistance; and thus, this strategy enables the synergistic combination of high optical transmittance and high electrical conductivity of DMD-based transparent electrodes.Fig. 2: Theoretical design of a DMD electrode.a Design parameters and reflection waves at various interfaces of a DMD transparent electrode. b Phasor diagram of reflection waves. c An ideal DMD and its parameters. d The calculated values of the left and right terms in Eq. (9) when using 24 nm TiO2 and 9 nm Ag as Dielectric 2 and the metallic layer, respectively, in the DMD structure, showing that Eq. (9) is well satisfied across the entire visible range. e Optimal averaged transmittance of the DMD electrode across the visible range (400–700 nm) dependent on the thickness of Dielectric 2. Here, TiO2 and 9 nm Ag are used as the bottom dielectric layer and the middle metallic layer, respectively. Each optimal transmittance value at a fixed TiO2 thickness is found out by sweeping the refractive index (n2) and the thickness (d2) of Dielectric 1.Full size imageAs a first step in the design process, a suitable material is selected for the sandwiched metallic layer by considering its (i) electrical conductivity; (ii) light absorption property in the visible range. Among all metals, silver (Ag), copper (Cu), and gold (Au) exhibit the lowest intrinsic electrical resistivity. The absorption of light by the metallic film in a DMD structure is expressed as$$A\left( {\lambda _0} \right) = \frac{{4\pi n_3\kappa _3}}{{\lambda _0}}\frac{{{\int}_0^{d_3} {\left| {E\left( z \right)} \right|^2{\mathrm{d}}z} }}{{\left| {E_0} \right|^2}},$$ (2) where E(z) is the electric field in the metallic film, E0 is the incident electric field and λ0 is the free-space wavelength. With a given thickness of the metallic layer, a metal with a small (nk) should be chosen for low absorption and high transmittance. Among Ag, Cu, and Au, Ag (n = 0.13 and k = 3.17 at 550 nm wavelength38) offers the lowest (nk) in the visible range, and is therefore, employed as the metallic layer in this study. Considering the small real part refractive index of Ag, n3 = 0 is reasonably assumed for the rest of this section for simplicity.In the next step, the thickness and the refractive index of Dielectric 2 are determined. To reduce the reflection loss at the top surface, the sum of two vectors shown in Fig. 2a should be designed to cancel r12, which is the complex reflection coefficient at the interface (1–2 interface) between the air (n1 = 1) and Dielectric 1 (\(r_{pq} = ( {\widetilde n_p{\kern 1pt} {\mathrm{cos}}{\kern 1pt} \theta _p - \widetilde n_q{\kern 1pt} {\mathrm{cos}}{\kern 1pt} \theta _q} )/( {\widetilde n_p{\kern 1pt} {\mathrm{cos}}{\kern 1pt} \theta _p + \widetilde n_q{\kern 1pt} {\mathrm{cos}}{\kern 1pt} \theta _q} )\) is the reflection coefficient at p–q interface for TE polarization where \(\widetilde n_{p(q)} = n_{p(q)} + {\mathrm{i}}\kappa _{p(q)}\) is the complex refractive index of layer p(q) and \(\theta _{p(q)}\) is the direction of wave propagation in the corresponding layer). The first vector is r23 with a phase shift acquired from the wave propagation in Dielectric 1. The second is r3,45 with a magnitude attenuation resulting from the propagation in the metallic film and with a phase shift due to the propagation in Dielectric 1. r3,45 is the total reflection coefficient at 3–4 interface (where the light reflected from the interface between 4 and 5 is taken into account), and can be expressed as$$r_{3,45} = \frac{{r_{34} + r_{45}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_4d_4} \right)}}{{1 + r_{34}r_{45}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_4d_4} \right)}}.$$ (3) In order to cancel out each other yielding R1 = 0, the trajectory of these three vectors should be a triangle or they should be aligned in the way as shown in Fig. 2b, therefore$$\left| {r_{3,45}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }{\mathrm{i}}\kappa _3d_3} \right){\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_2d_2} \right)} \right| \ge \left| {r_{23}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_2d_2} \right)} \right| - \left| {r_{12}} \right|.$$ (4) Since n3 = 0 as we assumed,$$\left| {r_{23}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_2d_2} \right)} \right| = 1 > \left| {r_{12}} \right|,$$ (5) $$\left| {r_{23}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_2d_2} \right)} \right| = 1 > \left| {r_{3,45}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }{\mathrm{i}}\kappa _3d_3} \right){\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{i}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_2d_2} \right)} \right|,$$ (6) Thus, the right term in Eq. (4) represents the minimum value of the left term, which is achieved when the three vectors are aligned in the way as shown in Fig. 2b. In this case, we have$$\left| {r_{3,45}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }{\mathrm{i}}\kappa _3d_3} \right){\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_2d_2} \right)} \right| = \left| {r_{23}{\kern 1pt} {\mathrm{exp}}{\kern 1pt} \left( {2{\mathrm{{i}}}\frac{{2{\mathrm{\pi }}}}{\lambda }n_2d_2} \right)} \right| - \left| {r_{12}} \right|,$$ (7) When the left term in Eq. (4) is smaller than this minimum value, the reflection cannot be completely suppressed.Since the left term in Eq. (4) decreases with increasing metallic film thickness (d3), |r3,45| should be designed to achieve its maximum value so that Eq. (4) can be satisfied for maximum metallic film thickness to achieve high electrical conductivity. To maximize |r3,45|, r34, and r45 should interfere constructively with each other so that$$\psi _{{\mathrm{prop}}} + \psi _{45} - \psi _{34} = 0,$$ (8) where ψ34 is the phase angle of r34, representing the phase shift due to the reflection at 3–4 interface, and ψ45 is the phase angle of r45, representing the reflection phase shift at 4–5 interface. n4 > n5 (shown later) gives ψ45 = 0. ψprop = 4πn4d4/λ is the propagation phase shift picked up when the wave is propagating in Dielectric 2. Then we get$$\psi _{34} = \frac{{4{\mathrm{\pi }}}}{\lambda }n_4d_4.$$ (9) Since |r3,45| increases with n4 (see Supplementary Fig. 1), materials of high refractive index should be used as Dielectric 2 to achieve the maximum value of |r3,45| and its thickness can be calculated using Eq. (9). It is interesting to notice that \(\psi _{34} = {\mathrm{arctan}}{\kern 1pt} \left[ {2\kappa _3n_4/\left( {\kappa _3^2 - n_4^2} \right)} \right]\) decreases with wavelength due to the increase of κ3 with the wavelength for Ag, and the value of 4πn4d4/λ also decreases with the wavelength. Therefore, Eq. (9) can be potentially self-fulfilled over a broad spectrum, resulting in broadband high transmittance.Finally, the refractive index and the thickness of Dielectric 1 are suitably selected for the optimal transmittance. The phasor diagram in Fig. 2b also gives$$\psi _{23} - \psi _{3,45} = {\mathrm{\pi }},$$ (10) $$\psi _{23} + \frac{{4{\mathrm{\pi }}}}{\lambda }n_2d_2 - \psi _{12} = {\mathrm{\pi }}.$$ (11) Substituting ψ12 = π into Eq. (11) gives$$\psi _{23} + \frac{{4{\mathrm{\pi }}}}{\lambda }n_2d_2 = 0.$$ (12) 4πn2d2/λ decreases with the wavelength and \(\psi _{23} = {\mathrm{arctan}}{\kern 1pt} \left[ {2\kappa _3n_2/\left( {\kappa _3^2 - n_2^2} \right)} \right]\) also decreases with the wavelength because κ3 increases with the wavelength for Ag. Unlike Eq. (9), Eq. (12) cannot be satisfied over a broad spectrum. Thus, to achieve broadband high transmittance, we need to sweep the refractive index and the thickness of Dielectric 1 within a small range to find out the optimal value. Since the refractive index of typical dielectrics are between ~1.38 (magnesium fluoride, MgF2) and ~2.6 (titanium dioxide, TiO2), the required efforts are significantly reduced compared to the case where the refractive indices and thicknesses of all the three D–M–D layers need to be optimized.Now, we summarize the design procedure for highly transparent DMD electrodes. Step 1 (select n3, κ3, and d3): choose a low-loss and highly conductive metallic film for the DMD layer (in most cases, use Ag). The metallic film thickness is designed to achieve the electrical conductivity required in practical applications. It is worth noting that higher overall transmittance can be achieved by using a thinner metallic film. Step 2 (select n4): use high refractive index materials for Dielectric 2. The larger n4 is used, the higher overall transmittance can be obtained. Step 3 (determine d4): the thickness of Dielectric 2 is calculated using Eq. (9). Step 4 (design n2 and d2): optimize the refractive index and the thickness of Dielectric 1 within a small range to achieve the optimal broadband transmittance.Proof of conceptFirst, we use the results for an ideal DMD structure at a single wavelength (Fig. 2c) reported in a previous study39 to verify the design guidance provided in Eqs. (9), (10), and (12). According to ref. 39, an ideal DMD can be achieved at a selected wavelength for n1 = n5 = 1, n3 = 0, n2 = n4 = κ3, and d2 = d4 = λ/(8κ3). Substituting n4 = κ3 into Eq. (9) gives d4 = λ/(8κ3), substituting n2 = κ3 into Eq. (12) gives d2 = λ/(8κ3), and substituting n4 = κ3 into Eq. (10) gives n2 = n4, all of which are perfectly consistent with the conclusions obtained in ref. 39.In realistic cases, the substrate cannot be air (i.e., n5 ≠ n1) and the refractive indices of common dielectrics are usually much smaller than the extinction coefficients of metals in the visible range (i.e., n4 « κ3). This implies that different materials are required for Dielectrics 1 and 2 in a realistic DMD structure targeting optimal broadband transparency, i.e., n2 ≠ n4. However, the same material was used for Dielectrics 1 and 2 in most of the previous studies (more than 70 published works28), which means those results are not optimal in terms of optical transmittance and indicates the necessity of providing quantitative guidelines in order to obtain optimized DMD electrode designs.Next, the optical performance of a DMD electrode on a common optical substrate, PET, is designed based on our proposed guidelines. The flexible PET substrate exhibits an averaged absolute transmittance of ~88.0% from 400 to 700 nm as shown in Supplementary Fig. 2. A 9-nm-thick Ag film is employed as the middle metallic layer and a TiO2 film with recorded-high n4 = 2.6 reported in ref. 27 is used for Dielectric 2. At the center wavelength of the visible range (i.e., 550 nm), ψ34 is calculated as 1.37, which leads to Dielectric 2 thickness d4 = 24 nm according to Eq. (9). As displayed in Fig. 2d, Eq. (9) is satisfied over the entire visible spectrum at d4 = 24 nm because both the left (ψ34) and right (4πn4d4/λ) terms of the equation decreases with the wavelength, which is consistent with our prediction in the above discussion. As summarized in Fig. 2e, a more straightforward way to find out the optimal d4 value at which the DMD structure presents the highest averaged transmittance in the visible range (400–700 nm) is to sweep d4 within a range (e.g., from 10 to 50 nm) and obtain the best transmittance at each d4 value by sweeping the refractive index (n2) and the thickness (d2) of Dielectric 1. As seen from the plot, the highest averaged absolute transmittance of ~89.8% is achieved at d4 = 24 nm, which agrees perfectly with the calculated results using Eq. (9) and thereby validates the accuracy of our proposed method. Correspondingly, the refractive index and the thickness of Dielectric 1 are n2 = 1.7 and d2 = 63 nm, respectively, when the highest transmittance is achieved at d4 = 24 nm. The absolute transmittance and reflectance spectra of the optimized structure, i.e., PET substrate/24 nm TiO2/9 nm Ag/63 nm Dielectric 1, spanning the entire visible range is provided in Supplementary Fig. 3. This n2 can be realized by Al2O3 film fabricated using common deposition methods. The relative transmittance of the optimized DMD electrode Tr is ~102.0% compared to the simulated transmittance (~88.0%) of the PET substrate, which indicates that our proposed design procedure effectively guarantees the optimal optical performance of the DMD structure and a relative transmittance >100% is achievable with suitable material selection and structure design.Experimental demonstrationsBased on the previous analysis, high transparency can be achieved by suppressing the metal absorption loss with thin Ag films. However, large surface roughness is an unavoidable issue for physical-vapor-deposited (evaporated or sputtered) ultra-thin (<20 nm) Ag films due to Ag atom’s intrinsic three-dimensional (3D) Volmer–Weber growth mode40. As an example, the root-mean-square (RMS) roughness of an evaporated 15 nm Ag film can be as large as 6 nm41. Such an issue will affect the film conductivity and induce additional scattering loss, thereby impairing both electrical and optical performance of the DMD electrodes.We recently demonstrated a novel approach to achieve ultra-thin (down to 6 nm) and ultra-smooth (RMS roughness < 1 nm) Ag film by doping a metallic element (e.g., Aluminum, Copper, Titanium, or Chromium) during Ag deposition (as illustrated in Supplementary Fig. 4)2,5,29,42. In this work, we choose Cu-doped Ag as the metallic layer as this film offers the lowest optical loss among different doped Ag films5. The deposition rates of Cu and Ag were chosen as 0.19 and 11.09 Å s−1, respectively, which corresponds to ~2% Cu atomic concentration. The added Cu atoms effectively suppress the 3D island formation of the Ag atoms during film deposition and promote an early-stage formation of ultra-thin (<10 nm) Ag films. An atomic force microscopy (AFM) image showing details of a 6.5-nm-thick Cu-doped Ag film deposited on a fused silica substrate is displayed in Fig. 3a. The RMS roughness of the film is measured to be ~0.47 nm, which is more than 10 times lower than that of pure Ag at a similar thickness reported in previous works2,5,41. A 3D view of the AFM scan is further provided in Supplementary Fig. 5 to show the surface morphology and roughness of the ultrathin Cu-doped Ag film.Fig. 3: Characterizations of a Cu-doped Ag thin film.a Atomic force microscopy (AFM) characterizations of a Cu-doped Ag thin film. The scale bar is 100 nm. b Measured relative permittivity (ε = ε1 + iε2) values as a function of free-space wavelength of Cu-doped Ag and pure Ag. The shaded region refers to the visible range (400–700 nm), over which our DMD electrode is optimized.Full size imageAs a direct representation of the light absorption property of a film, the imaginary part (ε2) of the measured permittivity (ε = ε1 + iε2 = (n + iκ)2) of the 6.5-nm-thick Cu-doped Ag (Fig. 3b) is very close to that of pure Ag in the visible range (400–700 nm), which indicates the low optical loss of the ultrathin Cu-doped Ag film. Here, a 20 nm-thick pure Ag film was sputter-deposited for the calibration and the refractive indices of both Cu-doped Ag and pure Ag films were characterized using the spectroscopic ellipsometry. In addition, the sheet resistance of this thin Cu-doped Ag film was measured as ~18.6 Ω sq−1, showing the high conductivity of the ultrathin Cu-doped Ag film.Next, a DMD-based transparent electrode is constructed employing this 6.5-nm-thick Cu-doped Ag. Here, ~50-μm-thick PET film, which exhibits excellent mechanical flexibility, is used as the substrate. Zinc oxide (ZnO) is selected as Dielectric 2 due to its high refractive index (n ~ 2.0) and negligible optical loss in the visible range (ZnO is the highest refractive index material achievable in our group). The refractive indices of PET, ZnO, and Cu-doped Ag across the whole visible wavelength range are provided in Supplementary Fig. 6. The thickness (d4) of Dielectric 2 is calculated as ~24 nm with Eq. (9) considering n4 ~ 2.05 and ñ3 ~ 0.19 + 3.39i at 550 nm wavelength. As shown in Fig. 4a, the interesting compensation between the left (ψ34) and right (4πn4d4/λ) sides of Eq. (9) spanning 400–700 nm is also effectively verified in this case. Thus, 24 nm ZnO is an optimal selection not only for a single wavelength (550 nm) but also for the entire visible range. As the last step, the refractive index and the thickness of the top dielectric (Dielectric 1) is swept from 1.3 to 2.6 and from 1 to 100 nm with a step of 0.05 and 1 nm, respectively, to find out the optimal combination for the highest averaged absolute transmittance across the 400–700 nm wavelength range. Figure 4b shows that n2 = 1.65 and d2 = 56 nm for the optimal condition and this n2 is close to the refractive index of Al2O3. By applying the measured refractive index of Al2O3 (Supplementary Fig. 6), d2 was re-optimized and is still equal to 56 nm.Fig. 4: Experimental demonstration of the designed DMD electrode.a The calculated values of the left and right terms in Eq. (9) when using 24 nm ZnO and 6.5 nm Cu-doped Ag as Dielectric 2 and the metallic layer, respectively, in the DMD structure. It shows Eq. (9) is well satisfied across the entire visible range. b Averaged absolute visible transmittance (%) of the DMD electrode dependent on the refractive index and the thickness of the top dielectric (Dielectric 1) when Dielectric 2 and the metallic layer are selected as 24 nm ZnO and 6.5 nm Cu-doped Ag, respectively. c Calculated (red solid curve) and measured (blue dashed curve) absolute transmittance from 400 to 700 nm of the designed DMD transparent electrode, showing great consistency with each other. The representative measured spectrum with an averaged transmittance of ~88.4% is taken from the measurement results of 15 samples. As a comparison, one representative transmittance spectrum of the bare PET substrate with the averaged transmittance of ~88.1% is provided to show how the optical performance of the optimized DMD electrode gets improved. Inset presents the configuration of the designed DMD electrode, i.e., PET substrate/24 nm ZnO/6.5 nm Cu-doped Ag/56 nm Al2O3. d A photograph of the fabricated flexible electrode, showing high transparent and neutral appearance.Full size imageAs illustrated in the inset in Fig. 4c, the final configuration of the DMD transparent electrode is determined as PET substrate/24 nm ZnO/6.5 nm Cu-doped Ag/56 nm Al2O3. The measured and simulated optical spectra of the designed electrode are presented in Fig. 4c, showing great consistency with each other. The averaged absolute transmittances across 400–700 nm calculated from experimental and theoretical results are ~(88.4 ± 0.1)% and ~88.4%, respectively. Figure 4c clearly shows that the transmittance of the DMD electrode is higher than the absolute transmittance of the bare PET substrate (~(88.1 ± 0.4)% from the measurement) with the optical transmittance from 416 to 607 nm effectively enhanced with the two antireflection dielectrics. Here, the cited uncertainties above represent one standard deviation of the measured data. The experimental averaged transmittances of the DMD electrode and the PET substrate are calculated with the measurement results of 15 samples. Detailed data can be found in Supplementary Figs. 2 and 7. In contrast to all the results reported in previous works, the relative transmittance of our proposed DMD electrode surpasses 100% (Tr ~ 100.3% based on the measurement data) as its optical loss (~11.6%) resulting from the stack reflection and absorption (=100%−transmittance) is well suppressed below the ~11.9% optical loss of the bare PET substrate (Supplementary Fig. 2). The angular performance of the DMD electrode is further investigated in Supplementary Fig. 8, which shows that high transmittance >75% is maintained up to 60° angle of incidence. A photograph of the fabricated flexible DMD structure on the PET substrate is displayed in Fig. 4d, which clearly shows the high transparency.It is worthwhile noting that even higher transmittance can be achieved by using a bottom dielectric of higher refractive index than that of ZnO, such as TiO2 (whose refractive index is about 2.6 in the visible range27,43,44). In this case, the thickness of the high-index TiO2 layer is determined as d4 = 22 nm using Eq. (9). The refractive index and the thickness of Dielectric 1 are optimized as n2 = 1.6 and d2 = 72 nm, respectively, by sweeping n2 and d2 within a broad range. The optimal structure ‘PET substrate/22 nm TiO2/6.5 nm Cu-doped Ag/72 nm Dielectric 1’ gives out an averaged absolute transmittance of 89.6% across 400–700 nm, which corresponds to a relative transmittance of ~101.8% as presented in Supplementary Fig. 9.Finally, we discuss the functionality of our designed DMD electrode and other material options depending on the intended applications. Firstly, the use of the insulating Al2O3 layer will make the proposed DMD structure an excellent candidate for the transparent heat mirror28, that can be used for window defrosting or deicing applications. Secondly, to use the DMD structure as an electrode in optoelectronic devices, the Al2O3 layer can be replaced by other dielectric materials featuring a similar refractive index that can also function as an effective electron/hole transport layer. Possible candidates include ZnO sol–gel films1, which are commonly used as electron transport layers in solar cells and organic LEDs (OLEDs). Due to its porous structure, the refractive index is much lower than that of a dense ZnO film prepared by vacuum-deposition methods and is very close to the ideal refractive index (n2 = 1.65) required in our design (Supplementary Fig. 10). Notably, our proposed design principles are not limited to treating the ambient as air, in Supplementary Information we show that they are also applicable when designing a DMD electrode used in a solar cell, as an example of optoelectronic devices (see Supplementary Fig. 11 for more details). Thirdly, the dielectric layers (ZnO and Al2O3) also play a critical role in protecting the ultrathin metallic film from degradation. As presented in Supplementary Fig. 12, the DMD electrode on the PET substrate has survived the accelerated test under high temperature and humidity (85 °C, 85% relative humidity), showing ~12.6% change of the sheet resistance after 120 h test, while the sheet resistance of the ultrathin Cu-doped Ag without any protections increases quickly to infinity under the same test condition after 8 h.
Discussion|Relative transmittance higher than 100% was achieved in this work by the integration of a novel Cu-doped Ag film into an optimized DMD structure. This Cu-doped Ag film features ultra-thin thickness (~6.5 nm thick), ultra-smooth morphology (roughness < 1 nm), low optical loss, and high electrical conductivity (sheet resistance ~18.6 Ω sq−1), and was fabricated using a room-temperature deposition method. The optimized DMD structure was designed by quantitative principles that were generalized for the first time in this work. We experimentally demonstrated that the flexible DMD-based electrode, although not optimal, has 88.4% absolute averaged transmittance over the visible spectrum, which is higher than 88.1% transmittance of its PET substrate. This study provides an exciting pathway to address the major challenge faced by existing flexible transparent electrodes and to replace traditional ITO counterparts, thus facilitating high-performance flexible optoelectronic devices.
Methods|Film depositionMaterials used in this work were all deposited by sputtering (LAB 18, Kurt J. Lesker Co.). Cu-doped Ag films were deposited by DC co-sputtering from Cu and Ag targets with 4.5 mTorr argon (Ar) pressure. The optimized deposition rates of Cu and Ag were 0.19 and 11.09 Å s−1. Pure Ag was deposited by DC sputtering for 12.4 Å s−1 and 4.5 mTorr condition. ZnO and Al2O3 films were deposited by RF sputtering with 4.5 and 3.0 mTorr Ar pressure, respectively.Material and optical characterizationsAll optical simulations were performed using the transfer matrix method. The refractive indices and thicknesses of materials and transmission spectra of electrodes were measured by spectroscopic ellipsometry method (M-2000, J. A. Woollam Inc.). The reflection spectra of the fabricated electrodes were measured by a thin-film measurement instrument (F20, Filmetrics) integrated with a spectrometer and light source. The sheet resistances were measured using a four-point probe method (FPP-5000, Miller Design & Equipment). The surface morphology of Cu-doped Ag films was characterized by tapping mode atomic force microscopy (Dimension Icon AFM, Bruker Corporation).
Abstract|Purpose: Health providers' implicit racial bias negatively affects communication and patient reactions to many medical interactions. However, its effects on racially discordant oncology interactions are largely unknown. Thus, we examined whether oncologist implicit racial bias has similar effects in oncology interactions. We further investigated whether oncologist implicit bias negatively affects patients' perceptions of recommended treatments (i.e., degree of confidence, expected difficulty). We predicted oncologist implicit bias would negatively affect communication, patient reactions to interactions, and, indirectly, patient perceptions of recommended treatments. Methods: Participants were 18 non-black medical oncologists and 112 black patients. Oncologists completed an implicit racial bias measure several weeks before video-recorded treatment discussions with new patients. Observers rated oncologist communication and recorded interaction length of time and amount of time oncologists and patients spoke. Following interactions, patients answered questions about oncologists' patient-centeredness and difficulty remembering contents of the interaction, distress, trust, and treatment perceptions. Results: As predicted, oncologists higher in implicit racial bias had shorter interactions, and patients and observers rated these oncologists' communication as less patient-centered and supportive. Higher implicit bias also was associated with more patient difficulty remembering contents of the interaction. In addition, oncologist implicit bias indirectly predicted less patient confidence in recommended treatments, and greater perceived difficulty completing them, through its impact on oncologists' communication (as rated by both patients and observers). Conclusion: Oncologist implicit racial bias is negatively associated with oncologist communication, patients' reactions to racially discordant oncology interactions, and patient perceptions of recommended treatments. These perceptions could subsequently directly affect patient-treatment decisions. Thus, implicit racial bias is a likely source of racial treatment disparities and must be addressed in oncology training and practice.
Abstract|Objective: To determine the household and community characteristics most closely associated with variation in COVID-19 incidence on American Indian reservations in the lower 48 states. Design: Multivariate analysis with population weights. Setting: Two hundred eighty-seven American Indian Reservations and tribal homelands (in Oklahoma) and, as of April 10, 2020, 861 COVID-19 cases on these reservation lands. Main outcome measures: The relationship between rate per 1000 individuals of publicly reported COVID-19 cases at the tribal reservation and/or community level and average household characteristics from the 2018 5-Year American Community Survey records. Results: By April 10, 2020, in regression analysis, COVID-19 cases were more likely by the proportion of homes lacking indoor plumbing (10.83, P = .001) and were less likely according to the percentage of reservation households that were English-only (-2.43, P = .03). Household overcrowding measures were not statistically significant in this analysis (-6.40, P = .326). Conclusions: Failure to account for the lack of complete indoor plumbing and access to potable water in a pandemic may be an important determinant of the increased incidence of COVID-19 cases. Access to relevant information that is communicated in the language spoken by many reservation residents may play a key role in the spread of COVID-19 in some tribal communities. Household overcrowding does not appear to be associated with COVID-19 infections in our data at the current time. Previous studies have identified household plumbing and overcrowding, and language, as potential pandemic and disease infection risk factors. These risk factors persist. Funding investments in tribal public health and household infrastructure, as delineated in treaties and other agreements, are necessary to protect American Indian communities.
Abstract|With the ever-increasing availability of whole-genome sequences, machine-learning approaches can be used as an alternative to traditional alignment-based methods for identifying new antimicrobial-resistance genes. Such approaches are especially helpful when pathogens cannot be cultured in the lab. In previous work, we proposed a game-theory-based feature evaluation algorithm. When using the protein characteristics identified by this algorithm, called ‘features’ in machine learning, our model accurately identified antimicrobial resistance (AMR) genes in Gram-negative bacteria. Here we extend our study to Gram-positive bacteria showing that coupling game-theory-identified features with machine learning achieved classification accuracies between 87% and 90% for genes encoding resistance to the antibiotics bacitracin and vancomycin. Importantly, we present a standalone software tool that implements the game-theory algorithm and machine-learning model used in these studies.
Introduction|Antimicrobial resistance (AMR) refers to a property of bacteria when they become less susceptible to an antimicrobial agent1,2,3,4. Bacteria can gain AMR by overexpressing or duplicating available genes, undergoing chromosomal mutation, or obtaining resistance genes from other bacteria by means of horizontal gene transfer1, 5. According to a recently released report by the Centers for Disease Control and Prevention (CDC), at least 2.8 million people in the United States are infected every year by antimicrobial-resistant organisms, and these infections result in more than 35,000 deaths6. Also, according to a recently released report by the Organisation for Economic Co-operation and Development (OECD), 2.4 million deaths are predicted in Europe, North America, and Australia in the next 30 years due to antimicrobial-resistant infections, and such infections could cause up to US$3.5 billion in additional health care costs per year7, 8. As AMR becomes a threat worldwide, both economically and to public health9,10,11,12,13, there is an urgent need to develop a preclinical tool for efficient prediction of AMR.One conventional strategy for identifying genetically-encoded mechanisms for AMR involves sequence assembly14,15,16,17 and read-based techniques18,19,20 that map sequence data directly to reference databases. Although these methods perform well for known and highly conserved AMR genes, they may produce an unacceptable number of false positives (genes predicted to encode resistance when they do not) for highly dissimilar sequences as was demonstrated previously for Gram-negative bacteria21. Machine-learning techniques can be applied as an alternative solution for predicting putative AMR genes. Rather than using sequence similarity, a machine-learning model detects features, i.e., characteristics of a protein sequence, that are unique to AMR genes. Several machine-learning methods have been proposed to identify novel AMR genes from metagenomic and pan-genome data12, 22, 23, but these methods used a small number of genetic features for predictions. Moreover, these approaches did not use a feature-selection strategy to remove irrelevant and redundant features that might compromise the accuracy of a machine-learning model.We recently introduced a game-theory-based feature selection approach (“game theoretic dynamic weighting based feature evaluation”, or GTDWFE) predicated on the supposition that a single feature might provide limited predictive value, but that it might contribute to form a strong coalition when used with other features21. We applied our feature selection approach in Gram-negative bacteria and obtained prediction accuracies ranging from 93% to 99% for prediction of genes that encode resistance to acetyltransferase (aac), \(\beta \)-lactamase (bla), and dihydrofolate reductase (dfr). In our current study, we test the GTDWFE algorithm with data from Gram-positive bacteria. We then combine the results for both studies and introduce “Prediction of Antimicrobial Resistance via Game Theory” (PARGT), a software program with a graphical-user interface (GUI) that is designed to identify antimicrobial-resistance genes for both Gram-positive and -negative bacteria.A major objective was to develop a software tool with a simple and intuitive GUI that is capable of extracting protein features without the need for manual curation and then use these features to identify putative AMR genes. PARGT integrates all of the tools and scripts required to identify protein features and to automatically generate feature subsets obtained via the GTDWFE algorithm. PARGT can be used with the Windows, Linux, or macOS, operating systems, and it provides options for predicting bac and van resistance genes in any Gram-positive bacteria and aac, bla, and dfr resistance genes in any Gram-negative bacteria. Users can test a single sequence or an entire genome for these genes. In addition, PARGT allows users to add newly confirmed AMR or non-AMR sequences to the training set as well as to reset the training data back to the original training set downloaded with the tool.
Results|Validation of PARGTWe validated the GTDWFE algorithm for feature selection as implemented previously21. In our earlier work, we considered the AMR (positive) and non-AMR (negative) amino-acid sequences of aac, bla, and dfr for Acinetobacter, Klebsiella, Campylobacter, Salmonella, and Escherichia as training datasets and tested our trained support vector machine (SVM)24, 25 machine-learning model with sequences from Pseudomonas, Vibrio, and Enterobacter. The combination of GTDWFE and SVM resulted in correct classification rates of 93%, 99%, and 97% for aac, bla, and dfr, respectively. This demonstrated that our approach was promising and that the GTDWFE algorithm is capable of identifying the most relevant, non-redundant, and interdependent features necessary for accurate prediction.In this paper we consider validation of our GTDWFE model for AMR proteins in Gram-positive bacteria. We use the unique AMR and non-AMR sequences available for bac and van from the Gram-positive bacteria Clostridium spp. and Enterococcus spp. as the training datasets for our SVM model. These training datasets are used to generate the best feature subsets by means of the GTDWFE approach. The training datasets contain 25 and 52 AMR (positive) examples for bac and van, respectively. A total of 52 non-AMR examples are considered as negative samples for each of the training datasets. In the GTDWFE approach, we select features based on the relevance, non-redundancy, and interdependency values of all features. For this analysis, we need to set an interdependent group size \(\delta \) to measure the interdependency between features, where \(\delta \) is used in the computation of the Banzhaf power index26 and indicates the size of each feature group. We selected a value of \(\delta =3\) based on previous work21 where we found that an interdependent group size of 3 was sufficient to identify best feature subsets from training datasets. We then test our trained model with known AMR and non-AMR samples from Staphylococcus, Streptococcus, and Listeria. The test datasets contain 6 and 9 AMR (positive) sequences for bac and van, respectively, and 14 non-AMR (negative) sequences are used for each test dataset.Table 1 Predicted bac AMR sequences for Staphylococcus, Streptococcus, and Listeria using the GTDWFE algorithm.Full size table Table 2 Predicted van AMR sequences for Staphylococcus, Streptococcus, and Listeria using the GTDWFE algorithm.Full size table Tables 1 and 2 list the predicted bac and van AMR sequences from our test datasets, respectively. In each table, we provide the NCBI accession number27 for each protein sequence together with its name, and we note whether an AMR protein was correctly classified as AMR (true positive) or a non-AMR sequence was incorrectly classified as AMR (false positive). The GTDWFE algorithm successfully identified all six bac AMR genes (true positives). However, it missclassified 2 of the 14 non-AMR sequences as AMR (false positives). Therefore, the number of true positives, true negatives (negatives accurately classified), false positives, and false negatives (positives classified as negatives) for bac are 6, 12, 2, and 0, respectively, and the sensitivity, specificity, and accuracy for bac are 100%, 86%, and 90%, respectively. As shown in Table 2 for van, 8 of 9 AMR sequences were correctly classified as AMR (true positives) whereas 2 of 14 non-AMR sequences were classified as AMR (false positives). Therefore, the number of true positives, true negatives, false positives, and false negatives for van are 8, 12, 2, and 1, respectively, and the sensitivity, specificity, and accuracy for van are 89%, 86%, and 87%, respectively. Note that the two tables contain one hypothetical protein and one putative uncharacterized protein. We have categorized these two proteins as false positives because they were identified as essential (non-AMR) genes in the Pathosystems Resource Integration Center (PATRIC)28, 29. However, it is quite possible that PARGT correctly identified them as AMR proteins given the number of annotation errors in public databases30. CDC71755 is from a Staphylococcus organism identified from a metagenome sequence, and EUJ19660 is from a Listeria aquatica organism obtained from an environmental water sample.Performance comparison with BLASTp and Kalign toolsWe also compared the performance of our GTDWFE algorithm with BLASTp (https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE=Proteins) and Kalign31 (https://www.ebi.ac.uk/Tools/msa/kalign/) results using default parameter settings. The outcomes shown in Supplementary Table S1 are the percent identities for bac AMR and non-AMR samples from Staphylococcus, Streptococcus, and Listeria with respect to the bac AMR samples of Clostridium and Enterococcus. A percent identity for BLASTp and Kalign as low as 38.13% and 46.19%, respectively, are needed to identify all the bac AMR sequences; however, these low percent identities lead to 6 and 3 of 14 false positives for BLASTp and Kalign, respectively, in which non-AMR sequences are miscategorized. Therefore, the low percent identities for BLASTp and Kalign required to identify all AMR sequences increase the number of false positives for a set of diverse AMR sequences. In Supplementary Table S2 we show that the performances of BLASTp and Kalign when identifying van AMR sequences are actually better than that of the GTDWFE algorithm. This is due to the very high similarity (>98.5% identity) between the training AMR and test AMR datasets for van. When training and test sets share high similarity, BLASTp and Kalign are guaranteed to give good results. However, as in the case of bac for which the training and test AMR data similarity ranges between 38.13% and 41.01%, BLASTp does not perform well. For Kalign, the similarity ranges between 46.19% and 49.17% so that it performs better than BLASTp. However, the GTDWFE algorithm will outperform both BLASTp and Kalign because it does not use sequence similarity but rather protein features for prediction. BLASTp and Kalign do not predict; they match sequence similarity.
Discussion|In this work, we implemented a software package PARGT and extended our earlier work of identifying AMR genes in Gram-negative to Gram-positive bacteria. PARGT integrates the required software tools and scripts needed to generate all protein features automatically, and it performs predictions on user-inputted sequences. Moreover, users can update PARGT by including their own known AMR and non-AMR sequences to train the machine-learning model to potentially improve prediction accuracy. As our previous work described the experimental results for Gram-negative bacteria, in this paper we only included prediction results for Gram-positive bacteria. Simulation results showed that PARGT can predict AMR sequences for Gram-positive bacteria with accuracy ranging from 87% to 90%. PARGT gave better results for bac due to the diversity of sequences available, but BLASTp and Kalign exhibited better performance in the case of van because of the high similarity of sequences. To generate evolutionary and secondary structure features, we used the Uniprot database (containing 538,585 FASTA sequences) as our reference database for relatively fast execution; however, more accurate values for these features can be obtained using large-scale protein databases such as UniRef90 or UniRef100 (http://www.uniprot.org/help/uniref) as target/reference databases. Note, however, that there is a trade-off between accuracy and computational time when using a large-scale reference database to generate features. A parallel version of PARGT would reduce the execution time of the tool for and ameliorate the use of large-scale reference databases.
Methods|GTDWFE algorithm for feature selectionFeature collection, feature extraction, calculation of feature values, and feature selection using the GTDWFE algorithm are explained in detail in previous works21, 32. Briefly, a total of 621D candidate features were collected by means of a thorough literature search, where D stands for dimension (some features are single values, i.e., 1D, while others are vector values, e.g., 20D for the 20 different amino acids). We extracted all 621D features from both our positive (AMR) and negative (non-AMR) datasets and calculated their values. The GTDWFE algorithm was then used to select features for use in our machine-learning model. The GTDWFE selects the best feature at each iteration based on the relevance, non-redundancy, and interdependency values of all features. Initially, the weights of all features are the same i.e., 1. The relevance of a feature to the target class (AMR or non-AMR) and the distance of the feature to other features are calculated using Pearson’s correlation coefficient and the Tanimoto coefficient, respectively. These calculations are performed for all features, and the feature with the highest summation of relevance and distance is chosen as the initial selected feature. The Banzhaf power index26 is then calculated to estimate the interdependency between the selected feature and the remaining features. We measure the contribution of each feature when it forms a group with other features, and the conditional mutual information is calculated to find the Banzhaf power index of the features. The weight of each remaining feature is updated by adding the product of the current weight and the Banzhaf power index to the feature selected previously. In other words, at each step, we readjust the weight of the remaining features dynamically based on the features selected in earlier steps. Thus, the weight of a candidate feature actually corresponds to the interdependence values with the earlier selected features. The feature with the highest summation of relevance and distance values multiplied by the revised weight is chosen as the next selected feature. This process is repeated until the desired number of features has been reached.Machine-learning algorithmAfter identifying the best feature subset for use with our classifier by means of the GTDWFE algorithm, we trained an SVM machine-learning model using this feature subset. This binary classifier was then used for prediction. As was true for our previous work, in PARGT we tuned the SVM using the training datasets and chose the best SVM model to predict the AMR proteins in the test sequences. We considered 10-fold cross validation to tune the SVM model. The SVM model with a radial basis function (RBF) kernel and a cost value of 4 was identified as the best model for both bac and van training datasets. For the SVM, the RBF is used as a function in the kernel trick to implicitly transform the original space of the data to a high-dimensional space to make the data samples linearly separable, and the cost parameter is used to regulate the classification error.Figure 1The components of PARGT. Components outlined by dotted lines indicate additional training samples supplied by a user.Full size image Overview of PARGT softwarePARGT is an open-source software package designed and implemented for predicting antimicrobial resistance genes in bacteria. PARGT is written using both Python 3 and R. R scripts were written to identify physicochemical and secondary structure features and for machine-learning modeling, and Python 3 was used to run the R scripts, to generate position-specific scoring matrix (PSSM) features, and to implement the GUI. PARGT weight the importance of protein features based on their contributions during classification. All the required bioinformatics tools33,34,35,36,37,38,39 and scripts necessary to generate the protein features required in our machine-learning model are included in PARGT. PARGT uses the best feature subset identified by our GTDWFE algorithm to make predictions. It allows users to add new AMR and non-AMR sequences to the training datasets, and the software automatically updates the machine-learning model with the additional sequences, potentially resulting in an increase in the accuracy of the model. To minimize execution time, PARGT uses the UniProt database containing 538,585 protein sequences as a reference database, rather than a larger database, for generating PSSM and secondary structure features.Architecture of PARGTFigures 1 and 2 depict the architecture and GUI for PARGT, respectively. PARGT allows a user to input a set of known AMR and non-AMR sequences to use in the training dataset, generating all required feature values for these sequences automatically. As shown in Fig. 1, the 20D amino acid composition feature vector, 168D feature vector based on the composition, transition and distribution (CTD) model40, 41, 400D feature vector based on the PSSM, and 33D feature vector based on the secondary structure sequence and secondary structure probability matrix are generated from the input protein sequences. Then the best feature subset is constructed using our GTDWFE feature selection algorithm. An SVM is used as the machine-learning model that is trained using the selected feature set. Recall that the SVM model used for PARGT is automatically tuned during the training phase. Finally, the trained SVM model is applied to predict AMR sequences from the test dataset.As shown in Fig. 2, PARGT provides the option of predicting aac, bla, and dfr resistance genes for Gram-negative bacteria and bac and van resistance genes for Gram-positive bacteria. A user must select the appropriate option for predicting AMR from the GUI menu and also supply the test file for the set of protein sequences in FASTA format that they wish to have classified as AMR or non-AMR. PARGT automatically computes all the required feature values for the test sequences, and it provides an output file containing the set of predicted AMR sequences for the user’s test file. If a user wants to include new known AMR or non-AMR sequences to augment the training datasets, PARGT provides an option to do so for the five above-mentioned resistance classes. In addition, it provides the option of restoring the original training datasets in case a user decides they prefer to use them or or else wants to compare predictions using two different sets of training data.Figure 2Illustration of the PARGT GUI with its pop-up menu.Full size image DatasetsWe retrieved protein sequences for AMR genes from the Antibiotic Resistance Genes Database (ARDB)42, and non-AMR sequences were obtained from the PATRIC28, 29. Initially, we gathered 124 bac and 374 van AMR sequences for the Gram-positive bacteria Clostridium spp. and Enterococcus spp., and we randomly chose 52 essential protein sequences to use as non-AMR sequences. As many of the protein sequences were duplicates, CD-HIT43, 44 was applied to find unique sequences. A sequence identity of \(\ge \) 90% was used as a threshold for removing duplicate sequences. After eliminating redundant protein sequences, our final counts were 25 bac and 52 van AMR sequences; none of the 52 non-AMR sequences were duplicates. We used this dataset to train our machine-learning model. In addition to the training dataset, we also gathered 102 bac and 22 van AMR sequences and 14 non-AMR sequences for the Gram-positive bacteria Staphylococcus spp., Streptococcus spp., and Listeria spp. from the data sources indicated above. We again applied CD-HIT to this dataset, and after the removal of duplicate sequences, 6 bac and 9 van AMR sequences and 14 non-AMR sequences remained. We used these as our test dataset to measure the accuracy of the classifier. The sequence identity of protein sequences could be as low as 10%. After validating our GTDWFE algorithm with the training and test sequences for the bac and van AMR classes, we again trained our classifier, but we used the sequences from all five bacterial genera, i.e., both training and test sequences, to potentially increase the accuracy of PARGT. The same retraining was also performed for our Gram-negative bacteria.
Abstract|The density structure of the interstellar medium determines where stars form and release energy, momentum and heavy elements, driving galaxy evolution1,2,3,4. Density variations are seeded and amplified by gas motion, but the exact nature of this motion is unknown across spatial scales and galactic environments5. Although dense star-forming gas probably emerges from a combination of instabilities6,7, convergent flows8 and turbulence9, establishing the precise origin is challenging because it requires gas motion to be quantified over many orders of magnitude in spatial scale. Here we measure10,11,12 the motion of molecular gas in the Milky Way and in nearby galaxy NGC 4321, assembling observations that span a spatial dynamic range 10−1–103 pc. We detect ubiquitous velocity fluctuations across all spatial scales and galactic environments. Statistical analysis of these fluctuations indicates how star-forming gas is assembled. We discover oscillatory gas flows with wavelengths ranging from 0.3–400 pc. These flows are coupled to regularly spaced density enhancements that probably form via gravitational instabilities13,14. We also identify stochastic and scale-free velocity and density fluctuations, consistent with the structure generated in turbulent flows9. Our results demonstrate that the structure of the interstellar medium cannot be considered in isolation. Instead, its formation and evolution are controlled by nested, interdependent flows of matter covering many orders of magnitude in spatial scale.