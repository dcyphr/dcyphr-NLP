{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PERSONAL_PEGASUS",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcyphr/dcyphr-NLP/blob/master/PERSONAL_PEGASUS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYdH6K2nYSIX",
        "colab_type": "text"
      },
      "source": [
        "SETUP AND INSTALLATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba8fLBipjTjB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "6eb260ad-d885-4c3a-c234-d7f9a5d8849a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Aug 29 22:30:15 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO2K5NsH__Gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "004fabd7-d9aa-4c90-cd9f-5eb9ccc3d26e"
      },
      "source": [
        "%%bash\n",
        "cd /content/\n",
        "git clone https://github.com/dcyphr/dcyphr-NLP.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'dcyphr-NLP' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5J70LqGC8UG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c2433373-b81c-4ea8-d109-843da3a16b27"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!pip install -q -e .\n",
        "%cd examples\n",
        "!pip install -U -q -r requirements.txt\n",
        "!pip install -U pyarrow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 39749 (delta 5), reused 7 (delta 2), pack-reused 39728\u001b[K\n",
            "Receiving objects: 100% (39749/39749), 28.57 MiB | 28.05 MiB/s, done.\n",
            "Resolving deltas: 100% (27591/27591), done.\n",
            "/content/transformers/examples/seq2seq/transformers/examples/transformers\n",
            "/content/transformers/examples/seq2seq/transformers/examples/transformers/examples\n",
            "Requirement already up-to-date: pyarrow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU8j03AAC-no",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "848ce435-6962-41ce-ca90-7246bf918570"
      },
      "source": [
        "%cd /content/transformers/examples/seq2seq/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers/examples/seq2seq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gg53EbEYXy-",
        "colab_type": "text"
      },
      "source": [
        "This installs the XSUM Dataset and is unneccesary if you are working with your own dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME7pQTdnjzCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "f3a67dad-f6a1-4508-9722-3206de148aca"
      },
      "source": [
        "!wget https://s3.amazonaws.com/datasets.huggingface.co/summarization/xsum.tar.gz\n",
        "!tar -xzvf xsum.tar.gz\n",
        "!export XSUM_DIR=${PWD}/xsum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-29 22:30:39--  https://s3.amazonaws.com/datasets.huggingface.co/summarization/xsum.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.30.38\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.30.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 204844092 (195M) [application/x-tar]\n",
            "Saving to: ‘xsum.tar.gz.1’\n",
            "\n",
            "xsum.tar.gz.1       100%[===================>] 195.35M  48.7MB/s    in 4.4s    \n",
            "\n",
            "2020-08-29 22:30:43 (44.3 MB/s) - ‘xsum.tar.gz.1’ saved [204844092/204844092]\n",
            "\n",
            "xsum/\n",
            "xsum/train.target\n",
            "xsum/train.source\n",
            "xsum/val.source\n",
            "xsum/val.target\n",
            "xsum/test.source\n",
            "xsum/test.target\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf_whcahYlWN",
        "colab_type": "text"
      },
      "source": [
        "This Finetunes the PEGASUS Model for One epoch on the Dcyphr fine-tuning data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDvxD4sQDDqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "f02c9638-00ad-487e-b854-411f87a6d1b0"
      },
      "source": [
        "!bash finetune_pegasus_xsum.sh \\\n",
        "  --model_name_or_path google/pegasus-xsum \\\n",
        "  --data_dir /content/dcyphr-NLP/fine_tuning_data \\\n",
        "  --output_dir xsum_pegasus_test_5 \\\n",
        "  --train_batch_size 2 \\\n",
        "  --eval_batch_size 2 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --n_train 100 \\\n",
        "  --n_val 100 \\\n",
        "  --sortish_sampler \\\n",
        "  --gpus 1 \\\n",
        "  --val_check_interval 0.25 \\\n",
        "  --gradient_accumulation_steps 4 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version 1.6.0+cu101 available.\n",
            "2020-08-29 22:30:51.548899: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "TensorFlow version 2.3.0 available.\n",
            "Traceback (most recent call last):\n",
            "  File \"finetune.py\", line 394, in <module>\n",
            "    main(args)\n",
            "  File \"finetune.py\", line 332, in main\n",
            "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
            "ValueError: Output directory (xsum_pegasus_test_5) already exists and is not empty.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJGRWhbQy14N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "mkdir gens\n",
        "export DATA_DIR=xsum\n",
        "python run_eval.py google/pegasus-xsum \\\n",
        "    $DATA_DIR/test.source gens/peg_xsum_test_generation.txt \\\n",
        "    --reference_path $DATA_DIR/test.target \\\n",
        "    --score_path gens/peg_xsum_rouge.txt --task summarization \\\n",
        "    --device cuda \\\n",
        "    --bs 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5pAV-OBYyzX",
        "colab_type": "text"
      },
      "source": [
        "**Training on Dcyphr Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjvoR88sZXjY",
        "colab_type": "text"
      },
      "source": [
        "Command to Finetune on the dcyphr dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0rUC-QMkJaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "export DATA_DIR=/content/dcyphr-NLP/fine_tuning_data\n",
        "python run_eval.py sshleifer/distilbart-cnn-12-6 $DATA_DIR/val.source dbart_val_generations.txt \\\n",
        "    --reference_path $DATA_DIR/val.target \\\n",
        "    --score_path cnn_rouge.json \\\n",
        "    --task summarization \\\n",
        "    --n_obs 100 \\\n",
        "    --device cuda \\\n",
        "    --bs 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8oiUsm8SgPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk_wTV6ZA2oZ",
        "colab_type": "text"
      },
      "source": [
        "Okay so the output of the data can be found at transformers/examples/seq2seq/dbart_val_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPegKUoTLnkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "971e48fd-5a53-4aa8-e4a4-431989d2839f"
      },
      "source": [
        "output_dir=output_dir= 'transformers/examples/seq2seq/xsum_pegasus_test_5'\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(f'{output_dir}/best_tfmr')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version 1.6.0+cu101 available.\n",
            "TensorFlow version 2.3.0 available.\n",
            "loading configuration file transformers/examples/seq2seq/xsum_pegasus_test_5/best_tfmr/config.json\n",
            "Model config PegasusConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"architectures\": [\n",
            "    \"PegasusForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 16,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 16,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"extra_pos_embeddings\": 1,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 64,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"pegasus\",\n",
            "  \"normalize_before\": true,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 8,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"save_step\": 2,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"vocab_size\": 96103\n",
            "}\n",
            "\n",
            "loading weights file transformers/examples/seq2seq/xsum_pegasus_test_5/best_tfmr/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n",
            "\n",
            "All the weights of PegasusForConditionalGeneration were initialized from the model checkpoint at transformers/examples/seq2seq/xsum_pegasus_test_5/best_tfmr.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use PegasusForConditionalGeneration for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPH5xnGqYAxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from transformers import BartTokenizer, BartModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuquRsz1PApS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "3630d200-fa5f-4292-b0fa-3ed6a626b9d2"
      },
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
        "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n",
        "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp4lgyibjs\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json in cache at /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "creating metadata file for /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp7dboqvju\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt in cache at /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "creating metadata file for /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[' IP sides statute']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6tD5xOAYcLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}