
Abstract
The ongoing outbreak of coronavirus disease 2019 (COVID-19) has spread rapidly on a global scale. Although it is clear that severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is transmitted through human respiratory droplets and direct contact, the potential for aerosol transmission is poorly understood1,2,3. Here we investigated the aerodynamic nature of SARS-CoV-2 by measuring viral RNA in aerosols in different areas of two Wuhan hospitals during the outbreak of COVID-19 in February and March 2020. The concentration of SARS-CoV-2 RNA in aerosols that was detected in isolation wards and ventilated patient rooms was very low, but it was higher in the toilet areas used by the patients. Levels of airborne SARS-CoV-2 RNA in the most public areas was undetectable, except in two areas that were prone to crowding; this increase was possibly due to individuals infected with SARS-CoV-2 in the crowd. We found that some medical staff areas initially had high concentrations of viral RNA with aerosol size distributions that showed peaks in the submicrometre and/or supermicrometre regions; however, these levels were reduced to undetectable levels after implementation of rigorous sanitization procedures. Although we have not established the infectivity of the virus detected in these hospital areas, we propose that SARS-CoV-2 may have the potential to be transmitted through aerosols. Our results indicate that room ventilation, open space, sanitization of protective apparel, and proper use and disinfection of toilet areas can effectively limit the concentration of SARS-CoV-2 RNA in aerosols. Future work should explore the infectivity of aerosolized virus.
Methods
Data reportingNo statistical methods were used to predetermine sample size. The experiments were not randomized and the investigators were not blinded to allocation during experiments and outcome assessment.Sample collectionThe sampling was conducted between 17 February and 2 March 2020 in the locations in two rounds as shown in Table 1. All aerosol samples were collected on presterilized gelatin filters (Sartorius). A total of 30 aerosol samples of total suspended particles were collected on 25-mm-diameter filters loaded into styrene filter cassettes (SKC) by sampling air at a fixed flow rate of 5.0 l min−1 using a portable pump (APEX2, Casella). A total of three size-segregated aerosol samples was collected using a miniature cascade impactor (Sioutas Impactor, SKC) that separated aerosols into five ranges (>2.5 μm, 1.0–2.5 μm, 0.50–1.0 μm and 0.25–0.50 μm on 25-mm filter substrates, and 0–0.25 μm on 37-mm filters) at a flow rate of 9.0 l min−1. A total of two aerosol deposition samples was collected using 80-mm-diameter filters packed into a holder with an effective deposition area of 43.0 cm2 and the filters were placed intact on the floor in two corners of the intensive care unit room of Renmin Hospital for 7 days. Sampling durations and operation periods are described in Supplementary Table 1. All sampling instruments were located in the centre of the respective sampling area, where the sampling inlet was at a height of 1.5 m from the floor. Considering the limited experimental conditions and the small sample size, the integrity and robustness of the experiment protocol was examined extensively in the laboratory before field sampling and these results are described in Supplementary Table 2.Analytical method and data analysisAfter the collection of aerosol samples, all samples were processed immediately in the BSL-2 laboratory of Wuhan University. The 25-, 37-mm and 80-mm filter samples were dissolved in deionized water, after which TRIzol LS reagent (Invitrogen) was added to inactivate SARS-CoV-2 viruses and extract RNA according to the manufacturer’s instructions. First-strand cDNA was synthesized using the PrimeScript RT kit (TakaRa). Optimized ddPCR was used to detect the presence of SARS-CoV-2 viruses according to a previous study10. Analysis of the ddPCR data was performed using QuantaSoft software (Bio-Rad). The concentration reported by the procedure equals the number of copies of template per microlitre of the final 1× ddPCR reaction, which was normalized to copies m−3 in all of the results; therefore, the virus or viral RNA concentration in aerosol is expressed in copies m−3 throughout. A detailed protocol is provided in the Supplementary Information.Reporting summaryFurther information on research design is available in the Nature Research Reporting Summary linked to this paper.
Abstract
Carotenoids are colored compounds produced by plants, fungi, and microorganisms and are required in the diet of most animals for oxidation control or light detection. Pea aphids display a red-green color polymorphism, which influences their susceptibility to natural enemies, and the carotenoid torulene occurs only in red individuals. Unexpectedly, we found that the aphid genome itself encodes multiple enzymes for carotenoid biosynthesis. Phylogenetic analyses show that these aphid genes are derived from fungal genes, which have been integrated into the genome and duplicated. Red individuals have a 30-kilobase region, encoding a single carotenoid desaturase that is absent from green individuals. A mutation causing an amino acid replacement in this desaturase results in loss of torulene and of red body color. Thus, aphids are animals that make their own carotenoids.
Abstract
The development of the body plan in the Drosophila embryo depends on the activity of maternal determinants localized at the anterior and posterior of the egg. These activities define both the polarity of the anterior-posterior (AP) axis and the spatial domains of expression of the zygotic gap genes, which in turn control the subsequent steps in segmentation. The nature and mode of action of one anterior determinant, the bicoid(bcd) gene product, has recently been defined, but the posterior determinants are less well characterized. At least seven maternally acting genes are required for posterior development. Mutations in these maternal posterior-group genes result in embryos lacking all abdominal segments. Cytoplasmic transplantation studies indicate that the maternally encoded product of the nanos(nos) gene may act as an abdominal determinant, whereas the other maternal posterior-group genes appear to be required for the appropriate localization and stabilization of this signal. Here we show that the lack of the nos gene product can be compensated for by eliminating the maternal activity of the gap gene hunchback (hb). Embryos lacking both of these maternally derived gene products are viable and can survive as fertile adults. These results suggest that the nos gene product functions by repressing the activity of the maternal hb products in the posterior of the egg.
Abstract
Somatic mutations in cancer genomes are caused by multiple mutational processes, each of which generates a characteristic mutational signature1. Here, as part of the Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium2 of the International Cancer Genome Consortium (ICGC) and The Cancer Genome Atlas (TCGA), we characterized mutational signatures using 84,729,690 somatic mutations from 4,645 whole-genome and 19,184 exome sequences that encompass most types of cancer. We identified 49 single-base-substitution, 11 doublet-base-substitution, 4 clustered-base-substitution and 17 small insertion-and-deletion signatures. The substantial size of our dataset, compared with previous analyses3,4,5,6,7,8,9,10,11,12,13,14,15, enabled the discovery of new signatures, the separation of overlapping signatures and the decomposition of signatures into components that may represent associated—but distinct—DNA damage, repair and/or replication mechanisms. By estimating the contribution of each signature to the mutational catalogues of individual cancer genomes, we revealed associations of signatures to exogenous or endogenous exposures, as well as to defective DNA-maintenance processes. However, many signatures are of unknown cause. This analysis provides a systematic perspective on the repertoire of mutational processes that contribute to the development of human cancer.
Discussion
There are important constraints, limitations and assumptions in the analytic frameworks used here to characterize mutational signatures. Signatures extracted from sample sets in which multiple processes are operative remain mathematical approximations, with profiles that are potentially influenced by the mathematical approach used and other factors. For conceptual and practical simplicity, we assume that a single signature is associated with each mutational process and provide an average reference signature to represent it. However, we do not discount the possibility that further nuances and variations of signature profiles exist. We have estimated the contributions from each signature to the mutation burden in each sample. However, with increasing numbers of signatures and differences of multiple orders of magnitude in mutation burdens between some signatures, prior knowledge has helped to avoid biologically implausible results. Thus, the further development of methods for deciphering and attributing mutational signatures is warranted, ideally supported by signatures derived from experimental systems in which the causes are known. Nevertheless, signatures with many similarities and some differences can be found by different mathematical approaches, and these can be confirmed in several ways, including experimentally elucidated signatures5,31,39,42,43,54,55,56,57,58,59,60,61,62 and tumours dominated by a single signature (syn12016215).This analysis includes most publicly available exome and whole-genome cancer sequences. Some rare or geographically restricted signatures may not have been captured, signatures conferring limited mutation burdens may have been missed and signatures of therapeutic mutagenic exposures have not been exhaustively explored. Nevertheless, it is likely that a substantial proportion of the naturally occurring mutational signatures found in human cancer have now been described. This comprehensive repertoire provides a foundation for research into the aetiologies of geographical and temporal differences in cancer incidence, the mutational processes that operate in healthy tissues and non-neoplastic disease states, clinical and public health applications of signatures and mechanistic understanding of the mutational processes that underlie carcinogenesis.
Methods
No statistical methods were used to predetermine sample size. The experiments were not randomized and investigators were not blinded to allocation during experiments and outcome assessment.These online methods contain an abridged description of the methodology used in the current manuscript; extensive details about the methodology we used are provided in Supplementary Note 2. Importantly, two independently developed computational frameworks (SigProfiler and SignatureAnalyzer) based on NMF were applied separately to the examined sets of mutational catalogues. SigProfiler and SignatureAnalyzer take different approaches for deciphering mutational signatures and for assigning each signature to each sample. By using two methods, we aimed to provide a perspective on the effect that different methodologies can have on the numbers of signatures generated, signature profiles and attributions. In addition to applying SigProfiler and SignatureAnalyzer to cancer data, the tools were also applied to realistic synthetic data with known solutions.Analysis of mutational signatures with SigProfilerSigProfiler incorporates two distinct steps for identification of mutational signatures, based on the previously described methodology6,11,17 (Extended Data Fig. 8). The first step (SigProfilerExtraction) encompasses a hierarchical de novo extraction of mutational signatures based on somatic mutations and their immediate sequence context, and the second step (SigProfilerAttribution) focuses on accurately estimating the number of somatic mutations associated with each extracted mutational signature in each sample. SigProfilerExtraction is an extension of a previous framework for the analysis of mutational signatures11,17. In brief, for a given set of mutational catalogues, the algorithm deciphers a minimal set of mutational signatures that optimally explains the proportion of each mutation type and estimates the contribution of each signature to each sample. More specifically, for each NMF iteration, SigProfilerExtraction minimizes a generalized Kullback–Leibler divergence constrained for nonnegativity (Supplementary Note 2). The algorithm uses multiple NMF iterations (in most cases 1,024) to identify the matrix of mutational signatures and the matrix of the activities of these signatures, as previously described17. The unknown number of signatures is determined by human assessment of the stability and accuracy of solutions for a range of values, as previously described17. The framework is applied hierarchically to increase its ability to find mutational signatures that generate few mutations or are present in few samples.After signatures are discovered by SigProfilerExtraction, SigProfilerAttribution estimates their contributions to individual samples. For each examined sample, the estimation algorithm involves finding the minimum of the Frobenius norm of a constrained function using a nonlinear convex optimization programming solver using the interior-point algorithm63. See Supplementary Note 2 and Extended Data Fig. 8b for further details.Analysis of mutational signatures with SignatureAnalyzerSignatureAnalyzer uses a Bayesian variant of NMF that infers the number of signatures through the automatic relevance determination technique and delivers highly interpretable and sparse representations for both signature profiles and attributions that strike a balance between data fitting and model complexity. Further details of the actual implementation of the computational approach have previously been published9,27,64. SignatureAnalyzer was applied by using a two-step signature extraction strategy using 1,536 pentanucleotide contexts for SBSs, 83 indel features and 78 DBS features. In addition to the separate extraction of SBS, indel and DBS signatures, we performed a ‘COMPOSITE’ signature extraction based on all 1,697 features (1,536 SBS + 78 DBS + 83 indel). For SBSs, the 1,536 SBS COMPOSITE signatures are preferred; for DBSs and indels, the separately extracted signatures are preferred.In step 1 of the two-step extraction process, global signature extraction was performed for the samples with a low mutation burden (n = 2,624). These excluded hypermutated tumours: those with putative polymerase epsilon (POLE) defects or mismatch repair defects (microsatellite instable tumours), skin tumours (which had intense UV-light mutagenesis) and one tumour with temozolomide (TMZ) exposure. Because the underlying algorithm of SignatureAnalyzer performs a stochastic search, different runs can produce different results. In step 1, we ran SignatureAnalyzer 10 times and selected the solution with the highest posterior probability. In step 2, additional signatures unique to hypermutated samples were extracted (again selecting the highest posterior probability over ten runs) while allowing all signatures found in the samples with low mutation burden, to explain some of the spectra of hypermutated samples. This approach was designed to minimize a well-known ‘signature bleeding’ effect or a bias of hyper- or ultramutated samples on the signature extraction. In addition, this approach provided information about which signatures are unique to the hypermutated samples, which was later used when attributing signatures to samples.A similar strategy was used for signature attribution: we performed a separate attribution process for low- and hypermutated samples in all COMPOSITE, SBS, DBS and indel signatures. For downstream analyses, we preferred to use the COMPOSITE attributions for SBSs and the separately calculated attributions for DBSs and indels. Signature attribution in samples with a low mutation burden was performed separately in each tumour type (for example, Biliary–AdenoCA, Bladder–TCC, Bone–Osteosarc, and so on). Attribution was also performed separately in the combined microsatellite instable tumours (n = 39), POLE (n = 9), skin melanoma (n = 107) and TMZ-exposed samples (syn11738314). In both groups, signature availability (which signatures were active, or not) was primarily inferred through the automatic relevance determination process applied to the activity matrix H only, while fixing the signature matrix W. The attribution in samples with a low mutation burden was performed using only signatures found in the step 1 of the signature extraction. Two additional rules were applied in SBS signature attribution to enforce biological plausibility and minimize a signature bleeding: (i) allow SBS4 (smoking signature) only in lung, head and neck cases; and (ii) allow SBS11 (TMZ signature) in a single GBM sample. This was enforced by introducing a binary, signature-by-sample signature indicator matrix Z (1, allowed; 0, not allowed), which was multiplied by the H matrix in every multiplication update of H. No additional rules were applied to indel or DBS signature attributions, except that signatures found in hypermutated samples were not allowed in samples with a low mutation burden.Application of SigProfiler and SignatureAnalyzer to synthetic dataOur goal was to evaluate SignatureAnalyzer and SigProfiler on realistic synthetic data to identify any potential limitations of these two methods. SignatureAnalyzer and SigProfiler were tested on 11 sets of synthetic data, encompassing a total of 64,400 synthetic samples, in which known signature profiles were used to generate catalogues of synthetic mutational spectra. We operationally defined ‘realistic’ data as those based on the characteristics of either SignatureAnalyzer’s or SigProfiler’s analysis of the PCAWG genome data. SignatureAnalyzer’s reference signature profiles were based on COMPOSITE signatures, consisting of 1,536 types of strand-agnostic SBSs in pentanucleotide context, 78 types of DBSs and 83 types of small indels, for a total of 1,697 mutation types. SigProfiler’s reference analysis was based on strand-agnostic SBSs in the context of one 5′ and one 3′ base. For each test, we generated two sets of realistic data: SigProfiler-realistic (based on SigProfiler’s reference signatures and attributions) and SignatureAnalyzer-realistic (based on SignatureAnalyzer’s reference signatures and attributions), as well as two other types of data that involved using SignatureAnalyzer profiles with SigProfiler attributions and vice versa. A detailed description of each of the 11 sets of synthetic data and the results from applying SigProfiler and SignatureAnalyzer are provided in Supplementary Note 2.Analysis of clustered mutational signaturesSomatic SBSs were considered clustered if they had intermutational distances < 1,000 bp. More specifically, for each sample, an SBS mutational catalogue was generated for substitutions that were <1,000 bp from another substitution. Subsequently, the set of SBS mutational catalogues containing clustered mutations underwent de novo extraction of mutational signatures. Any novel mutational signature (one that was not previously observed in the complete SBS catalogues) was reported as a clustered mutational signature.Better separation compared to COSMIC v.2 signaturesAs described in the manuscript, all mutational signatures previously reported in COSMIC v.2 were confirmed in the new set of analyses with median cosine similarity of 0.95. However, the separation between the COSMIC v.2 mutational signatures (https://cancer.sanger.ac.uk/cosmic/signatures_v2) is much worse than the separation between the mutational signatures reported here. For example, in COSMIC v.2, signatures 5 and 16 had a cosine similarity of 0.90, making them hard to distinguish from one another. By contrast, in the current analysis, SBS5 and SBS16 have a cosine similarity of 0.65. This allows us to unambiguously assign SBS5 and SBS16 to different samples. In the current analysis, the larger number of samples has allowed the reduction of bleeding between signatures and has given more unique and easily distinguishable signatures. One can evaluate the overall separation of a set of mutational signatures by examining the distribution of cosine similarities between the signatures in the set. The signatures in COSMIC v.2 had a median cosine similarity of 0.238. By contrast, the current signatures have a much lower median cosine similarity of 0.098. This twofold reduction in similarity is highly statistically significant (P value 9.1 × 10−25) and indicates a better separation between the signatures in the current analysis.Correlations of mutational signature activity with ageBefore evaluating the association between age and the activity of a mutational signature, all outliers for both age and numbers of mutations attributed to a signature in a cancer type were removed from the data. An outlier was defined as any value outside three standard deviations from the mean value. A robust linear regression model that estimated the slope of the line and whether this slope was significantly different from zero (F test; P value < 0.05) was performed using the MATLAB function robustfit (https://www.mathworks.com/help/stats/robustfit.html) with default parameters. The P values from the F tests were corrected using the Benjamini–Hochberg procedure for false discovery rates. Results are available at syn12030687 and syn20317940.Reporting summaryFurther information on research design is available in the Nature Research Reporting Summary linked to this paper.
Abstract
Pain is an integrative phenomenon that results from dynamic interactions between sensory and contextual (i.e., cognitive, emotional, and motivational) processes. In the brain the experience of pain is associated with neuronal oscillations and synchrony at different frequencies. However, an overarching framework for the significance of oscillations for pain remains lacking. Recent concepts relate oscillations at different frequencies to the routing of information flow in the brain and the signaling of predictions and prediction errors. The application of these concepts to pain promises insights into how flexible routing of information flow coordinates diverse processes that merge into the experience of pain. Such insights might have implications for the understanding and treatment of chronic pain.
Abstract
Investigation of tree growth in Isle Royale National Park in Michigan revealed the influence of herbivores and carnivores on plants in an intimately linked food chain. Plant growth rates were regulated by cycles in animal density and responded to annual changes in primary productivity only when released from herbivory by wolf predation. Isle Royale's dendrochronology complements a rich literature on food chain control in aquatic systems, which often supports a trophic cascade model. This study provides evidence of top-down control in a forested ecosystem.
Abstract
The vagaries of history lead to the prediction that repeated instances of evolutionary diversification will lead to disparate outcomes even if starting conditions are similar. We tested this proposition by examining the evolutionary radiation of Anolis lizards on the four islands of the Greater Antilles. Morphometric analyses indicate that the same set of habitat specialists, termed ecomorphs, occurs on all four islands. Although these similar assemblages could result from a single evolutionary origin of each ecomorph, followed by dispersal or vicariance, phylogenetic analysis indicates that the ecomorphs originated independently on each island. Thus, adaptive radiation in similar environments can overcome historical contingencies to produce strikingly similar evolutionary outcomes.
Abstract
A series of dipyridodiazepinones have been shown to be potent inhibitors of human immunodeficiency virus-1 (HIV-1) reverse transcriptase (RT). One compound, BI-RG-587, had a Ki of 200 nanomolar for inhibition of HIV-1 RT that was noncompetitive with respect to deoxyguanosine triphosphate. BI-RG-587 was specific for HIV-1 RT, having no effect on feline and simian RT or any mammalian DNA polymerases. BI-RG-587 inhibited HIV-1 replication in vitro as demonstrated by in situ hybridization, inhibition of protein p24 production, and the lack of syncytia formation in cultured human T cell lines and freshly isolated human peripheral blood lymphocytes. Cytotoxicity studies of BI-RG-587 on human cells showed a high therapeutic index (greater than 8000) in culture.
Abstract
A complementary DNA for the Aequorea victoria green fluorescent protein (GFP) produces a fluorescent product when expressed in prokaryotic (Escherichia coli) or eukaryotic (Caenorhabditis elegans) cells. Because exogenous substrates and cofactors are not required for this fluorescence, GFP expression can be used to monitor gene expression and protein localization in living organisms.
Abstract
Many filamentous cyanobacteria grow as multicellular organisms that show a developmental pattern of single nitrogen-fixing heterocysts separated by approximately 10 vegetative cells. Overexpression of a 54-base-pair gene, patS, blocked heterocyst differentiation in Anabaena sp. strain PCC 7120. A patS null mutant showed an increased frequency of heterocysts and an abnormal pattern. Expression of a patS-gfp reporter was localized in developing proheterocysts. The addition of a synthetic peptide corresponding to the last five amino acids of PatS inhibited heterocyst development. PatS appears to control heterocyst pattern formation through intercellular signaling mechanisms.
Abstract
Retrotransposition of processed mRNAs is a common source of novel sequence acquired during the evolution of genomes. Although the vast majority of retroposed gene copies, or retrogenes, rapidly accumulate debilitating mutations that disrupt the reading frame, a small percentage become new genes that encode functional proteins. By using a multibreed association analysis in the domestic dog, we demonstrate that expression of a recently acquired retrogene encoding fibroblast growth factor 4 (fgf4) is strongly associated with chondrodysplasia, a short-legged phenotype that defines at least 19 dog breeds including dachshund, corgi, and basset hound. These results illustrate the important role of a single evolutionary event in constraining and directing phenotypic diversity in the domestic dog.
Abstract
A key mutational process in cancer is structural variation, in which rearrangements delete, amplify or reorder genomic segments that range in size from kilobases to whole chromosomes1,2,3,4,5,6,7. Here we develop methods to group, classify and describe somatic structural variants, using data from the Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium of the International Cancer Genome Consortium (ICGC) and The Cancer Genome Atlas (TCGA), which aggregated whole-genome sequencing data from 2,658 cancers across 38 tumour types8. Sixteen signatures of structural variation emerged. Deletions have a multimodal size distribution, assort unevenly across tumour types and patients, are enriched in late-replicating regions and correlate with inversions. Tandem duplications also have a multimodal size distribution, but are enriched in early-replicating regions—as are unbalanced translocations. Replication-based mechanisms of rearrangement generate varied chromosomal structures with low-level copy-number gains and frequent inverted rearrangements. One prominent structure consists of 2–7 templates copied from distinct regions of the genome strung together within one locus. Such cycles of templated insertions correlate with tandem duplications, and—in liver cancer—frequently activate the telomerase gene TERT. A wide variety of rearrangement processes are active in cancer, which generate complex configurations of the genome upon which selection can act.
Discussion
We have described the patterns and signatures of structural variation in a large cohort of uniformly analysed cancer genomes. A major grouping of patterns in structural variants that emerges from our study is one in which extra copies of genomic templates are inserted during the rearrangement process. This includes simple events such as tandem duplications, as well as a range of more-complex events with duplications and triplications that are rearranged locally as well as inserted distantly. Our signature analysis grouped a large proportion of these more-complex events together with tandem duplications, which suggests that they represent a continuum of processes that share underlying properties. A replication-based mechanism has previously been proposed to explain local two-jumps4,23,24, in which stalled replication forks or other DNA lesions cause the DNA polymerase to switch templates and continue replication in a new location. Studies in experimental models are now revealing that a wide range of mechanisms and DNA lesions can result in templated insertions: these mechanisms include tandem duplications in BRCA1 deficiency10, translocations with templated insertions caused by dysregulated strand invasion38 and distant templated insertions in the absence of replication helicases39.Genomic instability in cancer is not a single phenomenon. Instead, many different mutational processes can act to restructure the genome and, in doing so, generate a notably flexible array of possible structures. Any given tumour draws on a subset of the available processes, shaped by the cell of origin, germline predisposition and other, unknown, factors: selection then does the rest, promoting the clone that has chanced on the structure that increases its potential for self-determination.
Methods
No statistical methods were used to predetermine sample size. The experiments were not randomized and investigators were not blinded to allocation during experiments and outcome assessment.A detailed description of the methods used in this paper and many additional results are described in Supplementary Information. Here, we summarize the key aspects of the analysis.Generation of the structural-variant call setThe final set of structural variants used in this Article was generated by the Technical Working Group of the PCAWG Consortium and is described in the main PCAWG paper8. In brief, four variant callers were used to identify somatically acquired structural variants from matched tumour and germline whole genome sequencing data: SvABA (Broad pipeline), DELLY (DKFZ pipeline), BRASS (Sanger pipeline) and dRanger (Broad pipeline). These were merged into a final call set using a graph-based algorithm to identify overlapping breakpoint junctions across algorithms. Detailed visual inspection of structural-variant calls suggested that a simple approach of accepting all structural-variant calls made by two or more of the four algorithms gave the best trade-off between sensitivity and specificity.Structural-variant clustering and annotationTo identify clusters of structural variants, we developed a method for grouping structural variants into clusters and footprints to allow structural and mechanistic inferences to be made systematically. In parallel, we processed the somatic copy-number data and merged it with structural-variant junctions to enable us produce rearrangement patterns from the generated structural-variant clusters and footprints. We produced normalized representations of structural-variant cluster patterns, which enable us to tabulate the number of different cluster and footprint patterns and analyse their features. Finally, we performed manual and simulation-assisted interpretation of the recurrently observed cluster and footprint patterns. The individual steps of the structural-variant classification pipeline are outlined below and detailed in the subsequent subsections: (1) computing the exact breakpoint coordinates from clipped reads; (2) removing redundant ‘segment-bypassing’ structural variants; (3) merging rearrangement breakpoints with copy-number data to yield structural-variant breakpoint-demarcated, normalized, absolute copy-number data; (4) clustering individual structural variants into structural-variant clusters and footprints; (5) heuristically refining structural-variant clusters and footprints; (6) filtering artefactual fold-back-type structural variants with insufficient support; (7) determining balanced overlapping breakpoints (this step is to distinguish very short templated insertions from mutually overlapping balanced breakpoints); and (8) computing rearrangement patterns and categories.Distribution of structural variants across the genomeWe divided the hg19 human reference genome (autosomes and chromosome X) into 3,036,315 pixels of 1 kb, and calculated a suite of metrics per pixel to summarize a variety of genome properties with potential relevance to the distribution of rearrangements, as listed in the Supplementary Information. Properties were matched as closely as possible to the tissue of origin for cancer samples from the PCAWG data. All other genome properties were held fixed across all tissues. To test for associations between structural-variant event classes and the library of genome properties, the genome property metrics were compared between real structural-variant positions (randomly choosing one side of each breakpoint junction to reduce dependence between observations) and one million uniform random positions from the callable genome space. To compare the tissue-specific properties, each random position was assigned a random tissue type, drawing from the observed tissue-type distribution in the structural-variant call set. For each genome property and each event class, the real observations were pooled amongst the random ones, and then rank-transformed and normalized on a scale from 0 to 1. Under the null hypothesis of no event-versus-property association, the ranks of the real observations would follow a uniform distribution. We tested this in each case with a Kolmogorov–Smirnov test then applied a Benjamini–Yekutieli correction for false-discovery rate across the entire suite of tests and set the threshold for significance reporting at 0.01.Structural-variant-signature analysisWe used two algorithms for extracting structural-variant signatures. Both used the same input files, comprising a matrix of counts per patient (across all patients) of structural-variant clusters falling into a number of mutually exclusive categories. These categories included the major classes of structural variants, with the more-common events (deletions, tandem duplications and inversions) split by size and/or replication timing. The two algorithms that were  used for extracting the signatures were (1) a hierarchical Dirichlet process and (2) non-negative matrix factorization. Further details on the implementation of these algorithms are available in the Supplementary Information.Reporting summaryFurther information on research design is available in the Nature Research Reporting Summary linked to this paper.
Abstract
In immigration enforcement, many undocumented immigrants with children are often detained and deported. But it is their US-born citizen-children that have been overlooked in immigration debates and enforcement policies and practices. Citizen-children are at risk for negative psychological outcomes when families are fractured and destabilized by arrest, detention, and deportation. The children risk being torn from their parents and, often, their undocumented siblings. To add to the small but growing empirical base on the effects of living under the threat of deportation and actual deportation of parents, we compared the psychological status of three groups of citizen-children: (1) a group living in Mexico with their deported parents; (2) a group in the US with parents affected by detention or deportation; and (3) a comparison group of citizen-children whose undocumented parents were not affected by detention or deportation. We compared children on self-report and parent-report measures of behavioral adjustment, depression, anxiety, and self-concept. Across the three groups we found elevated levels of distress, and differences between children who had experienced a parent's detention or deportation and those who had not. We discuss findings in the context of children's clinical needs, future research, and implications for immigration enforcement policy and practices.
Abstract
Cancer is driven by genetic change, and the advent of massively parallel sequencing has enabled systematic documentation of this variation at the whole-genome scale1,2,3. Here we report the integrative analysis of 2,658 whole-cancer genomes and their matching normal tissues across 38 tumour types from the Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium of the International Cancer Genome Consortium (ICGC) and The Cancer Genome Atlas (TCGA). We describe the generation of the PCAWG resource, facilitated by international data sharing using compute clouds. On average, cancer genomes contained 4–5 driver mutations when combining coding and non-coding genomic elements; however, in around 5% of cases no drivers were identified, suggesting that cancer driver discovery is not yet complete. Chromothripsis, in which many clustered structural variants arise in a single catastrophic event, is frequently an early event in tumour evolution; in acral melanoma, for example, these events precede most somatic point mutations and affect several cancer-associated genes simultaneously. Cancers with abnormal telomere maintenance often originate from tissues with low replicative activity and show several mechanisms of preventing telomere attrition to critical levels. Common and rare germline variants affect patterns of somatic mutation, including point mutations, structural variants and somatic retrotransposition. A collection of papers from the PCAWG Consortium describes non-coding mutations that drive cancer beyond those in the TERT promoter4; identifies new signatures of mutational processes that cause base substitutions, small insertions and deletions and structural variation5,6; analyses timings and patterns of tumour evolution7; describes the diverse transcriptional consequences of somatic mutation on splicing, expression levels, fusion genes and promoter activity8,9; and evaluates a range of more-specialized features of cancer genomes8,10,11,12,13,14,15,16,17,18.
Methods
SamplesWe compiled an inventory of matched tumour–normal whole-cancer genomes in the ICGC Data Coordinating Centre. Most samples came from treatment-naive, primary cancers, although a small number of donors had multiple samples of primary, metastatic and/or recurrent tumours. Our inclusion criteria were: (1) matched tumour and normal specimen pair; (2) a minimal set of clinical fields; and (3) characterization of tumour and normal whole genomes using Illumina HiSeq paired-end sequencing reads.We collected genome data from 2,834 donors, representing all ICGC and TCGA donors that met these criteria at the time of the final data freeze in autumn 2014 (Extended Data Table 1). After quality assurance (Supplementary Methods 2.5), data from 176 donors were excluded as unusable, 75 had minor issues that could affect some analyses (grey-listed donors) and 2,583 had data of optimal quality (white-listed donors)(Supplementary Table 1). Across the 2,658 white- and grey-listed donors, whole-genome sequences were available from 2,605 primary tumours and 173 metastases or local recurrences. Matching normal samples were obtained from blood (2,064 donors), tissue adjacent to the primary tumour (87 donors) or from distant sites (507 donors). Whole-genome sequencing data were available for tumour and normal DNA for the entire cohort. The mean read coverage was 39× for normal samples, whereas tumours had a bimodal coverage distribution with modes at 38× and 60× (Supplementary Fig. 1). The majority of specimens (65.3%) were sequenced using 101-bp paired-end reads. An additional 28% were sequenced with 100-bp paired-end reads. Of the remaining specimens, 4.7% were sequenced with read lengths longer than 101 bp, and 1.9% with read lengths shorter than 100 bp. The distribution of read lengths by tumour cohort is shown in Supplementary Fig. 11. Median read length for whole-genome sequencing paired-end reads was 101 bp (mean = 106.2, s.d. = 16.7; minimum–maximum = 50–151). RNA-sequencing data were collected and re-analysed centrally for 1,222 donors, including 1,178 primary tumours, 67 metastases or local recurrences and 153 matched normal tissue samples adjacent to the primary tumour.Demographically, the cohort included 1,469 men (55%) and 1,189 women (45%), with a mean age of 56 years (range, 1–90 years) (Supplementary Table 1). Using population ancestry-differentiated single nucleotide polymorphisms, the ancestry distribution was heavily weighted towards donors of European descent (77% of total) followed by East Asians (16%), as expected for large contributions from European, North American and Australian projects (Supplementary Table 1).We consolidated histopathology descriptions of the tumour samples, using the ICD-0-3 tumour site controlled vocabulary89. Overall, the PCAWG dataset comprises 38 distinct tumour types (Extended Data Table 1 and Supplementary Table 1). Although the most common tumour types are included in the dataset, their distribution does not match the relative population incidences, largely owing to differences among contributing ICGC/TCGA groups in the numbers of sequenced samples.Uniform processing and somatic variant callingTo generate a consistent set of somatic mutation calls that could be used for cross-tumour analyses, we analysed all 6,835 samples using a uniform set of algorithms for alignment, variant calling and quality control (Extended Data Fig. 1, Supplementary Fig. 2, Supplementary Table 3 and Supplementary Methods 2). We used the BWA-MEM algorithm90 to align each tumour and normal sample to human reference build hs37d5 (as used in the 1000 Genomes Project91). Somatic mutations were identified in the aligned data using three established pipelines, which were run independently on each tumour–normal pair. Each of the three pipelines—labelled ‘Sanger’92,93,94,95, ‘EMBL/DKFZ’96,97 and ‘Broad’98,99,100,101 after the computational biology groups that created or assembled them—consisted of multiple software packages for calling somatic SNVs, small indels, CNAs and somatic SVs (with intrachromosomal SVs defined as those >100 bp). Two additional variant algorithms102,103 were included to further improve accuracy across a broad range of clonal and subclonal mutations. We tested different merging strategies using validation data, and choses the optimal method for each variant type to generate a final consensus set of mutation calls (Supplementary Methods S2.4).Somatic retrotransposition events, including Alu and LINE-1 insertions72, L1-mediated transductions73 and pseudogene formation104, were called using a dedicated pipeline73. We removed these retrotransposition events from the somatic SV call-set. Mitochondrial DNA mutations were called using a published algorithm105. RNA-sequencing data were uniformly processed to quantify normalized gene-level expression, splicing variation and allele-specific expression, and to identify fusion transcripts, alternative promoter usage and sites of RNA editing8.Integration, phasing and validation of germline variant call-setsCalls of common (≥1% frequency in PCAWG) and rare (<1%) germline variants including single-nucleotide polymorphisms, indels, SVs and mobile-element insertions (MEIs) were generated using a population-scale genetic polymorphism-detection approach91,106. The uniform germline data-processing workflow comprised variant identification using six different variant-calling algorithms96,107,108 and was orchestrated using the Butler workflow system109.We performed call-set benchmarking, merging, variant genotyping and statistical haplotype-block phasing91 (Supplementary Methods 3.4). Using this strategy, we identified 80.1 million germline single-nucleotide polymorphisms, 5.9 million germline indels, 1.8 million multi-allelic short (<50 bp) germline variants, as well as germline SVs ≥ 50 bp in size including 29,492 biallelic deletions and 27,254 MEIs (Supplementary Table 2). We statistically phased this germline variant set using haplotypes from the 1000 Genomes Project91 as a reference panel, yielding an N50-phased block length of 265 kb based on haploid chromosomes from donor-matched tumour genomes. Precision estimates for germline SNVs and indels were >99% for the phased merged call-set, and sensitivity estimates ranged from 92% to 98%.Core alignment and variant calling by cloud computingThe requirement to uniformly realign and call variants on nearly 5,800 whole genomes (tumour plus normal) presented considerable computational challenges, and raised ethical issues owing to the use of data from different jurisdictions (Extended Data Table 2). To process the data, we adopted a cloud-computing architecture26 in which the alignment and variant calling was spread across 13 data centres on 3 continents, representing a mixture of commercial, infrastructure-as-a-service, academic cloud compute and traditional academic high-performance computer clusters (Supplementary Table 3). Together, the effort used 10 million CPU-core hours.To generate reproducible variant calling across the 13 data centres, we built the core pipelines into Docker containers28, in which the workflow description, required code and all associated dependencies were packaged together in stand-alone packages. These heavily tested, extensively validated workflows are available for download (Box 1).Validation, benchmarking and merging of somatic variant callsTo evaluate the performance of each of the mutation-calling pipelines and determine an integration strategy, we performed a large-scale deep-sequencing validation experiment (Supplementary Notes 1). We selected a pilot set of 63 representative tumour–normal pairs, on which we ran the 3 core pipelines, together with a set of 10 additional somatic variant-calling pipelines contributed by members of the PCAWG SNV Calling Methods Working Group. Sufficient DNA remained for 50 of the 63 cases for validation, which was performed by hybridization of tumour and matched normal DNA to a custom RNA bait set, followed by deep sequencing, as previously described29. Although performed using the same sequencing chemistry as the original whole-genome sequencing analyses, the considerably greater depth achieved in the validation experiment enabled accurate assessment of sensitivity and precision of variant calls. Variant calls in repeat-masked regions were not tested, owing to the challenge of designing reliable validation probes in these areas.The 3 core pipelines had individual estimates of sensitivity of 80–90% to detect a true somatic SNV called by any of the 13 pipelines; with >95% of SNV calls made by each of the core pipelines being genuine somatic variants (Fig. 1a). For indels—a more-challenging class of variants to identify in short-read sequencing data—the 3 core algorithms had individual sensitivity estimates in the range of 40–50%, with precision 70–95% (Fig. 1b). Validation of SV calls is inherently more difficult, as methods based on PCR or hybridization to RNA baits often fail to isolate DNA that spans the breakpoint. To assess the accuracy of SV calls, we therefore used the property that an SV must either generate a copy-number change or be balanced, whereas artefactual calls will not respect this property. For individual SV-calling algorithms, we estimated precision to be in the range of 80–95% for samples in the 63-sample pilot dataset.Next, we examined multiple methods for merging calls made by several algorithms into a single definitive call-set to be used for downstream analysis. The final consensus calls for SNVs were based on a simple approach that required two or more methods to agree on a call. For indels, because methods were less concordant, we used stacked logistic regression110,111 to integrate the calls. The merged SV set includes all calls made by two or more of the four primary SV-calling algorithms96,100,112,113. Consensus CNA calls were obtained by joining the outputs of six individual CNA-calling algorithms with SV consensus breakpoints to obtain base-pair resolution CNAs (Supplementary Methods 2.4.3). Consensus purity and ploidy were derived, and a multi tier system was developed for consensus copy-number calls (Supplementary Methods 2.4.3, and described in detail elsewhere7).Overall, the sensitivity and precision of the consensus somatic variant calls were 95% (90% confidence interval, 88–98%) and 95% (90% confidence interval, 71–99%), respectively, for SNVs (Extended Data Fig. 2). For somatic indels, sensitivity and precision were 60% (90% confidence interval, 34–72%) and 91% (90% confidence interval, 73–96%), respectively. Regarding SVs, we estimate the sensitivity of the merging algorithm to be 90% for true calls generated by any one calling pipeline; precision was estimated to be 97.5%. That is, 97.5% of SVs in the merged SV call-set had an associated copy-number change or balanced partner rearrangement. The improvement in calling accuracy from combining different pipelines was most noticeable in variants that had low variant allele fractions, which are likely to originate from subclonal populations of the tumour (Fig. 1c, d). There remains much work to be done to improve indel calling software; we still lack sensitivity for calling even fully clonal complex indels from short-read sequencing data.
Abstract
Objective:
      
    

    
      To investigate the therapeutic effect of CBD-ointment administered on severe skin chronic diseases and/or on their outcome scars.
    

    
  


        Methods:
      
    

    
      A spontaneous, anecdotal, retrospective study of 20 patients with two most frequent skin disorders: psoriasis (n: 5 patients), atopic dermatitis (n: 5) and resulting outcome scars (n: 10). The subjects were instructed to administer topical CBD-enriched ointment to lesioned skin areas twice daily for three months treatment.
    

    
  


        Results:
      
    

    
      Based on skin evaluations (hydration, TEWL, elasticity), clinical questionnaires (SCORAD, ADI, PASI), and supported by photographic data and investigators' clinical assessment, the results showed that topical treatment with CBD-enriched ointment significantly improved the skin parameters, the symptoms and also the PASI index score. No irritant or allergic reactions were documented during the period treatment.
    

    
  


        Conclusions:
      
    

    
      The topical administration of CBD ointment, without any THC, is a safe and effective non-invasive alternative for improve the quality of life in patients with some skin disorders, especially on inflammatory background.
Abstract
Background:
      
    

    
      Previous reports of lower triage acuity scores and longer Emergency Department (ED) wait times for African Americans compared to Caucasians had insufficient information to determine if this was due to bias or appropriately based on medical history and clinical presentation.
    

    
  


        Objective:
      
    

    
      (1) Determine if African Americans are assigned lower triage acuity scores (TAS) after adjusting for a number of demographic and clinical variables likely to affect triage scores. (2) Determine if lower TAS translate into clinically significant longer wait times to assignment to a treatment area.
    

    
  


        Methods:
      
    

    
      This was a retrospective matched cohort design analysis of de-identified data extracted from the ED electronic medical record system, which included demographic and clinical information, as well as TAS, and ED process times. Triage scores were assigned using a 5-point scale (ESI), with 1 being most urgent and 5 being least urgent. Mean TAS and wait times to a treatment area for specific chief complaints were compared by race; after adjusting for age, gender, insurance status, time of day, day of week, presence of co-morbidities, and abnormal vital signs using a 1:1 matched case analysis.
    

    
  


        Results:
      
    

    
      The overall mean TAS for African Americans was 2.97 vs. 2.81 for Caucasians (difference of 0.18; p<0.001), translating to a lower acuity rating. African Americans had a significantly longer wait time to a treatment area compared to case-matched Caucasians (10.9min; p<0.001), with much larger differences in wait times noted within certain specific chief complaint categories.
    

    
  


        Conclusion:
      
    

    
      Our current study supports the hypothesis that racial bias may influence the triage process.
Abstract
Background:
      
    

    
      Versus whites, blacks with diabetes have poorer control of hemoglobin A1c (HbA1c), higher systolic blood pressure (SBP), and higher low-density lipoprotein (LDL) cholesterol as well as higher rates of morbidity and microvascular complications.
    

    
  


        Objective:
      
    

    
      To examine whether several mutable risk factors were more strongly associated with poor control of multiple intermediate outcomes among blacks with diabetes than among similar whites.
    

    
  


        Design:
      
    

    
      Case-control study.
    

    
  


        Subjects:
      
    

    
      A total of 764 blacks and whites with diabetes receiving care within 8 managed care health plans.
    

    
  


        Measures:
      
    

    
      Cases were patients with poor control of at least 2 of 3 intermediate outcomes (HbA1c > or =8.0%, SBP > or =140 mmHg, LDL cholesterol > or =130 mg/dL) and controls were patients with good control of all 3 (HbA1c <8.0%, SBP <140 mmHg, LDL cholesterol <130 mg/dL). In multivariate analyses, we determined whether each of several potentially mutable risk factors, including depression, poor adherence to medications, low self-efficacy for reducing cardiovascular risk, and poor patient-provider communication, predicted case or control status.
    

    
  


        Results:
      
    

    
      Among blacks but not whites, in multivariate analyses depression (odds ratio: 2.28; 95% confidence interval: 1.09-4.75) and having missed medication doses (odds ratio: 1.96; 95% confidence interval: 1.01-3.81) were associated with greater odds of being a case rather than a control. None of the other risk factors were associated for either blacks or whites.
    

    
  


        Conclusions:
      
    

    
      Depression and missing medication doses are more strongly associated with poor diabetes control among blacks than in whites. These 2 risk factors may represent important targets for patient-level interventions to address racial disparities in diabetes outcomes.
Abstract
Flexible transparent electrodes are in significant demand in applications including solar cells, light-emitting diodes, and touch panels. The combination of high optical transparency and high electrical conductivity, however, sets a stringent requirement on electrodes based on metallic materials. To obtain practical sheet resistances, the visible transmittance of the electrodes in previous studies is typically lower than the transparent substrates the electrode structures are built on, namely, the transmittance relative to the substrate is <100%. Here, we demonstrate a flexible dielectric-metal-dielectric-based electrode with ~88.4% absolute transmittance, even higher than the ~88.1% transmittance of the polymer substrate, which results in a relative transmittance of ~100.3%. This non-trivial performance is achieved by leveraging an optimized dielectric-metal-dielectric structure guided by analytical and quantitative principles described in this work, and is attributed to an ultra-thin and ultra-smooth copper-doped silver film with low optical loss and low sheet resistance.
Introduction
Transparent electrodes are widely used in photovoltaics (PVs)1,2, light-emitting diodes (LEDs)3,4,5, touch panels6,7, and other optoelectronic devices. Indium tin oxide (ITO) is the conventional selection for the transparent electrode because of its high visible transmittance and electrical conductivity. However, the low abundance of the indium element on earth is a limiting factor of this material. In addition, its applications in emerging flexible optoelectronic devices are significantly hindered by both the poor mechanical flexibility and the high annealing temperature needed to reduce its resistivity8. Graphene9,10, carbon nanotubes (CNTs)11,12, and conductive polymers13 have been investigated to replace ITO. Unfortunately, their limited conductivity and high cost of mass-production remains a challenge in large-area applications. To overcome these limitations, transparent electrodes employing metal networks have been proposed14,15,16,17,18,19,20,21,22,23. Although these novel structures exhibit decent optical and electrical performance and can also be scaled up easily via different roll-to-roll (R2R) techniques24,25, the extruded or non-uniform surfaces may cause shorting problems when being used in LED and PV devices14. In addition, the high optical haze resulting from the scattering of the nanowires and mesh patterns is undesired in high-resolution displays14. In recent years, dielectric–metal–dielectric (DMD)-based transparent electrodes have been noted as potential alternatives. In this type of electrode, a thin metallic film is sandwiched between two antireflection dielectrics to induce high transparency. They feature high transparency and conductivity, low haze, excellent flexibility, facile fabrication, and great compatibility with different substrates2,26,27,28,29,30,31,32,33,34,35,36,37. The trade-off between electrical conductivity and optical transmittance has been a major challenge for metallic-material-based transparent electrodes. To achieve a practical conductivity (e.g., sheet resistance (Rs) < 20 Ω sq−1), the absolute visible transmittance of previously reported metallic-material-based transparent electrodes, including both metal network and DMD structures, is typically lower than that of the substrate itself, which has been taken for granted without serious questioning.Therefore, we are motivated to conduct a rigorous investigation and explore the limit of transmittance at a sufficiently low sheet resistance suitable for practical applications. In this work, we demonstrate a DMD-based transparent electrode with ~88.4% absolute transmittance averaged over the entire visible spectrum (400–700 nm) on polyethylene terephthalate (PET) polymeric substrate, which surpasses the transmittance of the substrate itself (~88.1%), leading to a relative transmittance >100%. This counter-intuitive, yet achievable performance is obtained by (i) quantitative design principles that are generalized in this work, particularly, analytic expression of the optimal bottom dielectric thickness and analytical result showing that different materials should be used for the two dielectrics, (ii) the use of an ultra-thin (~6.5 nm thick) and ultra-smooth (roughness < 1 nm) copper-doped silver (Cu-doped Ag) film providing low optical loss and high electrical conductivity at the same time. The proposed design principles and electrode structures have resolved the problems faced by other existing transparent electrodes and may have the potential to replace traditional ITO counterparts for flexible optoelectronics, thus facilitating high-performance flexible displays and optoelectronic devices.
Discussion
Relative transmittance higher than 100% was achieved in this work by the integration of a novel Cu-doped Ag film into an optimized DMD structure. This Cu-doped Ag film features ultra-thin thickness (~6.5 nm thick), ultra-smooth morphology (roughness < 1 nm), low optical loss, and high electrical conductivity (sheet resistance ~18.6 Ω sq−1), and was fabricated using a room-temperature deposition method. The optimized DMD structure was designed by quantitative principles that were generalized for the first time in this work. We experimentally demonstrated that the flexible DMD-based electrode, although not optimal, has 88.4% absolute averaged transmittance over the visible spectrum, which is higher than 88.1% transmittance of its PET substrate. This study provides an exciting pathway to address the major challenge faced by existing flexible transparent electrodes and to replace traditional ITO counterparts, thus facilitating high-performance flexible optoelectronic devices.
Abstract
Purpose:
      
    

    
      Health providers' implicit racial bias negatively affects communication and patient reactions to many medical interactions. However, its effects on racially discordant oncology interactions are largely unknown. Thus, we examined whether oncologist implicit racial bias has similar effects in oncology interactions. We further investigated whether oncologist implicit bias negatively affects patients' perceptions of recommended treatments (i.e., degree of confidence, expected difficulty). We predicted oncologist implicit bias would negatively affect communication, patient reactions to interactions, and, indirectly, patient perceptions of recommended treatments.
    

    
  


        Methods:
      
    

    
      Participants were 18 non-black medical oncologists and 112 black patients. Oncologists completed an implicit racial bias measure several weeks before video-recorded treatment discussions with new patients. Observers rated oncologist communication and recorded interaction length of time and amount of time oncologists and patients spoke. Following interactions, patients answered questions about oncologists' patient-centeredness and difficulty remembering contents of the interaction, distress, trust, and treatment perceptions.
    

    
  


        Results:
      
    

    
      As predicted, oncologists higher in implicit racial bias had shorter interactions, and patients and observers rated these oncologists' communication as less patient-centered and supportive. Higher implicit bias also was associated with more patient difficulty remembering contents of the interaction. In addition, oncologist implicit bias indirectly predicted less patient confidence in recommended treatments, and greater perceived difficulty completing them, through its impact on oncologists' communication (as rated by both patients and observers).
    

    
  


        Conclusion:
      
    

    
      Oncologist implicit racial bias is negatively associated with oncologist communication, patients' reactions to racially discordant oncology interactions, and patient perceptions of recommended treatments. These perceptions could subsequently directly affect patient-treatment decisions. Thus, implicit racial bias is a likely source of racial treatment disparities and must be addressed in oncology training and practice.
Abstract
Objective:
      
    

    
      To determine the household and community characteristics most closely associated with variation in COVID-19 incidence on American Indian reservations in the lower 48 states.
    

    
  


        Design:
      
    

    
      Multivariate analysis with population weights.
    

    
  


        Setting:
      
    

    
      Two hundred eighty-seven American Indian Reservations and tribal homelands (in Oklahoma) and, as of April 10, 2020, 861 COVID-19 cases on these reservation lands.
    

    
  


        Main outcome measures:
      
    

    
      The relationship between rate per 1000 individuals of publicly reported COVID-19 cases at the tribal reservation and/or community level and average household characteristics from the 2018 5-Year American Community Survey records.
    

    
  


        Results:
      
    

    
      By April 10, 2020, in regression analysis, COVID-19 cases were more likely by the proportion of homes lacking indoor plumbing (10.83, P = .001) and were less likely according to the percentage of reservation households that were English-only (-2.43, P = .03). Household overcrowding measures were not statistically significant in this analysis (-6.40, P = .326).
    

    
  


        Conclusions:
      
    

    
      Failure to account for the lack of complete indoor plumbing and access to potable water in a pandemic may be an important determinant of the increased incidence of COVID-19 cases. Access to relevant information that is communicated in the language spoken by many reservation residents may play a key role in the spread of COVID-19 in some tribal communities. Household overcrowding does not appear to be associated with COVID-19 infections in our data at the current time. Previous studies have identified household plumbing and overcrowding, and language, as potential pandemic and disease infection risk factors. These risk factors persist. Funding investments in tribal public health and household infrastructure, as delineated in treaties and other agreements, are necessary to protect American Indian communities.
Abstract
With the ever-increasing availability of whole-genome sequences, machine-learning approaches can be used as an alternative to traditional alignment-based methods for identifying new antimicrobial-resistance genes. Such approaches are especially helpful when pathogens cannot be cultured in the lab. In previous work, we proposed a game-theory-based feature evaluation algorithm. When using the protein characteristics identified by this algorithm, called ‘features’ in machine learning, our model accurately identified antimicrobial resistance (AMR) genes in Gram-negative bacteria. Here we extend our study to Gram-positive bacteria showing that coupling game-theory-identified features with machine learning achieved classification accuracies between 87% and 90% for genes encoding resistance to the antibiotics bacitracin and vancomycin. Importantly, we present a standalone software tool that implements the game-theory algorithm and machine-learning model used in these studies.
Introduction
Antimicrobial resistance (AMR) refers to a property of bacteria when they become less susceptible to an antimicrobial agent1,2,3,4. Bacteria can gain AMR by overexpressing or duplicating available genes, undergoing chromosomal mutation, or obtaining resistance genes from other bacteria by means of horizontal gene transfer1, 5. According to a recently released report by the Centers for Disease Control and Prevention (CDC), at least 2.8 million people in the United States are infected every year by antimicrobial-resistant organisms, and these infections result in more than 35,000 deaths6. Also, according to a recently released report by the Organisation for Economic Co-operation and Development (OECD), 2.4 million deaths are predicted in Europe, North America, and Australia in the next 30 years due to antimicrobial-resistant infections, and such infections could cause up to US$3.5 billion in additional health care costs per year7, 8. As AMR becomes a threat worldwide, both economically and to public health9,10,11,12,13, there is an urgent need to develop a preclinical tool for efficient prediction of AMR.One conventional strategy for identifying genetically-encoded mechanisms for AMR involves sequence assembly14,15,16,17 and read-based techniques18,19,20 that map sequence data directly to reference databases. Although these methods perform well for known and highly conserved AMR genes, they may produce an unacceptable number of false positives (genes predicted to encode resistance when they do not) for highly dissimilar sequences as was demonstrated previously for Gram-negative bacteria21. Machine-learning techniques can be applied as an alternative solution for predicting putative AMR genes. Rather than using sequence similarity, a machine-learning model detects features, i.e., characteristics of a protein sequence, that are unique to AMR genes. Several machine-learning methods have been proposed to identify novel AMR genes from metagenomic and pan-genome data12, 22, 23, but these methods used a small number of genetic features for predictions. Moreover, these approaches did not use a feature-selection strategy to remove irrelevant and redundant features that might compromise the accuracy of a machine-learning model.We recently introduced a game-theory-based feature selection approach (“game theoretic dynamic weighting based feature evaluation”, or GTDWFE) predicated on the supposition that a single feature might provide limited predictive value, but that it might contribute to form a strong coalition when used with other features21. We applied our feature selection approach in Gram-negative bacteria and obtained prediction accuracies ranging from 93% to 99% for prediction of genes that encode resistance to acetyltransferase (aac), \(\beta \)-lactamase (bla), and dihydrofolate reductase (dfr). In our current study, we test the GTDWFE algorithm with data from Gram-positive bacteria. We then combine the results for both studies and introduce “Prediction of Antimicrobial Resistance via Game Theory” (PARGT), a software program with a graphical-user interface (GUI) that is designed to identify antimicrobial-resistance genes for both Gram-positive and -negative bacteria.A major objective was to develop a software tool with a simple and intuitive GUI that is capable of extracting protein features without the need for manual curation and then use these features to identify putative AMR genes. PARGT integrates all of the tools and scripts required to identify protein features and to automatically generate feature subsets obtained via the GTDWFE algorithm. PARGT can be used with the Windows, Linux, or macOS, operating systems, and it provides options for predicting bac and van resistance genes in any Gram-positive bacteria and aac, bla, and dfr resistance genes in any Gram-negative bacteria. Users can test a single sequence or an entire genome for these genes. In addition, PARGT allows users to add newly confirmed AMR or non-AMR sequences to the training set as well as to reset the training data back to the original training set downloaded with the tool.
Results
Validation of PARGTWe validated the GTDWFE algorithm for feature selection as implemented previously21. In our earlier work, we considered the AMR (positive) and non-AMR (negative) amino-acid sequences of aac, bla, and dfr for Acinetobacter, Klebsiella, Campylobacter, Salmonella, and Escherichia as training datasets and tested our trained support vector machine (SVM)24, 25 machine-learning model with sequences from Pseudomonas, Vibrio, and Enterobacter. The combination of GTDWFE and SVM resulted in correct classification rates of 93%, 99%, and 97% for aac, bla, and dfr, respectively. This demonstrated that our approach was promising and that the GTDWFE algorithm is capable of identifying the most relevant, non-redundant, and interdependent features necessary for accurate prediction.In this paper we consider validation of our GTDWFE model for AMR proteins in Gram-positive bacteria. We use the unique AMR and non-AMR sequences available for bac and van from the Gram-positive bacteria Clostridium spp. and Enterococcus spp. as the training datasets for our SVM model. These training datasets are used to generate the best feature subsets by means of the GTDWFE approach. The training datasets contain 25 and 52 AMR (positive) examples for bac and van, respectively. A total of 52 non-AMR examples are considered as negative samples for each of the training datasets. In the GTDWFE approach, we select features based on the relevance, non-redundancy, and interdependency values of all features. For this analysis, we need to set an interdependent group size \(\delta \) to measure the interdependency between features, where \(\delta \) is used in the computation of the Banzhaf power index26 and indicates the size of each feature group. We selected a value of \(\delta =3\) based on previous work21 where we found that an interdependent group size of 3 was sufficient to identify best feature subsets from training datasets. We then test our trained model with known AMR and non-AMR samples from Staphylococcus, Streptococcus, and Listeria. The test datasets contain 6 and 9 AMR (positive) sequences for bac and van, respectively, and 14 non-AMR (negative) sequences are used for each test dataset.Table 1 Predicted bac AMR sequences for Staphylococcus, Streptococcus, and Listeria using the GTDWFE algorithm.Full size table
Table 2 Predicted van AMR sequences for Staphylococcus, Streptococcus, and Listeria using the GTDWFE algorithm.Full size table
Tables 1 and 2 list the predicted bac and van AMR sequences from our test datasets, respectively. In each table, we provide the NCBI accession number27 for each protein sequence together with its name, and we note whether an AMR protein was correctly classified as AMR (true positive) or a non-AMR sequence was incorrectly classified as AMR (false positive). The GTDWFE algorithm successfully identified all six bac AMR genes (true positives). However, it missclassified 2 of the 14 non-AMR sequences as AMR (false positives). Therefore, the number of true positives, true negatives (negatives accurately classified), false positives, and false negatives (positives classified as negatives) for bac are 6, 12, 2, and 0, respectively, and the sensitivity, specificity, and accuracy for bac are 100%, 86%, and 90%, respectively. As shown in Table 2 for van, 8 of 9 AMR sequences were correctly classified as AMR (true positives) whereas 2 of 14 non-AMR sequences were classified as AMR (false positives). Therefore, the number of true positives, true negatives, false positives, and false negatives for van are 8, 12, 2, and 1, respectively, and the sensitivity, specificity, and accuracy for van are 89%, 86%, and 87%, respectively. Note that the two tables contain one hypothetical protein and one putative uncharacterized protein. We have categorized these two proteins as false positives because they were identified as essential (non-AMR) genes in the Pathosystems Resource Integration Center (PATRIC)28, 29. However, it is quite possible that PARGT correctly identified them as AMR proteins given the number of annotation errors in public databases30. CDC71755 is from a Staphylococcus organism identified from a metagenome sequence, and EUJ19660 is from a Listeria aquatica organism obtained from an environmental water sample.Performance comparison with BLASTp and Kalign toolsWe also compared the performance of our GTDWFE algorithm with BLASTp (https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE=Proteins) and Kalign31 (https://www.ebi.ac.uk/Tools/msa/kalign/) results using default parameter settings. The outcomes shown in Supplementary Table S1 are the percent identities for bac AMR and non-AMR samples from Staphylococcus, Streptococcus, and Listeria with respect to the bac AMR samples of Clostridium and Enterococcus. A percent identity for BLASTp and Kalign as low as 38.13% and 46.19%, respectively, are needed to identify all the bac AMR sequences; however, these low percent identities lead to 6 and 3 of 14 false positives for BLASTp and Kalign, respectively, in which non-AMR sequences are miscategorized. Therefore, the low percent identities for BLASTp and Kalign required to identify all AMR sequences increase the number of false positives for a set of diverse AMR sequences. In Supplementary Table S2 we show that the performances of BLASTp and Kalign when identifying van AMR sequences are actually better than that of the GTDWFE algorithm. This is due to the very high similarity (>98.5% identity) between the training AMR and test AMR datasets for van. When training and test sets share high similarity, BLASTp and Kalign are guaranteed to give good results. However, as in the case of bac for which the training and test AMR data similarity ranges between 38.13% and 41.01%, BLASTp does not perform well. For Kalign, the similarity ranges between 46.19% and 49.17% so that it performs better than BLASTp. However, the GTDWFE algorithm will outperform both BLASTp and Kalign because it does not use sequence similarity but rather protein features for prediction. BLASTp and Kalign do not predict; they match sequence similarity.
Discussion
In this work, we implemented a software package PARGT and extended our earlier work of identifying AMR genes in Gram-negative to Gram-positive bacteria. PARGT integrates the required software tools and scripts needed to generate all protein features automatically, and it performs predictions on user-inputted sequences. Moreover, users can update PARGT by including their own known AMR and non-AMR sequences to train the machine-learning model to potentially improve prediction accuracy. As our previous work described the experimental results for Gram-negative bacteria, in this paper we only included prediction results for Gram-positive bacteria. Simulation results showed that PARGT can predict AMR sequences for Gram-positive bacteria with accuracy ranging from 87% to 90%. PARGT gave better results for bac due to the diversity of sequences available, but BLASTp and Kalign exhibited better performance in the case of van because of the high similarity of sequences. To generate evolutionary and secondary structure features, we used the Uniprot database (containing 538,585 FASTA sequences) as our reference database for relatively fast execution; however, more accurate values for these features can be obtained using large-scale protein databases such as UniRef90 or UniRef100 (http://www.uniprot.org/help/uniref) as target/reference databases. Note, however, that there is a trade-off between accuracy and computational time when using a large-scale reference database to generate features. A parallel version of PARGT would reduce the execution time of the tool for and ameliorate the use of large-scale reference databases.
Methods
GTDWFE algorithm for feature selectionFeature collection, feature extraction, calculation of feature values, and feature selection using the GTDWFE algorithm are explained in detail in previous works21, 32. Briefly, a total of 621D candidate features were collected by means of a thorough literature search, where D stands for dimension (some features are single values, i.e., 1D, while others are vector values, e.g., 20D for the 20 different amino acids). We extracted all 621D features from both our positive (AMR) and negative (non-AMR) datasets and calculated their values. The GTDWFE algorithm was then used to select features for use in our machine-learning model. The GTDWFE selects the best feature at each iteration based on the relevance, non-redundancy, and interdependency values of all features. Initially, the weights of all features are the same i.e., 1. The relevance of a feature to the target class (AMR or non-AMR) and the distance of the feature to other features are calculated using Pearson’s correlation coefficient and the Tanimoto coefficient, respectively. These calculations are performed for all features, and the feature with the highest summation of relevance and distance is chosen as the initial selected feature. The Banzhaf power index26 is then calculated to estimate the interdependency between the selected feature and the remaining features. We measure the contribution of each feature when it forms a group with other features, and the conditional mutual information is calculated to find the Banzhaf power index of the features. The weight of each remaining feature is updated by adding the product of the current weight and the Banzhaf power index to the feature selected previously. In other words, at each step, we readjust the weight of the remaining features dynamically based on the features selected in earlier steps. Thus, the weight of a candidate feature actually corresponds to the interdependence values with the earlier selected features. The feature with the highest summation of relevance and distance values multiplied by the revised weight is chosen as the next selected feature. This process is repeated until the desired number of features has been reached.Machine-learning algorithmAfter identifying the best feature subset for use with our classifier by means of the GTDWFE algorithm, we trained an SVM machine-learning model using this feature subset. This binary classifier was then used for prediction. As was true for our previous work, in PARGT we tuned the SVM using the training datasets and chose the best SVM model to predict the AMR proteins in the test sequences. We considered 10-fold cross validation to tune the SVM model. The SVM model with a radial basis function (RBF) kernel and a cost value of 4 was identified as the best model for both bac and van training datasets. For the SVM, the RBF is used as a function in the kernel trick to implicitly transform the original space of the data to a high-dimensional space to make the data samples linearly separable, and the cost parameter is used to regulate the classification error.Figure 1The components of PARGT. Components outlined by dotted lines indicate additional training samples supplied by a user.Full size image
Overview of PARGT softwarePARGT is an open-source software package designed and implemented for predicting antimicrobial resistance genes in bacteria. PARGT is written using both Python 3 and R. R scripts were written to identify physicochemical and secondary structure features and for machine-learning modeling, and Python 3 was used to run the R scripts, to generate position-specific scoring matrix (PSSM) features, and to implement the GUI. PARGT weight the importance of protein features based on their contributions during classification. All the required bioinformatics tools33,34,35,36,37,38,39 and scripts necessary to generate the protein features required in our machine-learning model are included in PARGT. PARGT uses the best feature subset identified by our GTDWFE algorithm to make predictions. It allows users to add new AMR and non-AMR sequences to the training datasets, and the software automatically updates the machine-learning model with the additional sequences, potentially resulting in an increase in the accuracy of the model. To minimize execution time, PARGT uses the UniProt database containing 538,585 protein sequences as a reference database, rather than a larger database, for generating PSSM and secondary structure features.Architecture of PARGTFigures 1 and 2 depict the architecture and GUI for PARGT, respectively. PARGT allows a user to input a set of known AMR and non-AMR sequences to use in the training dataset, generating all required feature values for these sequences automatically. As shown in Fig. 1, the 20D amino acid composition feature vector, 168D feature vector based on the composition, transition and distribution (CTD) model40, 41, 400D feature vector based on the PSSM, and 33D feature vector based on the secondary structure sequence and secondary structure probability matrix are generated from the input protein sequences. Then the best feature subset is constructed using our GTDWFE feature selection algorithm. An SVM is used as the machine-learning model that is trained using the selected feature set. Recall that the SVM model used for PARGT is automatically tuned during the training phase. Finally, the trained SVM model is applied to predict AMR sequences from the test dataset.As shown in Fig. 2, PARGT provides the option of predicting aac, bla, and dfr resistance genes for Gram-negative bacteria and bac and van resistance genes for Gram-positive bacteria. A user must select the appropriate option for predicting AMR from the GUI menu and also supply the test file for the set of protein sequences in FASTA format that they wish to have classified as AMR or non-AMR. PARGT automatically computes all the required feature values for the test sequences, and it provides an output file containing the set of predicted AMR sequences for the user’s test file. If a user wants to include new known AMR or non-AMR sequences to augment the training datasets, PARGT provides an option to do so for the five above-mentioned resistance classes. In addition, it provides the option of restoring the original training datasets in case a user decides they prefer to use them or or else wants to compare predictions using two different sets of training data.Figure 2Illustration of the PARGT GUI with its pop-up menu.Full size image
DatasetsWe retrieved protein sequences for AMR genes from the Antibiotic Resistance Genes Database (ARDB)42, and non-AMR sequences were obtained from the PATRIC28, 29. Initially, we gathered 124 bac and 374 van AMR sequences for the Gram-positive bacteria Clostridium spp. and Enterococcus spp., and we randomly chose 52 essential protein sequences to use as non-AMR sequences. As many of the protein sequences were duplicates, CD-HIT43, 44 was applied to find unique sequences. A sequence identity of \(\ge \) 90% was used as a threshold for removing duplicate sequences. After eliminating redundant protein sequences, our final counts were 25 bac and 52 van AMR sequences; none of the 52 non-AMR sequences were duplicates. We used this dataset to train our machine-learning model. In addition to the training dataset, we also gathered 102 bac and 22 van AMR sequences and 14 non-AMR sequences for the Gram-positive bacteria Staphylococcus spp., Streptococcus spp., and Listeria spp. from the data sources indicated above. We again applied CD-HIT to this dataset, and after the removal of duplicate sequences, 6 bac and 9 van AMR sequences and 14 non-AMR sequences remained. We used these as our test dataset to measure the accuracy of the classifier. The sequence identity of protein sequences could be as low as 10%. After validating our GTDWFE algorithm with the training and test sequences for the bac and van AMR classes, we again trained our classifier, but we used the sequences from all five bacterial genera, i.e., both training and test sequences, to potentially increase the accuracy of PARGT. The same retraining was also performed for our Gram-negative bacteria.
Abstract
The density structure of the interstellar medium determines where stars form and release energy, momentum and heavy elements, driving galaxy evolution1,2,3,4. Density variations are seeded and amplified by gas motion, but the exact nature of this motion is unknown across spatial scales and galactic environments5. Although dense star-forming gas probably emerges from a combination of instabilities6,7, convergent flows8 and turbulence9, establishing the precise origin is challenging because it requires gas motion to be quantified over many orders of magnitude in spatial scale. Here we measure10,11,12 the motion of molecular gas in the Milky Way and in nearby galaxy NGC 4321, assembling observations that span a spatial dynamic range 10−1–103 pc. We detect ubiquitous velocity fluctuations across all spatial scales and galactic environments. Statistical analysis of these fluctuations indicates how star-forming gas is assembled. We discover oscillatory gas flows with wavelengths ranging from 0.3–400 pc. These flows are coupled to regularly spaced density enhancements that probably form via gravitational instabilities13,14. We also identify stochastic and scale-free velocity and density fluctuations, consistent with the structure generated in turbulent flows9. Our results demonstrate that the structure of the interstellar medium cannot be considered in isolation. Instead, its formation and evolution are controlled by nested, interdependent flows of matter covering many orders of magnitude in spatial scale.